---
title: "UHI"
author: 'StudentID: 11455972'
date: "2024-03-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
library(readxl)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(imputeTS)
library(readr)
library(tidyverse)
library(reshape2)
library(sf)
library(leaflet)
library(RColorBrewer)
library(scales)
library(lattice)
library(rgl)
library(car)
library(broom)      # For tidying up model outputs
library(caret)      # For calculating model performance metrics
library(MASS)       # For stepwise regression
library(leaps)
library(gridExtra)
library(grid)
library(lme4)
library(Epi)  
library(performance)
library(lmerTest)
library(buildmer)
library(nlme)
library(MuMIn)
library(merTools)

```

# Data Preprocessing

```{r}
# Set the directory path
directory <- "C:\\Users\\user\\Desktop"

# Define the paths to the Excel and CSV files
excel_path <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\AllSites2014and2015 (1).xlsx")
excel_path2 <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\RostherneRadiation2014_15 (1).xlsx")
excel_path3 <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\locations_svf_ef.csv")

# Read data from the first Excel file (AllSites2014and2015 (1).xlsx)
# Skip initial non-data rows (6 rows for Allsites, 1 row for Rostherne2014/2015)
Allsites <- read_excel(excel_path, sheet = "Allsites", skip = 6)
Rostherne2014 <- read_excel(excel_path, sheet = "Rostherne2014", skip = 1)
Rostherne2015 <- read_excel(excel_path, sheet = "Rostherne2015", skip = 1)

# Read data from the second Excel file (RostherneRadiation2014_15 (1).xlsx)
# Skip initial non-data rows (2 rows for radiation data)
RostherneRadiation2014 <- read_excel(excel_path2, sheet = "RostherneRadiation2014", skip = 2)
RostherneRadiation2015 <- read_excel(excel_path2, sheet = "RostherneRadiation2015", skip = 2)


# Read the CSV file containing locations, SVF, and EF data
locations_svf_ef <- read_csv(excel_path3)
```

# Data Cleaning
# In this block, we perform basic data cleaning tasks including removing rows and columns that contain only NA values,and renaming a column for clarity.

```{r}
# Removing rows where all elements are NA
# This ensures that only rows with some data are kept in the dataset
Allsites <- Allsites[rowSums(is.na(Allsites)) != ncol(Allsites), ]

# Removing columns where all elements are NA
# This step removes columns that do not contain any useful data across all rows
Allsites <- Allsites[, colSums(is.na(Allsites)) != nrow(Allsites)]

# Rename the 'Date/Time' column to 'DateTime'
# Standardizes the column name to make it easier to reference in future analysis
colnames(Allsites)[colnames(Allsites) == "Date/Time"] <- "DateTime"

# Print the cleaned data to verify the changes
print(Allsites)

```


# Data Integration
# In this block, we will merge solar irradiation data from the `RostherneRadiation2014` dataset into the `Rostherne2014` dataset.
# This involves converting the time columns to a compatible format and performing a left join.

```{r}
# Convert the 'ob_end_time' column in RostherneRadiation2014 to POSIXct format
# This ensures that the time format matches with 'ob_time' in Rostherne2014 for accurate merging
RostherneRadiation2014$ob_end_time <- as.POSIXct(RostherneRadiation2014$ob_end_time, format="%Y-%m-%d %H:%M")


# Perform a left join to add the 'glbl_irad_amt' (global solar irradiation amount) column from RostherneRadiation2014
# The join is based on matching the 'ob_time' column in Rostherne2014 with the 'ob_end_time' column in RostherneRadiation2014
Rostherne2014 <- merge(Rostherne2014, RostherneRadiation2014[c('ob_end_time', 'glbl_irad_amt')],
                       by.x = 'ob_time', by.y = 'ob_end_time', all.x = TRUE)

# Check the first few rows of the updated Rostherne2014 to verify that the join was successful
head(Rostherne2014)


```
# The Rostherne2014 dataset now includes solar irradiation data from the RostherneRadiation2014 dataset.




# Data Integration for 2015
# In this block, we will merge solar irradiation data from the `RostherneRadiation2015` dataset into the `Rostherne2015` dataset.
# This involves converting the time columns to a compatible format and performing a left join.

```{r}

# Convert 'ob_end_time' in RostherneRadiation2015 from character to POSIXct format
# This ensures that the time format in both datasets matches for accurate merging
RostherneRadiation2015$ob_end_time <- as.POSIXct(RostherneRadiation2015$ob_end_time, format="%Y-%m-%d %H:%M")

# Perform the left join to add the 'glbl_irad_amt' (global solar irradiation amount) column from RostherneRadiation2015
# The join matches 'ob_time' in Rostherne2015 with 'ob_end_time' in RostherneRadiation2015, including all rows from Rostherne2015
Rostherne2015 <- merge(Rostherne2015, RostherneRadiation2015[c('ob_end_time', 'glbl_irad_amt')], 
                                by.x = 'ob_time', by.y = 'ob_end_time', all.x = TRUE)

# Check the first few rows of the result to verify that the join was successful
head(Rostherne2015)


```
# The Rostherne2015 dataset now includes solar irradiation data from the RostherneRadiation2015 dataset.




# Combine and Analyze 2014 and 2015 Data
# In this block, we will combine the datasets from 2014 and 2015, sort the combined data by time,
# and analyze the combined dataset to identify the presence of missing values.

```{r}

# Combine the Rostherne2014 and Rostherne2015 datasets
# The rbind() function is used to stack the datasets on top of each other
Rostherne_combined <- rbind(Rostherne2014, Rostherne2015)

# Sort the combined dataset by 'ob_time' to ensure chronological order
Rostherne_combined <- Rostherne_combined[order(Rostherne_combined$ob_time), ]

# Convert 'ob_time' to POSIXct format if it is not already in this format
# This step ensures that time-related operations can be performed accurately
Rostherne_combined$ob_time <- as.POSIXct(Rostherne_combined$ob_time)

# Calculate the total number of rows in the combined dataset
# This helps in determining the scale of the dataset and is used for further analysis
total_rows_combined <- nrow(Rostherne_combined)

# Recalculate the number of missing values in each column
# This identifies the extent of missing data across different variables in the combined dataset
missing_values_combined <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Calculate the percentage of missing values for each column
# This provides a relative measure of missingness, useful for data cleaning decisions
missing_percentage_combined <- (missing_values_combined / total_rows_combined) * 100

# Create a dataframe to summarize the status of missing values in the combined dataset
# The dataframe includes the column names, the count of missing values, and the percentage of missing data
missing_data_frame_combined <- data.frame(
  Column = names(missing_values_combined),
  MissingValues = missing_values_combined,
  MissingPercentage = missing_percentage_combined
)

# Print the dataframe to review the missing data status in the combined dataset
print(missing_data_frame_combined)


```
# The combined dataset is now ready, and an analysis of missing values has been conducted.







# Function to Analyze Missing Data
# In this block, we define a function to calculate and print the number and percentage of missing values 
# for each column in a given dataset. This function is then applied to the 'Allsites' dataset.


```{r}
# Define a function to calculate and print missing values
# The function takes a dataset and its name as arguments, calculates the number and percentage of missing values, 
# and prints a summary for each column in the dataset
print_missing_values <- function(data, dataset_name) {
  missing_values <- sapply(data, function(x) sum(is.na(x)))
  missing_percentage <- (missing_values / nrow(data)) * 100
  
  # Print the dataset name followed by the missing data summary for each column
  cat(paste("Missing Data for", dataset_name, ":\n"))
  for (i in 1:length(missing_values)) {
    cat(names(missing_values)[i], ": ", missing_values[i], " missing values (", round(missing_percentage[i], 2), "%)\n", sep = "")
  }
}

# Apply the function to the 'Allsites' dataset
# This will calculate and print the missing values for each column in the 'Allsites' dataset
print_missing_values(Allsites, "Allsites")
 
```
# The function provides a clear summary of missing data in the 'Allsites' dataset, which is useful for further data cleaning.

Remove stations with very high numbers of missing values (N6 with 53.77% and NE4 with 74.99%)



# Data Cleaning: Removing Stations with High Missing Values
# In this block, we identify and remove specific stations (columns) from the 'Allsites' dataset
# that have a very high number of missing values.

```{r}
# Define the stations (columns) with very high numbers of missing values to be removed
# These stations are specified by their column names
stations_to_remove <- c("NE4", "N6")

# Remove the specified stations from the 'Allsites' dataset
# This operation excludes the columns listed in 'stations_to_remove' from the dataset
Allsites <- Allsites[, !(names(Allsites) %in% stations_to_remove)]
```

# Summary of Cleaned Dataset
# In this block, we generate a summary of the cleaned 'Allsites' dataset after removing columns with high missing values.

```{r}
# Generate and print a summary of the 'Allsites' dataset
# The summary provides a quick overview of the dataset, including min, max, mean, and missing values for each column
summary(Allsites)
```

# The summary gives an insight into the current state of the 'Allsites' dataset after removing problematic stations.





# Visualizing NA Gaps
# In this block, we define a function to visualize gaps (missing values) in each column of the 'Allsites' dataset.
# The function is then applied to all columns that contain NA values.
```{r}

# Function to visualize NA gaps for a specific column in the dataset
# The function converts the column to a time series object and uses ggplot to create a visualization of the NA gaps
visualize_na_gaps <- function(data, column_name) {
  ts_data <- ts(data[[column_name]])  # Convert to time series object
  plot_title <- paste("NA Gaps for", column_name)
  plot <- ggplot_na_gapsize(ts_data) + ggtitle(plot_title)
  print(plot)
}

# Identify columns in 'Allsites' that contain NA values
# This helps target only those columns that need visualization of missing data
columns_with_na <- colnames(Allsites)[colSums(is.na(Allsites)) > 0]

# Visualize NA gaps for each column with NA values
# Loop through each column identified and apply the 'visualize_na_gaps' function to visualize missing data
for (column in columns_with_na) {
  visualize_na_gaps(Allsites, column)
}


```
# This block provides visualizations of NA gaps for all columns in the 'Allsites' dataset that contain missing values, helping to understand the pattern of missing data.




# Filtering and Finding Closest Stations
# In this block, we first filter out specific stations from the locations dataset. 
# Then, we define and apply a function to find the two closest stations for each station with missing data in the 'Allsites' dataset.
```{r}
# Create a copy of the locations data frame to preserve the original data
locations_svf_ef_filtered <- locations_svf_ef

# Define the list of stations that need to be removed due to high missing values or other issues
stations_to_remove <- c("CC1", "CC10", "N6", "NE4", "NW1", "NW4", "NE6", "NE5")

# Filter out the specified stations from the locations dataset
# This step removes the stations listed in 'stations_to_remove' from the dataset
locations_svf_ef_filtered <- locations_svf_ef_filtered %>% 
  filter(!Ref %in% stations_to_remove)

# Verify the removal of the specified stations by printing the filtered dataset
print(locations_svf_ef_filtered)

# Define a function to find the closest stations based on Euclidean distance
# This function will be used to identify the two closest stations for stations with missing data in 'Allsites'
find_closest_stations <- function(data, locations) {
  # Identify stations with missing values, excluding NE5 and NE6
  stations_with_na <- setdiff(colnames(data)[colSums(is.na(data)) > 0], c("NE5", "NE6"))
  
  # Initialize a list to store the closest stations
  closest_stations_list <- list()
  
  # Define a helper function to calculate Euclidean distance between two points
  euclidean_distance <- function(lon1, lat1, lon2, lat2) {
    sqrt((lon1 - lon2)^2 + (lat1 - lat2)^2)
  }
  
  # For each station with missing data, find the two closest stations based on their geographic coordinates
  for (station in stations_with_na) {
    current_location <- locations %>% filter(Ref == station)
    other_locations <- locations %>% filter(Ref != station)
    
    # Calculate distances from the current station to all other stations and sort by distance
    other_locations <- other_locations %>%
      mutate(distance = euclidean_distance(current_location$Longitude, current_location$Latitude, Longitude, Latitude)) %>%
      arrange(distance)
    
    # Identify the two closest stations
    closest_stations <- other_locations$Ref[1:2]
    closest_stations_list[[station]] <- closest_stations
  }
  
  return(closest_stations_list)
}

# Apply the 'find_closest_stations' function to find the closest stations using the filtered locations dataset
closest_stations <- find_closest_stations(Allsites, locations_svf_ef_filtered)
# Print the list of closest stations for stations with missing data
print(closest_stations)



```
# This block effectively filters out unwanted stations and then finds the two closest stations 
# for those stations in 'Allsites' that have missing data, facilitating further analysis or imputation.






```{r}

# Define the closest stations
closest_stations <- list(
  CC3 = c("CC4", "CC8"),
  CC4 = c("CC3", "CC8"),
  CC5 = c("CC6", "CC8"),
  CC6 = c("CC7", "CC8"),
  CC8 = c("CC2", "CC6"),
  CC9 = c("CC2", "CC11"),
  CC11 = c("CC9", "S1"),
  E1 = c("SE1", "NE1"),
  E2 = c("SE2", "E1"),
  S6 = c("S5", "S4"),
  SW5 = c("SW6", "SW4"),
  NE5= c("NE3", "NE6")
)
```




# Manual Imputation of Missing Values
# In this block, we define and apply a function to manually impute missing values for specific stations in the 'Allsites' dataset.
# The imputation is based on the average of values from the two closest stations identified earlier.

```{r}
# Define a function to manually impute missing values for a specific station
# The function imputes missing values in a given station using the mean of its closest neighboring stations
impute_station <- function(data, station, neighbors) {
  data[[station]] <- ifelse(is.na(data[[station]]), 
                            rowMeans(data[, neighbors], na.rm = TRUE), 
                            data[[station]])
  return(data)
}


# Apply the manual imputation for each station with missing values
# Iterate through each station identified as having missing values and apply the imputation function

for (station in names(closest_stations)) {
  Allsites <- impute_station(Allsites, station, closest_stations[[station]])
}

# The 'Allsites' dataset is now updated with missing values imputed using data from the closest stations.
```
# This block effectively handles missing data by imputing values based on nearby stations, which helps in maintaining the integrity of the dataset for further analysis.







# Visualizing Air Temperature Comparison
# In this block, we convert the 'DateTime' column to POSIXct format and create a plot to compare air temperatures 
# between two stations, CC2 and CC8, over time.
```{r}
# Convert the 'DateTime' column to POSIXct format
# This conversion ensures that the time series data is in the correct format for plotting and analysis
Allsites$DateTime <- as.POSIXct(Allsites$DateTime, format="%Y-%m-%d %H:%M:%S")

# Plot air temperatures for stations CC2 and CC8 to compare their variations over time
# The plot uses ggplot2 to create a line graph with DateTime on the x-axis and air temperature on the y-axis
ggplot(Allsites, aes(x = DateTime)) +
  geom_line(aes(y = CC2, color = "CC2")) +
  geom_line(aes(y = CC8, color = "CC8")) +
  labs(title = "Comparison of Air Temperatures: CC2 vs CC8",
       x = "Date/Time",
       y = "Air Temperature",
       color = "Legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
# This block generates a visual comparison of air temperatures between the two stations, CC2 and CC8, 
# helping to identify patterns or discrepancies in the temperature data over time.





#Missing values-Rostherne_combined (columns where more than 50% of the values are missing)

```{r}

# Calculate the number of missing values in each column
missing_values <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Create a dataframe to display the column names and the count of missing values
missing_data_frame_combined <- data.frame(Column = names(missing_values), MissingValues = missing_values)

# Print the dataframe that shows columns and their respective number of missing values
print(missing_data_frame_combined)

```




```{r}
# Calculate the number of missing values in each column
# This identifies the extent of missing data across all columns in the 'Rostherne_combined' dataset

column_missing_values <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Calculate the total number of rows in the dataset
# This value is used to compute the percentage of missing data for each column
total_rows <- nrow(Rostherne_combined)

# Calculate the percentage of missing values in each column
# The percentage provides a relative measure of how much data is missing in each column

column_missing_percentage <- (column_missing_values / total_rows) * 100

# Identify columns where more than 50% of the values are missing
# This step filters out columns with a significant amount of missing data for further investigation

columns_over_50_missing <- names(column_missing_percentage[column_missing_percentage > 50])

# Print the names of the columns, the number of missing values, and the percentage
# This loop provides a detailed report of the columns with over 50% missing data, making it easier to decide on next steps
print("Columns with more than 50% missing values:")
for (col in columns_over_50_missing) {
    print(paste("Column:", col, 
                "- Missing Values:", column_missing_values[col], 
                "- Percentage:", column_missing_percentage[col], "%"))
}


```



# This block of code is focused on cleaning the 'Rostherne_combined' dataset by removing columns that have more than 50% missing data.
# After removing these columns, the code recalculates the missing values to assess the impact of the cleaning process.


```{r}

# Remove the columns with more than 50% missing values from the dataframe
# This step helps in cleaning the dataset by excluding columns with excessive missing data

Rostherne_combined <- Rostherne_combined[, !(names(Rostherne_combined) %in% columns_over_50_missing)]

# Print the updated dataframe to verify that the columns have been removed
# This allows you to inspect the dataframe and ensure the removal was successful
print(Rostherne_combined)

# Recalculate the number of missing values in each column of the updated dataframe
# This step checks the status of missing data after the problematic columns have been removed
missing_values_updated <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Update the missing values dataframe to reflect the current state of the dataset
# This updated dataframe provides an overview of missing data in the cleaned dataset
missing_data_frame_201415 <- data.frame(Column = names(missing_values_updated), MissingValues = missing_values_updated)

# Print the updated missing values dataframe
# This output helps to verify the current missing data situation after column removal
print(missing_data_frame_201415)

```





# This block of code is focused on cleaning the 'Rostherne_combined' dataset by removing columns that have names ending with '_q' or '_j'.
# These columns may represent quality flags or other auxiliary data that are not needed for further analysis.

```{r}
# Remove columns that end with '_q' and '_j' from Rostherne_combined
# The dplyr::select function is used with the matches() function to identify and remove columns
# ending with '_q' (likely indicating quality flags) and '_j' from the dataset.
Rostherne_combined <- Rostherne_combined %>%
  dplyr::select(-matches(".*_q$"), -matches(".*_j$"))


```
# The resulting dataset, 'Rostherne_combined', no longer contains these auxiliary columns, making it more streamlined for analysis.



# This block of code is designed to streamline the 'Rostherne_combined' dataset by removing specific columns (2 through 9),
# which are presumed to be unnecessary ID columns, making the dataset more focused on the relevant data for analysis.
```{r}
# Remove ID columns (columns 2 through 9) from the 'Rostherne_combined' dataset
# The dplyr::select function is used with a negative index to drop columns 2 through 9
# These columns are likely ID columns or other metadata that are not needed for further analysis
Rostherne_combined <- Rostherne_combined %>%
  dplyr::select(-c(2:9))  # negative to drop columns 2 through 9


```
# The resulting 'Rostherne_combined' dataset is now streamlined, excluding unnecessary ID columns, and ready for further analysis.



# This block of code is focused on refining the 'Rostherne_combined' dataset by removing specific columns that are deemed unnecessary for the analysis.
# After removing these columns, a summary of the dataset is generated to review its current state.

```{r}
# Remove specific unnecessary columns from 'Rostherne_combined'
# The dplyr::select function is used to drop the following columns:
# - 'cld_amt_id_1' and 'cld_base_ht_id_1': Likely related to cloud amount and base height, which may not be relevant.
# - 'meto_stmp_time' and 'midas_stmp_etime': Timestamps or metadata columns that may not be needed for analysis.
Rostherne_combined <- Rostherne_combined %>%
    dplyr::select(-cld_amt_id_1, -cld_base_ht_id_1, -meto_stmp_time, -midas_stmp_etime)

```


# Generate a summary of the updated 'Rostherne_combined' dataset
# This summary provides an overview of the dataset's structure, including descriptive statistics for each column.
```{r}
summary(Rostherne_combined)

```

# This block of code is focused on visualizing the gaps (missing values) in specific columns of the 'Rostherne_combined' dataset.
# By generating plots for each column with NA values, the code helps to understand the pattern and extent of missing data.
```{r}

# List of columns to visualize NA gaps
# These columns are selected based on their relevance and the presence of missing values that need to be visualized
columns_with_na <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                     "visibility", "msl_pressure", "air_temperature", "dewpoint",
                     "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                     "drv_hr_sun_dur", "glbl_irad_amt")

# Function to visualize NA gaps for each column
# This function converts the selected column to a time series object and creates a plot
# showing the gaps (missing values) within the time series data
visualize_na_gaps <- function(data, column_name) {
  ts_data <- ts(data[[column_name]])  # Convert to time series object
  plot_title <- paste("NA Gaps for", column_name)
  plot <- ggplot_na_gapsize(ts_data) + ggtitle(plot_title)
  print(plot)
}

# Loop through the list of columns with NA values and visualize the gaps for each
# This loop applies the 'visualize_na_gaps' function to each specified column in the 'Rostherne_combined' dataset
for (column in columns_with_na) {
  visualize_na_gaps(Rostherne_combined, column)
}

# The plots generated in this block provide a visual understanding of the missing data patterns in key columns.
```



# This block of code is focused on handling missing values for specific variables in the 'Rostherne_combined' dataset.
# It uses linear interpolation to fill in small gaps in selected variables where the number of missing values is minimal.
 
```{r}
# List of variables to be interpolated
# These variables are chosen because they have small gaps and a small number of missing values, making them suitable for linear interpolation
variables_interpolation <- c("wind_direction", "wind_speed")

# Loop through each specified variable and apply linear interpolation
# The na_interpolation function is used with the 'linear' option to fill in missing values in a straightforward manner
for (col in variables_interpolation) {
  if (col %in% names(Rostherne_combined)) {
    Rostherne_combined[[col]] <- na_interpolation(Rostherne_combined[[col]], option = "linear")
  }
}


```




# This block of code is focused on handling missing values for variables with larger gaps or a higher number of missing values in the 'Rostherne_combined' dataset.
# Kalman smoothing with auto ARIMA is applied to these variables to fill in the missing data more effectively.
```{r}

# Function to apply Kalman smoothing with auto ARIMA
# This function uses the na_kalman function with the 'auto.arima' model to smooth and interpolate missing values
kalman_auto_arima <- function(series) {
  series_kalman <- na_kalman(series, model = "auto.arima")
  return(series_kalman)
}

# Identify columns with missing values, excluding those already interpolated
# This ensures that Kalman smoothing is only applied to variables that haven't been handled by linear interpolation
variables_kalman <- setdiff(names(Rostherne_combined), variables_interpolation)

# Apply Kalman smoothing with auto ARIMA to the remaining variables
# The loop iterates through the remaining columns and applies the Kalman smoothing if any missing values are present
for (col in variables_kalman) {
  if (any(is.na(Rostherne_combined[[col]]))) {
    Rostherne_combined[[col]] <- kalman_auto_arima(Rostherne_combined[[col]])
  }
}

# Display the result
# This prints the updated 'Rostherne_combined' dataset, showing the effect of the Kalman smoothing
print(Rostherne_combined)




```



# This block is intended to provide a comprehensive summary of the 'Rostherne_combined' dataset after applying the interpolation and Kalman smoothing techniques.
# The summary function will give an overview of the current state of the dataset, including descriptive statistics for each variable.

```{r}
# Generate a summary of the 'Rostherne_combined' dataset
# This summary includes key statistics such as minimum, maximum, mean, and the number of missing values for each column
summary(Rostherne_combined)
```




#Relative humidity values calculated from standard weather instruments range from as low as near 1 percent when the evaporation rate greatly #exceeds the condensation rate (a huge difference between temperature and dew point), to 100 percent when the evaporation rate equals the #condensation rate (temperature and dew point are equal).

```{r}

Rostherne_combined <- Rostherne_combined %>%
  # Replace negative values in glbl_irad_amt with zero
  
  mutate(glbl_irad_amt = ifelse(glbl_irad_amt < 0, 0, glbl_irad_amt)) %>%
  # Ensure rltv_hum values are not above 100
  
  mutate(rltv_hum = ifelse(rltv_hum > 100, 100, rltv_hum)) %>%
  
  # Round cld_ttl_amt_id values and convert to integers
  
  mutate(cld_ttl_amt_id = as.integer(round(cld_ttl_amt_id))) %>%

# Replace negative values in drv_hr_sun_dur with zero

  mutate(drv_hr_sun_dur = ifelse(drv_hr_sun_dur < 0, 0, drv_hr_sun_dur))

summary(Rostherne_combined)
```


# This block of code is focused on visualizing the distribution of various weather variables in the 'Rostherne_combined' dataset.
# By creating boxplots, you can easily compare the spread, central tendency, and presence of outliers across different weather variables.

```{r}


# Select the weather variables you want to plot
# These variables are chosen for their relevance to the analysis and are stored in the 'weather_vars' list
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                  "visibility", "msl_pressure", "air_temperature", "dewpoint",
                  "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                  "drv_hr_sun_dur", "glbl_irad_amt")

# Subset the data to include only the selected weather variables
# This step extracts only the relevant columns from the 'Rostherne_combined' dataset
weather_data <- Rostherne_combined %>% dplyr::select(all_of(weather_vars))

# Convert the data from wide to long format for plotting
# The pivot_longer function reshapes the data so that each variable's values are in a single column, 
# making it easier to plot multiple variables in one graph
weather_data_long <- weather_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Create the boxplots for each weather variable
# ggplot2 is used to generate boxplots, which help visualize the distribution and identify outliers for each variable
ggplot(weather_data_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  labs(title = "Boxplots of Weather Variables in Rostherne_combined",
       x = "Weather Variables",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```

# This block of code creates a scatter plot to visualize the variation of global radiation over time in the 'Rostherne_combined' dataset.
# The plot helps in identifying patterns, trends, or anomalies in global radiation values throughout the observation period.

```{r}

# Create a scatter plot for global radiation over time
# The scatter plot uses 'ob_time' as the x-axis (time) and 'glbl_irad_amt' as the y-axis (global radiation amount)
ggplot(Rostherne_combined, aes(x = ob_time, y = glbl_irad_amt)) +
  geom_point(color = "blue", alpha = 0.5) +
  labs(title = "Scatter Plot of Global Radiation Over Time",
       x = "Time",
       y = "Global Radiation (glbl_irad_amt)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability


```

```{r}
# Rename columns for clarity and consistency
colnames(Rostherne_combined)[colnames(Rostherne_combined) == "ob_time"] <- "DateTime"

```



#The datasets are merged on the DateTime column using a left join, meaning all rows from Allsites are kept, and matching rows from #Rostherne_combined are added.

```{r}

# Merge the datasets based on DateTime

combined_data <- merge(Allsites, Rostherne_combined, by = "DateTime", all.x = TRUE)

```



#Calculate UHI Intensities and Replace Urban Temperatures

#I calculate the Urban Heat Island (UHI) intensities by subtracting the rural temperature (air_temperature) from each urban sensor's #temperature and then I replace the urban temperature values with the calculated UHI intensities.

```{r}
# Define the pattern to identify sensor columns
# The pattern matches column names starting with CC, N, NE, E, SE, S, SW, W, or NW, which are likely to be sensor locations
sensor_patterns <- "^CC|^N|^NE|^E|^SE|^S|^SW|^W|^NW"
sensor_columns <- grep(sensor_patterns, names(combined_data), value = TRUE)

# Calculate UHI intensity for each sensor
# The loop iterates over each sensor column, calculates the UHI intensity by subtracting the 'air_temperature' (rural temperature),
# and then renames the column to indicate it now represents UHI intensity
for (sensor in sensor_columns) {
  combined_data[[sensor]] <- combined_data[[sensor]] - combined_data$air_temperature
  # Rename the column to include "UHI_intensity"
  names(combined_data)[names(combined_data) == sensor] <- paste0(sensor, "_UHI")
}
# Display the first few rows of the updated dataset
# The head function is used to verify the changes made to the dataset
head(combined_data)

```

# This block of code processes the 'combined_data' dataset by converting the DateTime column, reshaping the data to a long format,
# cleaning up the 'Ref' column, and arranging the data based on a specified order of sensor references. This prepares the dataset
# for further analysis or visualization by organizing it in a consistent and interpretable format.

```{r}
# Convert DateTime to datetime format
combined_data$DateTime <- as.POSIXct(combined_data$DateTime, format="%Y-%m-%d %H:%M:%S", tz="UTC")

#Reshape the dataset to long format
long_data <- pivot_longer(combined_data, cols = ends_with("_UHI"), names_to = "Ref", values_to = "UHI")

# Remove the "_UHI" suffix from the 'Ref' column
long_data$Ref <- sub("_UHI", "", long_data$Ref)

# Specify the desired order of 'Ref'
desired_order <- c("CC2", "CC3", "CC4", "CC5", "CC6", "CC7", "CC8", "CC9", "CC11", 
                   "N1", "N2", "N3", "N4", "N5", "NE1", "NE2", "NE3", "NE5", "NE6", 
                   "E1", "E2", "E3", "E4", "E5", "E6", "SE1", "SE2", "SE3", "SE4", 
                   "SE5", "SE6", "S1", "S2", "S3", "S4", "S5", "S6", "S7", "S8", 
                   "SW1", "SW2", "SW3", "SW4", "SW5", "SW6", "W1", "W2", "W3", 
                   "W4", "W5", "NW0", "NW2", "NW3", "NW5", "NW6")


# Create a custom sorting index and arrange the data
long_data <- long_data %>%
  mutate(Ref_order = match(Ref, desired_order)) %>%
  arrange(Ref_order, DateTime) %>%
  dplyr::select(-Ref_order)

```



```{r}

# Reorder the columns to have DateTime, Ref, UHI, and other observations together
long_data <- long_data %>% dplyr::select(DateTime, Ref, UHI, everything())

```



# This block of code enhances the 'long_data' dataset by performing a left join with the 'locations_svf_ef' dataset.
# The join adds geographical and other relevant information from 'locations_svf_ef' to the UHI data based on the 'Ref' column.

```{r}

# Perform the left join with the locations data
# The left_join function merges 'long_data' with 'locations_svf_ef' using the 'Ref' column as the key
# This adds location-specific information such as coordinates or SVF to each UHI observation in 'long_data'
long_data <- long_data %>% left_join(locations_svf_ef, by = "Ref")

# Print the first few rows of the resulting dataset
# This helps in verifying that the join was successful and that the data has been merged correctly
print(head(long_data))

# Print the structure of the resulting dataset
# The str function provides an overview of the dataset's structure, including data types and the presence of any new columns
print(str(long_data))

```
# This block of code is designed to identify and handle any mismatches between the 'long_data' dataset and the 'locations_svf_ef' dataset.
# It first checks for any unmatched references in both datasets and then filters out the rows in 'long_data' that do not have corresponding location data.

```{r}
# Identify and print location references in 'locations_svf_ef' that do not match any in 'long_data'
# The anti_join function is used to find rows in 'locations_svf_ef' that don't have a matching 'Ref' in 'long_data'
unmatched_locations <- anti_join(locations_svf_ef, long_data, by = "Ref")
print(unmatched_locations$Ref)

# Identify and print references in 'long_data' that do not have corresponding entries in 'locations_svf_ef'
# Another anti_join is performed to find rows in 'long_data' that lack a matching 'Ref' in 'locations_svf_ef'
missing_refs <- anti_join(long_data, locations_svf_ef, by = "Ref")
print(unique(missing_refs$Ref))

# Filter out rows in 'long_data' that do not have corresponding references in 'locations_svf_ef'
# The filter function removes any rows from 'long_data' that have a 'Ref' value identified as missing in the previous step
long_data <- long_data %>%
  filter(!Ref %in% missing_refs$Ref)

# The resulting 'long_data' dataset now only includes rows with valid location references, ensuring consistency between the datasets.
```


# This block of code is focused on identifying rows in the 'long_data' dataset that have missing values in the 'Latitude' or 'Longitude' columns.
# These rows are important to identify as they may indicate issues with location data, which is critical for spatial analysis.

```{r}

# Identify rows with NA values in 'Latitude' and 'Longitude' columns
# The filter function is used to select rows where either 'Latitude' or 'Longitude' is missing

na_rows <- long_data %>%
  filter(is.na(Latitude) | is.na(Longitude))


# Extract and print the unique 'Ref' values associated with these rows
# The unique function ensures that each reference is listed only once, making it easier to identify problematic entries
na_refs <- unique(na_rows$Ref)
print(na_refs)


```


# This block of code is designed to identify potential outliers in the Urban Heat Island (UHI) data by calculating the minimum and maximum UHI values for each station.
# Outliers can be identified by examining these extreme values across the different stations.
```{r}
# Function to find minimum and maximum UHI values for each station
# This function groups the data by station ('Ref') and calculates the minimum and maximum UHI values for each group
find_min_max_per_station <- function(data) {
  data %>%
    group_by(Ref) %>%
    summarise(
      min_UHI = min(UHI, na.rm = TRUE),
      max_UHI = max(UHI, na.rm = TRUE)
    )
}

# Find and print the minimum and maximum UHI values for each station
# This step applies the 'find_min_max_per_station' function to the 'long_data' dataset
min_max_per_station <- find_min_max_per_station(long_data)
print(min_max_per_station)
# Print the result to identify stations with extreme UHI values
```




# This block of code is used to determine the overall range of UHI values in the 'long_data' dataset.
# The range provides a quick overview of the minimum and maximum UHI values across all stations, which is useful for understanding the data distribution and identifying potential outliers.
```{r}
# Print the overall range of UHI values in the dataset
# The range function returns the minimum and maximum values of the 'UHI' column, giving an overview of the spread of UHI values
print(range(long_data$UHI))
```




# This block of code creates a histogram to visualize the distribution of Urban Heat Island (UHI) values in the 'long_data' dataset.
# The histogram helps to understand the frequency of different UHI values and can highlight the presence of any skewness or outliers in the data.
```{r}
# Create a histogram of UHI values
# The ggplot function is used to generate a histogram, with UHI values on the x-axis and their frequency on the y-axis
ggplot(long_data, aes(x = UHI)) + # Create histogram with specified bin width and styling
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) + # Add title and axis labels
  labs(title = "Histogram of UHI Values in Manchester", x = "UHI", y = "Frequency") +
  theme_minimal() # Apply a minimal theme for a clean and simple appearance
```


#Correlation Analysis 

# This block of code creates a heatmap to visualize the correlation matrix of selected weather variables in the 'combined_data' dataset.
# The heatmap provides an intuitive way to understand the relationships between different weather variables, with color intensity indicating the strength of the correlation.
```{r}
# Define the weather variables to include in the correlation matrix
# These variables are selected for their relevance to the analysis and are stored in the 'weather_vars' vector
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht", 
                  "visibility", "msl_pressure", "air_temperature", "dewpoint", 
                  "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                  "drv_hr_sun_dur", "glbl_irad_amt")


# Melt the correlation matrix for better visualization
# The melt function reshapes the correlation matrix into a long format that is suitable for plotting with ggplot
# Plot the heatmap with correlation values
ggplot(data = melt( cor(combined_data[, weather_vars], use = "complete.obs")  ), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name="Correlation") +
  geom_text(aes(label = sprintf("%.2f", value)), color = "black", size = 2.5) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1)) +
  coord_fixed()


```
High Positive Correlations:

wetb_temp and air_temperature (0.96): Wet bulb temperature is highly positively correlated with air temperature. This makes sense as wet bulb temperature is influenced by the air temperature and humidity.

dewpoint and air_temperature (0.85): Dewpoint is also strongly correlated with air temperature.

dewpoint and wetb_temp (0.95):Dewpoint is also strongly correlated with Wet bulb temperature.

wmo_hr_sun_dur and drv_hr_sun_dur (0.94): The World Meteorological Organization (WMO) hourly sunshine duration is highly correlated with derived hourly sunshine duration, indicating consistency between these measurements.

wmo_hr_sun_dur and glbl_irad_amt (0.73):Both WMO measured and derived sunshine durations are strongly correlated with global solar irradiation .
drv_hr_sun_dur and glbl_irad_amt (0.72):Both WMO measured and derived sunshine durations are strongly correlated with global solar irradiation .

stn_pres and msl_pressure (1.00): Station pressure is perfectly correlated with mean sea level pressure.

High Negative Correlations:

glbl_irad_amt and rltv_hum (-0.65): global solar irradiation  is strongly negatively correlated with relative humidity. This indicates that higher irradiance is associated with lower humidity levels.

cld_ttl_amt_id and cld_base_ht (-0.60): Cloud total amount is negatively correlated with WMO hourly sunshine duration. More clouds reduce the amount of sunshine.




# This block of code is focused on refining the 'long_data' dataset by removing specific columns that are not needed for further analysis.
# After removing these columns, the code displays the first few rows of the updated dataset to verify the changes.
```{r}
# Remove the specified columns from 'long_data'
# The dplyr::select function is used to drop the 'msl_pressure', 'wmo_hr_sun_dur', and 'drv_hr_sun_dur' columns, which may be redundant or irrelevant for the analysis
long_data <- long_data %>%
  dplyr::select(-msl_pressure, -wmo_hr_sun_dur, -drv_hr_sun_dur)
```

```{r}
# Display the first few rows of the updated dataset
# The head function is used to check the structure of 'long_data' after the specified columns have been removed
head(long_data)
```





#Exploratory Data Analysis (EDA) 


# This block of code refines the 'locations_svf_ef' dataset by removing specific locations and any rows with missing coordinates.
# It then converts the data to a spatial format and visualizes it using a leaflet map, where locations are color-coded based on their EF (emissivity factor) values.
```{r}
# Define the list of locations to be removed
# These locations are specified for removal based on criteria such as redundancy, irrelevance, or poor data quality
locations_to_remove <- c("CC1", "CC10", "N6", "NE4", "NW1", "NW4", "S7", "S8", "NE5", "NE6")

# Remove rows with missing coordinates and the specified locations
# The filter function removes rows where either 'Longitude' or 'Latitude' is missing and excludes the locations listed in 'locations_to_remove'
locations_svf_ef <- locations_svf_ef %>%
  filter(!is.na(Longitude) & !is.na(Latitude) & !Ref %in% locations_to_remove)

# Convert the dataset to an sf (spatial features) object
# This conversion is necessary for spatial operations and visualization, setting the coordinate system to WGS 84 (EPSG:4326)
data_sf <- st_as_sf(locations_svf_ef, coords = c("Longitude", "Latitude"), crs = 4326)

# Create a color palette for visualizing EF values
# The colorNumeric function generates a color scale from yellow to red, corresponding to the range of EF values
pal <- colorNumeric(palette = "YlOrRd", domain = data_sf$EF)

# Create the leaflet map for visualizing locations
# The map includes base tiles, circles representing locations with colors based on EF values, and labels for each location reference (Ref)
leaflet(data_sf) %>% 
  addTiles(options = tileOptions(opacity = 0.45)) %>% 
  addCircles(
    color = "red", 
    fillColor = ~pal(EF), 
    fillOpacity = 0.01, 
    radius = 50, 
    popup = ~paste("Ref: ", Ref, "<br>")
  ) %>% 
  addLabelOnlyMarkers(
    label = ~Ref,
    labelOptions = labelOptions(noHide = TRUE, direction = 'top', textOnly = TRUE)
  ) %>%
  addMiniMap()


```



Typically the temperature difference between rural and urban areas is larger at night than during the day, is most obvious when winds are weak and skies are clear, and is greater in the summer than in the winter (Oke, 1987).

First, I aimed to examine the frequency distribution of Urban Heat Island (UHI) intensities for all sites during the daytime in the summer months (June, July, August) for the years 2014 and 2015. Additionally, nocturnal UHI intensities for Brown Street and University Place were also selected for a similar histogram analysis. This was done because Brown Street has the smallest sky view factor (Great Ducie street has a much lower sky view factor, however, it is not a canyon, the monitoring station was located under a bridge) and University Place has the largest among the canyons.





# This block of code calculates the mean Urban Heat Island (UHI) intensity by month for the years 2014 and 2015.
# It then visualizes the monthly mean UHI values using a line plot, with separate facets for each year to compare the seasonal patterns.
```{r}

# Calculate mean UHI by month for both 2014 and 2015
# The mutate function is used to extract the year and month from the DateTime column, then the data is grouped by Year and Month
# The summarise function calculates the mean UHI for each month, excluding NA values
monthly_uhi <- long_data %>%
  mutate(Year = year(DateTime),
         Month = floor_date(DateTime, "month")) %>%
  group_by(Year, Month) %>%
  summarise(Mean_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Convert Month to Date format
# This step ensures that the Month column is in Date format, which is necessary for accurate plotting along the x-axis
monthly_uhi$Month <- as.Date(monthly_uhi$Month)

# Plot the mean UHI by month, faceted by year
# The ggplot function creates a line plot of mean UHI values over time, with separate lines for each year
# The facet_wrap function is used to create separate plots for 2014 and 2015, allowing for easy comparison between the years
ggplot(monthly_uhi, aes(x = Month, y = Mean_UHI, color = factor(Year))) +
  geom_line() +
  labs(title = 'Mean UHI Intensity by Month (2014 and 2015)',
       x = 'Month', y = 'Mean UHI Intensity (°C)',
       color = 'Year') +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  facet_wrap(~ Year, scales = 'free_x')


```






# This block of code calculates the mean Urban Heat Island (UHI) intensity by month, combining data from both 2014 and 2015.
# It then creates and customizes a line plot to visualize the monthly mean UHI values, enhancing readability by adjusting label and text sizes.
# Finally, the plot is saved as a PNG file with high resolution.
```{r}

# Calculate mean UHI by month, combining data from both 2014 and 2015
# The mutate function extracts the month from the DateTime column, and the data is grouped by month
# The summarise function calculates the mean UHI for each month across both years
monthly_uhi_combined <- long_data %>%
  mutate(Month = month(DateTime, label = FALSE)) %>%  # Extract the month as a number
  group_by(Month) %>%  # Group by the month number
  summarise(Mean_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')  # Calculate the mean UHI for each month

# Plot the mean UHI by month for the combined data
# The ggplot function is used to create a line plot, with additional customization for colors, labels, and text sizes
p<-ggplot(monthly_uhi_combined, aes(x = Month, y = Mean_UHI)) +
  geom_line(color = "#4F6272") +  # Medium Sea Green
  geom_point(color = "#4F6272", size = 2) +
  labs(x = 'Month', y = 'Mean UHI Intensity (°C)') +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +  # Use month abbreviations as labels
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p.png", plot = p, width = 8, height = 4, units = "in", dpi = 300)
```



# This block of code processes the 'long_data' dataset to filter for daytime hours and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensity for the years 2014 and 2015. The plot is customized for readability and saved as a PNG file.
```{r}


# Ensure the DateTime column is in the correct format and add Year and Hour columns
# The mutate function adds 'Year' (as a factor) and 'Hour' (as an integer) columns for further filtering and analysis
combined_data <- long_data %>%
  mutate(Year = as.factor(year(DateTime)),  # Convert Year to a factor
         Hour = hour(DateTime))  # Extract the hour to filter by daytime and nighttime

# Filter for daytime hours (6 AM to 6 PM)
# This step filters the data to include only observations between 6 AM and 6 PM, which are considered daytime hours
daytime_data <- combined_data %>%
  filter(Hour >= 6 & Hour < 18)

# Create the frequency distribution plot for both 2014 and 2015 side by side (Daytime)
# The ggplot function creates a histogram showing the distribution of UHI values, with separate plots for 2014 and 2015
p1<-ggplot(daytime_data, aes(x = UHI, fill = Year)) +
  geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
  scale_y_continuous(labels = percent) +
  labs(x = "Urban Heat Island Intensity (°C)",
       y = "% of time") +
  scale_fill_manual(values = c("2014" = "#FF9999", "2015" = "#9999FF")) +
  theme_minimal() +
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )+
  facet_wrap(~ Year, scales = "free_y")



# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p1.png", plot = p1, width = 8, height = 4, units = "in", dpi = 300)

```



# This block of code filters the UHI data for nighttime hours and creates a frequency distribution plot of UHI intensities for 2014 and 2015.
# The plot is customized for better readability and saved as a PNG file.
```{r}

# Filter for nighttime hours (6 PM to 6 AM)
# This step filters the data to include only observations between 6 PM and 6 AM, which are considered nighttime hours
nighttime_data <- combined_data %>%
nighttime_data <- combined_data %>%
  filter(Hour >= 18 | Hour < 6)

# Create the frequency distribution plot for both 2014 and 2015 side by side (Nighttime)
# The ggplot function creates a histogram showing the distribution of UHI values during nighttime, with separate plots for 2014 and 2015
p2 <- ggplot(nighttime_data, aes(x = UHI, fill = Year)) +
  geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
  scale_y_continuous(labels = percent) +
  labs(title = "Frequency Distribution of Nighttime UHI Intensities (2014 vs 2015)",
       x = "Urban Heat Island Intensity (°C)",
       y = "% of time") +
  scale_fill_manual(values = c("2014" = "#4F6272", "2015" = "#5E9EA0")) +
  theme_minimal() +theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )+
  facet_wrap(~ Year, scales = "free_y")



# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p2.png", plot = p2, width = 8, height = 4, units = "in", dpi = 300)


```

   
   
   
   
  
## Frequency distribution for daytime UHI Intensities during Summer

# Overall Comment:
# This block of code defines a function to filter data for summer months and daytime hours, and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensities for a specified year. The function is then used to generate plots for the summers of 2014 and 2015.

```{r}

# Function to filter data, create UHI columns, and plot the histogram
# This function filters the data for summer months (June, July, August) and daytime hours (6 AM to 6 PM) for a given year.
# It then creates a histogram to visualize the distribution of UHI intensities during these periods.
plot_uhi_distribution <- function(data, year) {
  # Filter for summer months (June, July, August) in the specified year and daytime hours (6 AM to 6 PM)
  summer_daytime_data <- data %>%
    filter(year(DateTime) == year, month(DateTime) %in% c(6, 7, 8), hour(DateTime) >= 6, hour(DateTime) < 18)
  
  # Create the frequency distribution plot
  ggplot(summer_daytime_data, aes(x = UHI)) +
    geom_histogram(binwidth = 0.5, fill = "red", color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = percent) +
    labs(title = paste("Frequency Distribution for Daytime UHI Intensities During Summer", year),
         x = "Urban Heat Island Intensity (°C)",
         y = "% of time") +
    theme_minimal()
}

# Plot for summer 2014
# This call to the function generates and displays the UHI distribution plot for the summer months of 2014
plot_uhi_distribution(long_data, 2014)

# Plot for summer 2015
# This call to the function generates and displays the UHI distribution plot for the summer months of 2015
plot_uhi_distribution(long_data, 2015)



```






## Frequency distribution for nocturnal UHI Intensities during Summer

# Overall Comment:
# This block of code defines a function to filter data for summer months and nocturnal hours, and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensities for both 2014 and 2015. The function is used to generate a comparative plot for nocturnal UHI intensities during the summer.

```{r}

# Function to filter data, create UHI columns, and plot the histogram for nocturnal hours
# This function filters the data for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM) across both 2014 and 2015.
# It then creates a histogram to visualize the distribution of UHI intensities during these periods, with a side-by-side comparison between the years.
plot_uhi_distribution_nocturnal <- function(data) {
  # Filter for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM)
  summer_nocturnal_data <- data %>%
    filter(month(DateTime) %in% c(6, 7, 8), hour(DateTime) < 6 | hour(DateTime) >= 18)
  
  # Create the frequency distribution plot
  ggplot(summer_nocturnal_data, aes(x = UHI, fill = as.factor(year(DateTime)))) +
    geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = percent) +
    labs(title = "Frequency Distribution for Nocturnal UHI Intensities During Summer (2014 vs 2015)",
         x = "Urban Heat Island Intensity (°C)",
         y = "% of time",
         fill = "Year") +
    scale_fill_manual(values = c("2014" = "#6A5ACD",  # Medium Slate Blue
                                 "2015" = "#20B2AA")) +  # Light Sea Green
    theme_minimal() +
    facet_wrap(~ year(DateTime), scales = "free_y")
}

# Plot for summer 2014 and 2015 nocturnal side by side
# This call to the function generates and displays the UHI distribution plot for the nocturnal hours during the summer months of 2014 and 2015
plot_uhi_distribution_nocturnal(long_data)



```





## Frequency distribution for nocturnal UHI Intensities for University Place during Summer
# This block of code defines a function to filter data for nocturnal hours during the summer for a specific sensor,
# and then creates a frequency distribution plot of Urban Heat Island (UHI) intensities. The function is applied to data for University Place (sensor CC11)
# for the summers of 2014 and 2015.

```{r}
# Function to plot nocturnal UHI intensities for a specific sensor
# This function filters the data for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM) for a given year and sensor.
# It then creates a histogram to visualize the distribution of UHI intensities for the specified location and year.
plot_uhi_distribution_for_sensor <- function(data, year, sensor_name, location_name) {
  
  # Filter for summer months (June, July, August) in the specified year and nocturnal hours (6 PM to 6 AM)
  summer_nocturnal_data <- data %>%
    filter(year(DateTime) == year & month(DateTime) %in% c(6, 7, 8) & (hour(DateTime) < 6 | hour(DateTime) >= 18) & Ref == sensor_name)
  
  # Create the frequency distribution plot
  ggplot(summer_nocturnal_data, aes(x = UHI)) +
    geom_histogram(binwidth = 0.5, fill = "red", color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = paste("Frequency distribution for nocturnal UHI Intensities for", location_name, "during Summer", year),
         x = "Urban Heat Island Intensity (°C)",
         y = "% of time") +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_text(size = 12)
    )
}

# Plot for summer 2014 nocturnal for University Place (CC11)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2014 at University Place (sensor #CC11)
plot_uhi_distribution_for_sensor(long_data, 2014, "CC11", "University Place")

# Plot for summer 2015 nocturnal for University Place (CC11)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2015 at University Place (sensor #CC11)
plot_uhi_distribution_for_sensor(long_data, 2015, "CC11", "University Place")
```







## Frequency distribution for nocturnal UHI Intensities for Brown Street during Summer
# This block of code generates frequency distribution plots of nocturnal Urban Heat Island (UHI) intensities during summer for Brown Street (sensor CC8).
# The plots are created for the years 2014 and 2015, allowing for a comparison of nocturnal UHI patterns across these two years.
```{r}
# Plot for summer 2014 nocturnal for Brown Street (CC8)
# This call to the previously defined function generates and displays the UHI distribution plot for the summer nocturnal hours of 2014 at Brown #Street (sensor CC8_
plot_uhi_distribution_for_sensor(long_data, 2014, "CC8", "Browns Street")

# Plot for summer 2015 nocturnal for Brown Street (CC8)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2015 at Brown Street (sensor #CC8)
plot_uhi_distribution_for_sensor(long_data, 2015, "CC8", "Browns Street")
```







## Graphs showing the mean UHI intensity daily profile  
# This block of code calculates the mean Urban Heat Island (UHI) intensity for each hour of the day across the entire year.
# It then visualizes this daily profile using a combination of scatter points and a smoothed line plot. The plot is customized for readability and saved as a PNG file.
```{r}

# Calculate the mean UHI intensity for each hour of the day for the entire year
# The data is grouped by hour, and the mean UHI intensity is calculated for each hour across all observations
p3 <- ggplot(long_data %>%
  mutate(hour = hour(DateTime)) %>%
  group_by(hour) %>%
  summarise(UHI_intensity = mean(UHI, na.rm = TRUE)), aes(x = hour, y = UHI_intensity)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_smooth(method = "loess", color = "black") +
  scale_x_continuous(breaks = seq(0, 23, by = 1), labels = seq(0, 23, by = 1)) +
  labs(x = "Time of day",
       y = "UHI Intensity (°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p3.png", plot = p3, width = 8, height = 4, units = "in", dpi = 300)



```

The UHI intensity is highest during the nighttime and early morning hours, particularly from around midnight (0:00) to 5:00 and again from 18:00 to 23:00. This is because urban areas retain and release the heat absorbed during the day more slowly than rural areas, leading to higher nighttime temperatures in urban areas. The peak UHI intensity is around 2°C, indicating that urban areas are significantly warmer than rural areas during these hours.
The UHI intensity starts to decline in the early morning hours, reaching its lowest point around noon (12:00). The lowest UHI intensity is around 0.5°C , suggesting that the temperature difference is minimal during these hours.
The UHI intensity begins to increase again in the late afternoon, around 15:00, and continues to rise into the evening. 

The graph confirms that the UHI effect is most significant during nighttime and early morning hours.





# This block of code filters the dataset for city center stations in 2014, calculates the monthly average UHI for each station,
# and then visualizes the results using a lattice plot. The plot is designed to show the monthly UHI effect for each city center station in 2014.

```{r}

# Filter the dataset for city center stations, the year 2014, and calculate monthly averages in one pipeline
cc_data_monthly_avg <- long_data %>%
  filter(grepl("^CC", Ref)) %>%                                 # Filter for city center stations
  filter(DateTime >= as.POSIXct("2014-01-01") & DateTime < as.POSIXct("2015-01-01")) %>%  # Filter for the year 2014
  mutate(Month = format(DateTime, "%Y-%m")) %>%                # Extract month from DateTime
  group_by(Ref, Month) %>%                                     # Group by station and month
  summarise(UHI = mean(UHI, na.rm = TRUE)) %>%                 # Calculate monthly average UHI
  ungroup() %>%                                                # Ungroup the data
  mutate(Month = as.Date(paste0(Month, "-01")))                # Convert Month to Date object

# Check the number of unique city center stations
# This step calculates the number of unique city center stations to adjust the plot layout accordingly
unique_stations <- unique(cc_data_monthly_avg$Ref)
num_stations <- length(unique_stations)
print(paste("Number of city center stations:", num_stations))

# Create the lattice plot with enhanced clarity
# The xyplot function from the lattice package is used to create a plot showing the monthly UHI effect for each city center station
xyplot(UHI ~ Month | Ref, data = cc_data_monthly_avg, type = c("p", "r"),
       pch = 16, cex = 0.7, col = "blue", # Enhance point clarity
       layout = c(5, ceiling(num_stations / 5)), # Adjust layout to fit the number of stations
       as.table = TRUE, 
       xlab = "Month", 
       ylab = "UHI Effect", 
       main = "Average Monthly UHI Effect by City Center Station (2014)",
       scales = list(x = list(rot = 45))) # Rotate x-axis labels for better readability

```
The differences that develop between an urbanizing area and the rural landscape greatly depend on the synoptic conditions. They are in essence a differentiation of topoclimates and as such depend on dissimilarity of radiative fluxes and turbulent exchanges. These contrasts are largest in clear, calm conditions. They tend to disappear in cloudy and windy weather.(Landsberg, H. E. (1981))

Hence, in order to observe the maximum urban heat island intensity, air temperature data was filtered according to different weather parameters, as the literature has already suggested that the urban heat island would be a maximum under clear and calm nights. As a result, air temperature data was selected for clear and calm conditions with the following criteria:

1. Rostherne wind speed less than or equal to 5 knots
2. Rostherne cloud level less than or equal to 2 oktas


## Graphs showing the Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for City Center Stations
# This block of code filters the dataset for clear and calm weather conditions during the summer months, 
# then calculates and visualizes the mean Urban Heat Island (UHI) intensity daily profile for city center stations.
# The resulting graph shows how UHI intensity varies throughout the day under specific weather conditions.

```{r}

# Filter the data for clear and calm conditions
# The data is filtered to include only observations with wind speeds <= 5 m/s and cloud cover <= 2 oktas, which are considered clear and calm conditions
clear_calm_data <- long_data %>%
  filter(wind_speed <= 5 & cld_ttl_amt_id <= 2)

# Filter for summer months (June, July, August)
# The data is further filtered to include only observations from the summer months (June, July, August)
summer_data <- clear_calm_data %>% filter(month(DateTime) %in% c(6, 7, 8))

# Extract hour of the day
# The Hour column is extracted from the DateTime to facilitate grouping by time of day
summer_data$Hour <- format(summer_data$DateTime, "%H:%M:%S")

# Calculate mean UHI intensity for each hour of the day for city center stations
# The data is filtered for city center stations (stations with references starting with "CC") and grouped by hour
# The mean UHI intensity is calculated for each hour across all relevant observations
mean_uhi_hourly <- summer_data %>%
  filter(grepl("^CC", Ref)) %>%
  group_by(Hour) %>%
  summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# Plot the mean UHI intensity daily profile for city center stations
# The ggplot function creates a plot showing the mean UHI intensity throughout the day, with time on the x-axis and UHI intensity on the y-axis
ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
  geom_point(color = "red", alpha = 0.5) +
  labs(title = 'Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for City Center Stations',
       x = 'Time of day',
       y = 'UHI Intensity (°C)') +
  theme_minimal() +
  scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```





## Graphs showing the Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for all stations
# This block of code calculates and visualizes the mean Urban Heat Island (UHI) intensity daily profile for all stations during the summer months under clear and calm conditions.
# The resulting graph provides insight into how UHI intensity varies throughout the day across all stations.
```{r}
# Calculate mean UHI intensity for each hour of the day for all stations
# The data is grouped by hour, and the mean UHI intensity is calculated for each hour across all stations under the specified conditions

mean_uhi_hourly <- summer_data %>%
  group_by(Hour) %>%
  summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# Plot the mean UHI intensity daily profile for all stations
# The ggplot function creates a plot showing the mean UHI intensity throughout the day, with time on the x-axis and UHI intensity on the y-axis
ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
  geom_point(color = "blue", alpha = 0.5) +
  labs(title = 'Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for All Stations',
       x = 'Time of day',
       y = 'UHI Intensity (°C)') +
  theme_minimal() +
  scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```




# This block of code defines a function to calculate and plot the mean Urban Heat Island (UHI) intensity daily profile for city center stations during a specified season.
# The function is then applied to different seasons to visualize how UHI intensity varies throughout the day in winter, spring, and fall under clear and calm conditions.

```{r}

# Define a function to plot UHI intensity for a given season for city center stations
# This function filters the data for the specified season and city center stations, calculates the mean UHI intensity for each hour, and plots the daily profile.
plot_uhi_intensity_cc <- function(data, season_months, season_name, color) {
  # Filter the data for the given season and city center stations
  season_data <- data %>%
    filter(month(DateTime) %in% season_months & grepl("^CC", Ref))
  
  # Extract hour of the day
  season_data$Hour <- format(season_data$DateTime, "%H:%M:%S")
  
  # Calculate mean UHI intensity for each hour of the day
  mean_uhi_hourly <- season_data %>%
    group_by(Hour) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))
  
  # Plot the mean UHI intensity daily profile
  # The ggplot function creates a plot showing the mean UHI intensity throughout the day for the specified season
  ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
    geom_point(color = color, alpha = 0.5) +
    labs(title = paste('Mean UHI Intensity Daily Profile of Clear and Calm Conditions During', season_name, 'for City Center Stations'),
         x = 'Time of day',
         y = 'UHI Intensity (°C)') +
    theme_minimal() +
    scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Filter the data for clear and calm conditions
# The data is filtered to include only observations with wind speeds <= 5 m/s and cloud cover <= 2 oktas, which are considered clear and calm conditions
clear_calm_data <- long_data %>%
  filter(wind_speed <= 5 & cld_ttl_amt_id <= 2)

# Plot UHI intensity for each season for city center stations
# The function is called with different seasonal parameters to generate UHI intensity profiles for winter, spring, and fall
plot_uhi_intensity_cc(clear_calm_data, c(12, 1, 2), "Winter", "blue")
plot_uhi_intensity_cc(clear_calm_data, c(3, 4, 5), "Spring", "green")
plot_uhi_intensity_cc(clear_calm_data, c(9, 10, 11), "Fall", "orange")


```





# This block of code defines a function to calculate and plot the mean Urban Heat Island (UHI) intensity daily profile for all stations during a specified season.
# The function is then applied to different seasons to visualize how UHI intensity varies throughout the day in winter, spring, and fall under clear and calm conditions across all stations.

```{r}

# Define a function to plot UHI intensity for a given season for all stations
# This function filters the data for the specified season, calculates the mean UHI intensity for each hour, and plots the daily profile for all stations.
plot_uhi_intensity_all <- function(data, season_months, season_name, color) {
  # Filter the data for the given season
  # The data is filtered by the specified months
  season_data <- data %>% filter(month(DateTime) %in% season_months)
  
  # Extract hour of the day
  # The Hour column is extracted to facilitate grouping by time of day
  season_data$Hour <- format(season_data$DateTime, "%H:%M:%S")
  
  # Calculate mean UHI intensity for each hour of the day
  # The mean UHI intensity is calculated for each hour across all relevant observations
  mean_uhi_hourly <- season_data %>%
    group_by(Hour) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))
  
  # Plot the mean UHI intensity daily profile
  # The ggplot function creates a plot showing the mean UHI intensity throughout the day for the specified season
  ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
    geom_point(color = color, alpha = 0.5) +  # Plot points with the specified color and semi-transparency
    labs(title = paste('Mean UHI Intensity Daily Profile of Clear and Calm Conditions During', season_name, 'for All Stations'),
         x = 'Time of day',
         y = 'UHI Intensity (°C)') +
    theme_minimal() +
    scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +  # Format the x-axis to show time in hours and minutes
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
}

# Plot UHI intensity for each season for all stations
# The function is called with different seasonal parameters to generate UHI intensity profiles for winter, spring, and fall across all stations
plot_uhi_intensity_all(clear_calm_data, c(12, 1, 2), "Winter", "blue")
plot_uhi_intensity_all(clear_calm_data, c(3, 4, 5), "Spring", "green")
plot_uhi_intensity_all(clear_calm_data, c(9, 10, 11), "Fall", "orange")

```





##SVF


# This block of code analyzes the relationship between Urban Heat Island (UHI) intensity and Sky View Factor (SVF) during clear and calm nights across different seasons.
# It filters the data by season, calculates the average UHI for each site, performs a linear regression with SVF as the predictor, and visualizes the results.

```{r}
# Filter the data for clear and calm conditions
# The data is filtered to include only observations with cloud cover <= 2 oktas and wind speeds <= 5 m/s, which are considered clear and calm conditions

clear_calm_conditions <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5)

# Define the seasons
# The seasons are defined by specifying the corresponding months for spring, summer, autumn, and winter

seasons <- list(
  spring = c(3, 4, 5),
  summer = c(6, 7, 8),
  autumn = c(9, 10, 11),
  winter = c(12, 1, 2)
)

# Function to process data and create plots for each season
# This function filters the data for a specific season and night hours, calculates the average UHI for each site, 
# performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship
process_season <- function(season_name, months) {
  
  season_nights <- clear_calm_conditions %>%
    filter(month(DateTime) %in% months & (hour(DateTime) >= 18 | hour(DateTime) < 6))
  
  # Calculate the overall average UHI for each site
  season_avg <- season_nights %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 
  lm_season <- lm(avg_UHI ~ SVF, data = season_avg)
  summary_season <- summary(lm_season)
  equation_season <- paste("y = ", round(coef(lm_season)[1], 2), " + ", round(coef(lm_season)[2], 2), "x", sep = "")
  r_squared_season <- round(summary_season$r.squared, 3)
  
  # Plot 
  plot <- ggplot(season_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(title = paste("Average UHI Intensity vs Sky View Factor (SVF) during", season_name, "Nights"),
         x = "Sky View Factor (SVF)", y = "Average UHI Intensity (°C)") +
    annotate("text", x = 0.25, y = 1.5, 
             label = paste(equation_season, "\nR² = ", r_squared_season), 
             hjust = 0, vjust = 0, size = 4, color = "black") +
    theme_minimal() +
    theme(legend.position = "none")
  
  return(plot)
}

# Generate plots for each season before removing the point
# The process_season function is applied to each season, generating a plot for each
plots_before <- lapply(names(seasons), function(season_name) {
  process_season(season_name, seasons[[season_name]])
})

# Display the plots for each season
# The plots for spring, summer, autumn, and winter are displayed
plots_before[[1]] # Spring 
plots_before[[2]] # Summer 
plots_before[[3]] # Autumn 
plots_before[[4]] # Winter 

```


# Identify the station with the smallest SVF and print its details
```{r}
# Identify the station with the smallest SVF and print its details
smallest_svf_station <- long_data[which.min(long_data$SVF), ]
print(smallest_svf_station)
```



# This block of code removes the station with the smallest Sky View Factor (SVF) from the analysis and then re-evaluates the relationship between Urban Heat Island (UHI) intensity and SVF during clear and calm nights across different seasons.
# The updated analysis is visualized with new plots that exclude the removed station (CC5).

```{r}
# Remove the point with the smallest SVF
# The dataset is filtered to exclude the station 'CC5', which has the smallest SVF
long_data <- long_data %>% filter(Ref != 'CC5')

# Filter the data for clear and calm conditions after removing CC5
# The data is filtered to include only observations with cloud cover <= 2 oktas and wind speeds <= 5 m/s, after excluding the station with the #smallest SVF
clear_calm_conditions_filtered <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5)

# Function to process data and create plots for each season after removing CC5
# This function filters the data for a specific season and night hours, calculates the average UHI for each site, performs a linear regression with SVF as the predictor,
# and creates a plot to visualize the relationship, after the removal of the station with the smallest SVF
process_season_filtered <- function(season_name, months) {

  # Filter the data for the given season and nighttime hours (6 PM to 6 AM)
  season_nights <- clear_calm_conditions_filtered %>%
    filter(month(DateTime) %in% months & (hour(DateTime) >= 18 | hour(DateTime) < 6))
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  season_avg <- season_nights %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 # Perform linear regression of average UHI against SVF
  lm_season <- lm(avg_UHI ~ SVF, data = season_avg)
  summary_season <- summary(lm_season)
  equation_season <- paste("y = ", round(coef(lm_season)[1], 2), " + ", round(coef(lm_season)[2], 2), "x", sep = "")
  r_squared_season <- round(summary_season$r.squared, 3)
  
  # Plot the updated relationship between UHI and SVF after removing CC5
  plot <- ggplot(season_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(title = paste("Average UHI Intensity vs Sky View Factor (SVF) during", season_name, "Nights (After Removal)"),
         x = "Sky View Factor (SVF)", y = "Average UHI Intensity (°C)") +
    annotate("text", x = 0.25, y = 1.5, 
             label = paste("Equation: ", equation_season, "\nR² = ", r_squared_season), 
             hjust = 0, vjust = 0, size = 5, color = "black") +
    theme_minimal() +
    theme(legend.position = "none")
  
  return(plot)
}

# Generate plots for each season after removing the point
# The process_season_filtered function is applied to each season, generating a plot for each after the removal of CC5
plots_after <- lapply(names(seasons), function(season_name) {
  process_season_filtered(season_name, seasons[[season_name]])
})

# Display the plots for each season after removal
# The updated plots for spring, summer, autumn, and winter are displayed
plots_after[[1]] # Spring after removal
plots_after[[2]] # Summer after removal
plots_after[[3]] # Autumn after removal
plots_after[[4]] # Winter after removal


```



#Averaged UHI intensity against Sky View Factor on clear and calm nights
# This block of code analyzes the relationship between averaged Urban Heat Island (UHI) intensity and Sky View Factor (SVF) on clear and calm nights.
# It filters the dataset for clear and calm nighttime conditions, calculates the average UHI for each site, performs a linear regression, and visualizes the results.

```{r} 

# Define clear and calm nights
# The dataset is filtered to include only observations with cloud cover <= 2 oktas, wind speeds <= 5 m/s, and nighttime hours (6 PM to 6 AM)

clear_calm_nights <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5 & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Function to create the plot for SVF
# This function calculates the average UHI for each site, performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship

create_svf_plot <- function(data) {
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  svf_avg <- data %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
  # Perform linear regression of average UHI against SVF
  lm_svf <- lm(avg_UHI ~ SVF, data = svf_avg)
  summary_svf <- summary(lm_svf)
  equation_svf <- paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x", sep = "")
  r_squared_svf <- round(summary_svf$r.squared, 3)
  
  # Create a plot to visualize the relationship between UHI and SVF
  plot <- ggplot(svf_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(x = "Sky View Factor (SVF)", 
      y = "Average UHI Intensity (°C)"
    ) +
    annotate("text", x = 0.45, y = max(svf_avg$avg_UHI, na.rm = TRUE) * 0.5, 
             label = paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x\nR² = ", round(summary(lm_svf)$r.squared, 3)), 
             size = 5, color = "black", fontface = "italic") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size
      axis.title.x = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
      panel.grid.minor = element_blank()
    )
  
  return(plot)
}

# Generate the SVF plot
# The create_svf_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and SVF

svf_plot <- create_svf_plot(clear_calm_nights)

# Display the plot
print(svf_plot)


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p4.png", plot = svf_plot, width = 8, height = 4, units = "in", dpi = 300)



```


# This block of code focuses on analyzing the relationship between averaged Urban Heat Island (UHI) intensity and Sky View Factor (SVF) for sites with SVF values less than 0.65.
# It filters the dataset for clear and calm nighttime conditions and SVF < 0.65, calculates the average UHI for each site, performs a linear regression, and visualizes the results.

```{r}

# Function to create the plot for SVF with SVF < 0.65
# This function filters the data for SVF values less than 0.65, calculates the average UHI for each site, performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship

create_svf_plot <- function(data) {
  
  # Filter for SVF < 0.65
  # The dataset is filtered to include only observations with SVF values less than 0.65
  filtered_data <- data %>% 
    filter(SVF < 0.65)
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  svf_avg <- filtered_data %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 # Perform linear regression of average UHI against SVF
  lm_svf <- lm(avg_UHI ~ SVF, data = svf_avg)
  summary_svf <- summary(lm_svf)
  
  # Create a plot to visualize the relationship between UHI and SVF for SVF < 0.65
  plot <- ggplot(svf_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(x = "Sky View Factor (SVF)", 
      y = "Average UHI Intensity (°C)"
    ) +
    annotate("text", x = 0.35, y = max(svf_avg$avg_UHI, na.rm = TRUE) * 0.75, 
             label = paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x\nR² = ", round(summary(lm_svf)$r.squared, 3)), 
             size = 5, color = "black", fontface = "italic") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size
      axis.title.x = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
      panel.grid.minor = element_blank()
      )
  
  return(plot)
}

# Generate the SVF plot with SVF < 0.65
# The create_svf_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and SVF for SVF < 0.65

svf_plot <- create_svf_plot(clear_calm_nights)

# Display the plot
print(svf_plot)


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p5.png", plot = svf_plot, width = 8, height = 4, units = "in", dpi = 300)

```



#EF

#Averaged UHI intensity against evapotranspiration fraction on clear and calm nights

```{r}

# Function to create the plot for EF
# This function calculates the average UHI intensity for each site based on the Evapotranspiration Fraction (EF), 
# performs a linear regression of UHI against EF, and creates a plot to visualize the relationship.

create_ef_plot <- function(data) {
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and EF, and the average UHI is calculated for each group.
  ef_avg <- data %>%
    group_by(Ref, EF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
  # Perform linear regression of average UHI against EF
  lm_ef <- lm(avg_UHI ~ EF, data = ef_avg)
  summary_ef <- summary(lm_ef)
  equation_ef <- paste("y = ", round(coef(lm_ef)[1], 2), " + ", round(coef(lm_ef)[2], 2), "x", sep = "")
  r_squared_ef <- round(summary_ef$r.squared, 3)
  
  # Create a plot to visualize the relationship between UHI and EF
  plot <- ggplot(ef_avg, aes(x = EF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +  # Plot points with transparency and size
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +  # Add a linear regression line
    labs(
      title = "Averaged UHI intensity against Evapotranspiration Fraction on clear and calm nights", 
      x = "Evapotranspiration Fraction (EF)", 
      y = "Average UHI Intensity (°C)"
    ) +
    annotate("text", x = 0.7, y = max(ef_avg$avg_UHI, na.rm = TRUE) * 0.9, 
             label = paste("y = ", round(coef(lm_ef)[1], 2), " + ", round(coef(lm_ef)[2], 2), "x\nR² = ", round(summary(lm_ef)$r.squared, 3)), 
             size = 4, color = "black", fontface = "italic") +  # Annotate the plot with the regression equation and R-squared value
    theme_minimal() +
    theme(
      legend.position = "none",  # Remove the legend for clarity
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size and formatting
      axis.title.x = element_text(size = 10, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 10, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 8, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 8, face = "plain")  # Axis text size and not bold
    )
  
  return(plot)
}

# Generate the EF plot
# The create_ef_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and EF
ef_plot <- create_ef_plot(clear_calm_nights)

# Display the plot
print(ef_plot)



```



# This block of code analyzes the relationship between wind speed and averaged Urban Heat Island (UHI) intensity.
# It calculates the average UHI for each wind speed value, creates a plot to visualize the relationship, and saves the plot as a PNG file.

```{r}


# Calculate average UHI intensity for each wind speed value
# The data is grouped by wind speed, and the average UHI is calculated for each group, excluding NA values

avg_uhi_by_wind_speed <- long_data %>%
  group_by(wind_speed) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE)) %>%
  ungroup()

# Create a plot to visualize the relationship between wind speed and average UHI intensity
# The ggplot function creates a scatter plot with points and a line connecting them, as well as a smooth curve to highlight trends
p6 <- ggplot(avg_uhi_by_wind_speed, aes(x = wind_speed, y = avg_UHI)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  geom_smooth(method = "loess", se = FALSE) +  
  labs(x = "Wind Speed (knots)", y = "Average UHI Intensity (°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
  )



# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p6.png", plot = p6, width = 8, height = 4, units = "in", dpi = 300)



```









```{r}

# Filter data for clear nights (all stations)
clear_nights <- long_data %>%
  filter(cld_ttl_amt_id <= 2  & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Calculate hourly averages for each wind speed value during clear nights
hourly_avg <- clear_nights %>%
  mutate(hour = floor_date(DateTime, "hour")) %>%
  group_by(hour, wind_speed) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Linear regression model
linear_model <- lm(avg_UHI ~ wind_speed, data = hourly_avg)
linear_model_summary <- summary(linear_model)
linear_coeff <- coefficients(linear_model)
linear_eq <- paste0("y = ", round(linear_coeff[1], 2), " + ", round(linear_coeff[2], 2), "x")

# Plot with stronger colors: vibrant blue for points and deep black for the line
ggplot(hourly_avg, aes(x = wind_speed, y = avg_UHI)) +
  geom_point(alpha = 0.8, color = "#1E90FF") +  # Dodger Blue points
  geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +  # Black line
  labs(title = "Averaged UHI Intensity against Wind Speed on Clear Nights",
       x = "Wind Speed (knots)", y = "UHI Intensity (°C)") +
  annotate("text", x = max(hourly_avg$wind_speed) * 0.6, y = max(hourly_avg$avg_UHI) - 1, 
           label = paste("Linear Fit:\nR² = ", round(linear_model_summary$r.squared, 4), "\n", linear_eq), color = "black", hjust = 0) +  
  theme_minimal()

print(linear_model_summary)
```

Cloud cover is measured in oktas, which are units of eighths of the sky covered by clouds. For example, 0 oktas means no cloud cover, 4 oktas means half of the sky is covered with clouds, and 8 oktas means the sky is fully covered with clouds. If the value is 9, it indicates that the sky is completely obscured, meaning that it's not possible to see the sky to determine the cloud cover.




# This block of code analyzes the relationship between cloud cover levels (in oktas) and averaged Urban Heat Island (UHI) intensity.
# It handles special cases where the sky is obscured, calculates the average UHI for each cloud cover bin, visualizes the results, and saves the plot as a PNG file.

```{r}

# Handle the special value 9 for sky obscured
# A new column 'cloud_cover_bin' is created to handle the special value '9' which indicates that the sky is obscured. This value is labeled separately.

combined_data <- long_data %>%
  mutate(cloud_cover_bin = ifelse(cld_ttl_amt_id == 9, "9 (Sky Obscured)", as.character(cld_ttl_amt_id)))

# Calculate average UHI intensity for each cloud cover bin, excluding "9 (Sky Obscured)"
# The data is filtered to exclude the "9 (Sky Obscured)" category, grouped by cloud cover level, and the average UHI is calculated for each group.
avg_uhi_by_cloud_bin <- combined_data %>%
  filter(cloud_cover_bin != "9 (Sky Obscured)") %>%  # Exclude the "9 (Sky Obscured)" category
  group_by(cloud_cover_bin) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE))

# Convert cloud_cover_bin to a factor with levels in the correct order
# The cloud cover bins are converted to a factor to ensure that they are ordered correctly on the x-axis.
avg_uhi_by_cloud_bin$cloud_cover_bin <- factor(avg_uhi_by_cloud_bin$cloud_cover_bin, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8"))

# Plot the data
# The ggplot function creates a scatter plot with points and a line connecting them, visualizing the relationship between cloud cover level and average UHI intensity.
p7 <- ggplot(avg_uhi_by_cloud_bin, aes(x = cloud_cover_bin, y = avg_UHI)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  labs(x = "Cloud Cover Level (oktas)", y = "UHI Intensity (°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
  )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution.
ggsave("p7.png", plot = p7, width = 8, height = 4, units = "in", dpi = 300)


# The 'cloud_cover_bin' column is removed from the combined dataset if it's no longer needed.
combined_data <- combined_data %>%
  dplyr::select(-cloud_cover_bin)

```


# This block of code analyzes the relationship between cloud cover levels and averaged Urban Heat Island (UHI) intensity during calm nights.
# It filters the dataset for calm nights, calculates hourly average UHI for each cloud cover level, fits a linear regression model, and visualizes the results.

```{r}

# Filter data for calm nights (all sites, all year) with wind speed <= 5 knots
# The dataset is filtered to include only nighttime observations (6 PM to 6 AM) with wind speeds <= 5 knots, considered calm nights.

calm_nights <- long_data %>%
  filter(wind_speed <= 5 & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Calculate hourly averages for each cloud cover level without averaging cloud cover
# The DateTime column is rounded to the nearest hour, and the data is grouped by hour and cloud cover level (cld_ttl_amt_id).
# The average UHI is then calculated for each group.
hourly_avg <- calm_nights %>%
  mutate(hour = floor_date(DateTime, "hour")) %>%
  group_by(hour, cld_ttl_amt_id) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Linear regression model
# A linear regression model is fitted to the data, with average UHI as the dependent variable and cloud cover level as the independent variable.

linear_model <- lm(avg_UHI ~ cld_ttl_amt_id, data = hourly_avg)
linear_coeffs <- coefficients(linear_model)
linear_model_summary <- summary(linear_model)

# Plot
ggplot(hourly_avg, aes(x = as.numeric(cld_ttl_amt_id), y = avg_UHI)) +
  geom_point(alpha = 0.7, color = "#696969") +  # Dim grey points for nighttime data
  geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
  scale_x_continuous(breaks = seq(0, 9, 1), limits = c(0, 9)) +
  labs(title = "Averaged UHI Intensity against Cloud Cover Level on Calm Nights",
       x = "Cloud Level (oktas)", y = "UHI Intensity (°C)") +
  annotate("text", x = 6, y = max(hourly_avg$avg_UHI) - 0.5, 
           label = paste("Linear Fit:\nR² = ", round(linear_model_summary$r.squared, 4), 
                         "\nEquation: y = ", round(linear_coeffs[1], 2), " + ", round(linear_coeffs[2], 4), "x"), 
           color = "black") +
  theme_minimal()

# Print the summary of the linear regression model
print(linear_model_summary)

```


# This block of code categorizes the wind speed and cloud cover levels, calculates the average Urban Heat Island (UHI) intensity for each combination of these categories, and filters the data to remove NA values.
```{r}
# Categorize wind speed and cloud cover levels
# The wind_speed is categorized into bins (e.g., "0-3", "3-6", etc.), and the cloud cover levels (cld_ttl_amt_id) are converted to factors.
avg_uhi_data <- long_data %>%
  mutate(wind_speed_cat = cut(wind_speed, 
                              breaks = c(0, 3, 6, 9, 12, 15, Inf), 
                              labels = c("0-3", "3-6", "6-9", "9-12", "12-15", "15+")),
         cloud_cat = as.factor(cld_ttl_amt_id))



# Calculate average UHI for each combination of cloud cover and wind speed category
# The data is grouped by cloud cover category and wind speed category, and the average UHI is calculated for each group, excluding NA values in UHI.
avg_uhi_data <- avg_uhi_data %>%
  filter(!is.na(UHI)) %>%  # Ensure no NA values in UHI
  group_by(cloud_cat, wind_speed_cat) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Filter out rows where wind_speed_cat is NA
# The data is further filtered to remove rows where the wind speed category is NA.
avg_uhi_data <- avg_uhi_data %>%
  filter(!is.na(wind_speed_cat))
```



#This script creates a 3D bar plot visualizing the relationship between cloud cover, wind speed categories, and average UHI intensity. The plot #uses a color gradient to distinguish wind speed categories and places the bars in a 3D space according to their corresponding cloud cover and #wind speed categories. Axis labels and grid lines are manually added for clarity, and the final plot is saved as a high-resolution PNG file.
```{r}

# Ensure wind_speed_cat is a factor with levels in the correct order (descending)
# This ensures that the wind speed categories are ordered correctly in descending order for plotting.
avg_uhi_data$wind_speed_cat <- factor(avg_uhi_data$wind_speed_cat, 
                                      levels = c("15+", "12-15", "9-12", "6-9", "3-6", "0-3"))

# Ensure cloud_cat is also a factor
# Convert the cloud cover levels into a factor to ensure proper handling in plots.
avg_uhi_data$cloud_cat <- factor(avg_uhi_data$cloud_cat)

# Convert wind_speed_cat and cloud_cat to numeric for plotting
# Numeric conversion is necessary for positioning the bars in the 3D plot.
avg_uhi_data$wind_speed_num <- as.numeric(avg_uhi_data$wind_speed_cat)
avg_uhi_data$cloud_num <- as.numeric(avg_uhi_data$cloud_cat)

# Filter out cloud cover category 9 (and potentially other unwanted categories)
# Remove any data corresponding to the cloud cover category '9' as it may represent an obscured sky or be irrelevant.

avg_uhi_data <- avg_uhi_data %>%
  filter(cloud_cat != "9")

# Create the 3D bar plot
open3d()

# Set up the color gradient for the bars
# A color gradient from light gray to black is used to differentiate bars corresponding to different wind speed categories.

colors <- colorRampPalette(c("gray80", "black"))(length(unique(avg_uhi_data$wind_speed_cat)))

# Map wind speed categories to color indices
color_map <- setNames(colors, unique(avg_uhi_data$wind_speed_cat))

# Create 3D bars
# Loop through each row of the data to create individual bars in the 3D space.
for (i in 1:nrow(avg_uhi_data)) {
  # Define the position and size of each bar
  x <- avg_uhi_data$cloud_num[i]
  y <- avg_uhi_data$wind_speed_num[i]
  z <- avg_uhi_data$avg_UHI[i]
  
  # Calculate the corners of the bar
  x_range <- c(x - 0.4, x + 0.4)
  y_range <- c(y - 0.4, y + 0.4)
  z_range <- c(0, z)
  
  # Define the vertices of the bar (a cuboid)
  vertices <- rbind(
    c(x_range[1], y_range[1], z_range[1]),  # Bottom face
    c(x_range[2], y_range[1], z_range[1]),
    c(x_range[2], y_range[2], z_range[1]),
    c(x_range[1], y_range[2], z_range[1]),
    
    c(x_range[1], y_range[1], z_range[2]),  # Top face
    c(x_range[2], y_range[1], z_range[2]),
    c(x_range[2], y_range[2], z_range[2]),
    c(x_range[1], y_range[2], z_range[2])
  )
  
  # Draw the quads to form the 3D bars
  quads3d(vertices[c(1, 2, 6, 5), ], color = color_map[as.character(avg_uhi_data$wind_speed_cat[i])], alpha = 1.0)  # Front
  quads3d(vertices[c(2, 3, 7, 6), ], color = color_map[as.character(avg_uhi_data$wind_speed_cat[i])], alpha = 1.0)  # Right
  quads3d(vertices[c(3, 4, 8, 7), ], color = color_map[as.character(avg_uhi_data$wind_speed_cat[i])], alpha = 1.0)  # Back
  quads3d(vertices[c(4, 1, 5, 8), ], color = color_map[as.character(avg_uhi_data$wind_speed_cat[i])], alpha = 1.0)  # Left
  quads3d(vertices[c(5, 6, 7, 8), ], color = color_map[as.character(avg_uhi_data$wind_speed_cat[i])], alpha = 1.0)  # Top
}

# Add Axes without automatic labels
#axes3d(edges = c("x--", "y--", "z--"), nticks = 0)

# Manually add axis labels for x and y, aligned with the axes
title3d(
  xlab = "", 
  ylab = "", 
  zlab = ""
)

# X-axis labels (Cloud Condition) along the axis, parallel to the axis
for (i in seq_along(levels(avg_uhi_data$cloud_cat))) {
  text3d(i, -0.5, 0, texts = levels(avg_uhi_data$cloud_cat)[i], adj = c(0.5, 0.5), color = "black")
}

# Y-axis labels (Wind Speed) along the axis, parallel to the axis
for (i in seq_along(levels(avg_uhi_data$wind_speed_cat))) {
  text3d(-0.5, i, 0, texts = levels(avg_uhi_data$wind_speed_cat)[i], adj = c(0.5, 0.5), color = "black")
}

# Z-axis labels (Avg UHI Intensity) on the back, rotated 90 degrees and parallel to the axis
z_values <- pretty(range(avg_uhi_data$avg_UHI))
z_values <- z_values[z_values >= 0]
for (i in z_values) {
  text3d(0, max(avg_uhi_data$wind_speed_num) + 0.5, i, texts = as.character(i), adj = c(1, 0.5), color = "black")
}

# Add the axis titles parallel to the axes
text3d(mean(range(avg_uhi_data$cloud_num)), -1.5, 0, texts = "Cloud Condition", adj = c(1, 2.5), color = "black", rotation = c(0, 0, 0))
text3d(-1.5, mean(range(avg_uhi_data$wind_speed_num)), 0, texts = "Wind Speed (knots)", adj = c(1, 1), color = "black", rotation = c(0, 0, 0))
text3d(0, max(avg_uhi_data$wind_speed_num) + 0.5, mean(z_values), texts = "Avg UHI Intensity", adj = c(1.2, 1), color = "black", rotation = c(0, 90, 0))


# Add grid for better visual alignment
grid3d(c("x+", "y+", "z-"), col = "lightgray")

# Adjust the aspect ratio
aspect3d(1.5, 1.5, 1)

# Lock the labels to the axes
par3d(ignoreExtent = TRUE)



# Set the width and height in pixels (e.g., 1200x800) and resolution (e.g., 300 DPI)
rgl::rgl.snapshot(filename = "3d_plot_highres.png", fmt = "png")

```





# This block of code analyzes the relationship between an interaction term (wind speed multiplied by cloud cover) and averaged Urban Heat Island (UHI) intensity.
# It creates a new interaction variable, calculates the average UHI for each interaction level, and visualizes the relationship using a scatter plot.

```{r}

# Create an interaction term by multiplying wind speed and cloud cover
# A new variable 'wind_cloud_interaction' is created by multiplying wind speed and cloud cover levels.
data_with_interaction <- long_data %>%
  mutate(wind_cloud_interaction = wind_speed * cld_ttl_amt_id)

# Group by the interaction term and calculate the average UHI
# The data is grouped by the interaction term, and the average UHI is calculated for each group.
avg_uhi_interaction <- data_with_interaction %>%
  group_by(wind_cloud_interaction) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Plot the relationship between the interaction term and UHI
# The ggplot function creates a scatter plot showing the relationship between the wind speed-cloud cover interaction and average UHI intensity.
ggplot(avg_uhi_interaction, aes(x = wind_cloud_interaction, y = avg_UHI)) +
  geom_point(color = "#1E90FF") +  # Plot points with a specific color
  labs(title = "Mean UHI against Interaction of Wind Speed and Cloud Cover",
       x = "Wind Speed * Cloud Cover",
       y = "UHI Intensity (°C)") +
  theme_minimal()

```
# Save the dataset
```{r}

write.csv(long_data, file = "long_data.csv", row.names = FALSE)

```





## Apply log transformation to "glbl_irad_amt" and create a new dataset "long_data_norm" that is the normalized version of "long_data" dataset using Z-Score standardization

```{r}
# Copy the dataset for normalization
long_data_norm <- long_data

# Apply log transformation to glbl_irad_amt
long_data_norm$glbl_irad_amt <- log1p(long_data_norm$glbl_irad_amt)  # log1p to handle zero values

# Function to apply Z-score standardization
z_score <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Identify the numeric columns to be standardized, excluding Latitude and Longitude
numeric_columns <- names(long_data_norm)[sapply(long_data_norm, is.numeric)]
columns_to_standardize <- setdiff(numeric_columns, c("Latitude", "Longitude"))

# Apply Z-score standardization to the selected numeric columns
long_data_norm <- long_data_norm %>%
  mutate(across(all_of(columns_to_standardize), ~ z_score(.)))

# Print the resulting dataframe
print(long_data_norm)

```

# This block of code prepares and visualizes the distribution of various weather variables from the normalized dataset 'long_data_norm'.
# It selects specific weather-related variables, reshapes the data for plotting, and generates boxplots to illustrate the distribution and identify any outliers or patterns within these variables.

```{r}

# Select the weather variables you want to plot
# Define a vector of weather variable names that will be included in the analysis and plotting.
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                  "visibility", "air_temperature", "dewpoint",
                  "wetb_temp", "stn_pres", "rltv_hum", 
                   "glbl_irad_amt", "UHI")

# Subset the data to include only the weather variables
# Extract only the specified weather variables from the normalized dataset 'long_data_norm' for further analysis.
weather_data <- long_data_norm %>% dplyr::select(all_of(weather_vars))

# Convert the data from wide to long format
# Reshape the data from wide format (multiple columns for variables) to long format (two columns: 'Variable' and 'Value') to facilitate plotting with ggplot2.
weather_data_long <- weather_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Create the boxplots
ggplot(weather_data_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  labs(title = "Boxplots of Weather Variables in Rostherne_combined",
       x = "Weather Variables",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability


```


# This block of code filters the dataset to include only specific reference sites (Ref) and then creates a boxplot to visualize the distribution of Urban Heat Island (UHI) intensity for these sites.

```{r}

# Specify the specific Ref sites
# A vector of specific reference site identifiers is defined to filter the data for these particular locations.
specific_refs <- c("CC6", "E6", "N1", "NE5", "NW0", "S6", "SE2", "SW7", "W7")


# Filter the dataset to include only the specified Ref sites
# The dataset 'long_data' is filtered to retain only the rows corresponding to the specified reference sites.

filtered_data <- long_data %>%
  filter(Ref %in% specific_refs)

# Create a boxplot for UHI by site for the specific Ref sites
ggplot(filtered_data, aes(x = Ref, y = UHI)) +
  geom_boxplot() +
  labs(x = "Site (Ref)", y = "Urban Heat Island (UHI)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```





##Multicollinearity Assessment

To ensure that multicollinearity is not adversely affecting the estimates of the coefficients in the model, I assessed the Variance Inflation Factor (VIF) for all independent variables. VIF values indicate the degree to which the variance of an estimated regression coefficient increases due to multicollinearity. 
```{r}
# Fit a linear regression model to predict UHI intensity
# The model includes weather variables (e.g., wind direction, air temperature) and site-related variables (e.g., SVF, EF).

model0 <- lm(UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + cld_base_ht + 
            visibility + air_temperature + dewpoint + wetb_temp + stn_pres + 
            rltv_hum + glbl_irad_amt + SVF + EF, data = long_data_norm)

# Calculate VIF values
# Variance Inflation Factor (VIF) values are calculated to detect multicollinearity among the predictors in the model. 
# High VIF values indicate potential multicollinearity issues.
vif_values <- vif(model0)

# Print the VIF values
# The calculated VIF values are printed to identify which variables may be contributing to multicollinearity.
print(vif_values)

# Identify variables with high VIF values and calculate pairwise correlations
# Variables with high VIF values are selected, and their pairwise correlations are calculated to explore the extent of multicollinearity between them.

high_vif_vars <- long_data_norm %>%
  dplyr::select(air_temperature, dewpoint, wetb_temp, rltv_hum)

# Calculate the correlation matrix for the selected variables
# The correlation matrix provides insights into how strongly these variables are correlated with each other.
cor_matrix <- cor(high_vif_vars, use = "complete.obs")
print(cor_matrix)


```

To address the high multicollinearity, I removed the variables with the highest VIF values, specifically dewpoint and wetb_temp. The pairwise correlation matrix among variables with high VIF values confirmed high correlations, justifying their removal. 

```{r}

model_refined <- lm(UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + cld_base_ht + 
                    visibility + air_temperature + rltv_hum + stn_pres + wetb_temp +
                    glbl_irad_amt + SVF + EF, data = long_data_norm)

# Calculate VIF values for the refined model
vif_values_refined <- vif(model_refined)

# Print the VIF values
print(vif_values_refined)

```

All the VIF values are below the threshold of 10, indicating that multicollinearity is not a significant issue after the removal of dewpoint and wetb_temp. Therefore, the selected variables can be included in the linear mixed model without concerns for multicollinearity affecting the reliability of the coefficient estimates.












## Multiple Linear Regression Model




# This block of code prepares the data for a multiple linear regression analysis. 
# It involves logging the global irradiation variable to handle zero values, 
# filtering the data for a specific reference site (CC8), converting the DateTime column to the correct format, 
# splitting the dataset into training and testing sets, and verifying the splits and date ranges.

```{r}
# Log-transform the global irradiation variable
# The 'glbl_irad_amt' variable is log-transformed using log1p to handle zero values, ensuring they don't cause issues in the regression model.

long_data$glbl_irad_amt <- log1p(long_data$glbl_irad_amt)  # log1p to handle zero values

# Filter the data for the specific reference site (CC8)
# The dataset is filtered to include only the data corresponding to the reference site "CC8".
cc8_data <- long_data %>% filter(Ref == "CC8")

# Ensure the DateTime column is in POSIXct format
# The DateTime column is converted to POSIXct format to facilitate proper time-based operations and ordering.

cc8_data$DateTime <- as.POSIXct(cc8_data$DateTime, format = "%Y-%m-%d %H:%M:%S")
long_data$DateTime <- as.POSIXct(long_data$DateTime, format = "%Y-%m-%d %H:%M:%S")

# Sort the data by DateTime to ensure chronological order
# The data is sorted by the DateTime column to ensure that the training and testing sets are chronologically ordered.
long_data <- long_data %>% arrange(DateTime)
cc8_data <- cc8_data %>% arrange(DateTime)

# Determine the split point (70% of the data)
# The dataset is split into training and testing sets, with 70% of the data used for training and 30% for testing.

split_point <- floor(0.7 * nrow(cc8_data))
split_point_whole <-  floor(0.7 * nrow(long_data))

# Create training and testing sets for the CC8 data
train_set_1 <- cc8_data[1:split_point, ]
test_set_1 <- cc8_data[(split_point + 1):nrow(cc8_data), ]

# Create training and testing sets for the entire dataset
train_set_whole <- long_data[1:split_point_whole, ]
test_set_whole <- long_data[(split_point_whole + 1):nrow(long_data), ]

# Check the dimensions of the training and testing sets
# The dimensions of the training and testing sets are checked to confirm that the splits have been performed correctly.

dim(train_set_1)
dim(test_set_1)
dim(train_set_whole)
dim(test_set_whole)

# Verify the date range in the training set
# The date ranges of the training sets are verified to ensure they cover the intended period.

range(train_set_1$DateTime)
range(train_set_whole$DateTime)

# Verify the date range in the testing set
# The date ranges of the testing sets are verified to ensure they cover the intended period after the training data.

range(test_set_1$DateTime)
range(test_set_whole$DateTime)


```



# This block of code performs best subset selection to identify the optimal set of predictors for a multiple linear regression model predicting Urban Heat Island (UHI) intensity.
# It evaluates models based on the Bayesian Information Criterion (BIC) and Mallows' Cp, and visualizes the selection process by plotting BIC and Cp values against the number of predictors.

```{r}

# Define the formula
formula <- UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + 
                  cld_base_ht + air_temperature + dewpoint + wetb_temp + 
                  stn_pres + rltv_hum + glbl_irad_amt

# Perform the best subset selection
best_subset <- regsubsets(formula, data = train_set_1, nvmax = 10, method = "exhaustive")

# Get summary of the selection
subset_summary <- summary(best_subset)

# Identify the best models based on BIC and Cp
# The models with the lowest BIC and Cp values are identified as the best models according to these criteria.

best_bic_index <- which.min(subset_summary$bic)
best_cp_index <- which.min(subset_summary$cp)

png("p8.png", width = 8, height = 4, units = "in", res = 300)

# Set up the plotting area with two plots side by side
# Prepare the plotting area to display two plots (BIC and Cp) side by side.
par(mfrow = c(1, 2))

# Plot BIC with the best model highlighted
# A plot of BIC values against the number of predictors is created. The best model (with the lowest BIC) is highlighted with a red dot.

plot(subset_summary$bic, type="b", 
     xlab="Number of Variables", 
     ylab="BIC",
     cex.lab = 1.4,  # Increase size of x and y axis labels
     cex.main = 1.5, # Increase size of the main title
     cex.axis = 1.2  # Increase size of the axis tick labels (numbers)
)
grid(nx = 5, ny = 5, col = "lightgray", lty = "dotted")
points(best_bic_index, subset_summary$bic[best_bic_index], col="red", pch=19) # Add red dot for best BIC

# Plot Cp with the best model highlighted
# A plot of Cp values against the number of predictors is created. The best model (with the lowest Cp) is highlighted with a blue dot.

plot(subset_summary$cp, type="b", 
     xlab="Number of Variables", 
     ylab="Cp",
     cex.lab = 1.4,  # Increase size of x and y axis labels
     cex.main = 1.5, # Increase size of the main title
     cex.axis = 1.2  # Increase size of the axis tick labels (numbers)
)
grid(nx = 5, ny = 5, col = "lightgray", lty = "dotted")
points(best_cp_index, subset_summary$cp[best_cp_index], col="blue", pch=19) # Add blue dot for best Cp

# Reset the plotting area to the default
# After plotting, the plotting area is reset to the default single plot layout.
par(mfrow = c(1, 1))


```





# This block of code visualizes the results of the best subset selection process for a multiple linear regression model. 
# It creates plots to show the performance of different models based on BIC and Cp criteria, allowing for easy comparison of model selection.

```{r}
png("p9.png", width = 8, height = 4, units = "in", res = 300)

# Set up the plotting area to have 1 row and 2 columns
# The plotting area is configured to display two plots side by side, making it easier to compare the results for BIC and Cp.
par(mfrow = c(1, 2))

# Plot the selection summary for BIC
# This plot shows the best subset selection results using BIC as the selection criterion. Models with lower BIC values are preferred.
plot(best_subset, scale = "bic", 
     cex.lab = 1.4,  # Increase the size of the x and y axis labels for better readability
     cex.main = 1.5, # Increase the size of the main title
     cex.axis = 1.2) # Increase the size of the axis tick labels (numbers)

# Plot the selection summary for Cp
# This plot shows the best subset selection results using Cp as the selection criterion. Models with lower Cp values are preferred.
plot(best_subset, scale = "Cp", 
     cex.lab = 1.4,  # Increase the size of the x and y axis labels for better readability
     cex.main = 1.5, # Increase the size of the main title
     cex.axis = 1.2) # Increase the size of the axis tick labels (numbers)

# Reset the plotting area to the default
# The plotting area is reset to the default layout (a single plot) after displaying the two comparison plots.
par(mfrow = c(1, 1))

```


# This block of code fits a linear regression model using the predictors identified as the best subset based on BIC or Cp. 
# The model is then summarized to provide detailed information on the coefficients, statistical significance, and overall model performance.
```{r}

# Fit the model using the best subset
# A linear regression model is fitted using the selected predictors that were identified as optimal in the best subset selection process.

best_model <- lm( UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + cld_base_ht + air_temperature + wetb_temp + stn_pres + rltv_hum + glbl_irad_amt, data = train_set_1)

# Summary of the fitted model
summary(best_model)

```
# This block of code generates predictions for the Urban Heat Island (UHI) intensity using the test set on CC8 only based on the best-fit model.
# The predictions are then added to the test set for comparison with the actual UHI values.

```{r}
# Generate predictions for the test set
predictions <- predict(best_model, newdata = test_set_1)

# Add predictions to the test set for comparison
test_set_1$predicted_UHI <- predictions

```


# This block of code calculates key performance metrics for the linear regression model using the test set on CC8 only. 
# The metrics include Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared, which help evaluate the model's predictive accuracy.


```{r}

# Calculate RMSE
rmse <- sqrt(mean((test_set_1$UHI - test_set_1$predicted_UHI)^2))

# Calculate MAE
mae <- mean(abs(test_set_1$UHI - test_set_1$predicted_UHI))

# Calculate R-squared
r_squared <- 1 - sum((test_set_1$UHI - test_set_1$predicted_UHI)^2) / sum((test_set_1$UHI - mean(test_set_1$UHI))^2)
# Print the performance metrics
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("R-squared:", r_squared, "\n")


```



# This block of code evaluates the performance of the linear regression model on a test set that includes data from other locations, not just CC8.
# It generates predictions, calculates performance metrics (RMSE, MAE, R-squared), and prints the results for this broader dataset.

```{r}

# Generate predictions for the test set other locations
predictions_whole <- predict(best_model, newdata = test_set_whole)

# Add predictions to the test set for comparison
test_set_whole$predicted_UHI <- predictions_whole

# Calculate RMSE
rmse <- sqrt(mean((test_set_whole$UHI - test_set_whole$predicted_UHI)^2))

# Calculate MAE
mae <- mean(abs(test_set_whole$UHI - test_set_whole$predicted_UHI))

# Calculate R-squared
r_squared <- 1 - sum((test_set_whole$UHI - test_set_whole$predicted_UHI)^2) / sum((test_set_whole$UHI - mean(test_set_whole$UHI))^2)
# Print the performance metrics
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("R-squared:", r_squared, "\n")
```



# This block of code calculates performance metrics (MSE, RMSE, R², MAE) for the linear regression model at each location in the test set.
# It provides a detailed breakdown of the model's predictive accuracy by location, allowing for a more granular assessment of model performance.


```{r}
# Calculate RMSE, MAE and R² for each location
error_metrics_by_location <- test_set_whole %>%
  group_by(Ref) %>%
  summarise(
    MSE = mean((UHI - predicted_UHI)^2),
    RMSE = sqrt(MSE),
    R2 = 1 - sum((UHI - predicted_UHI)^2) / sum((UHI - mean(UHI))^2),
    MAE = mean(abs(UHI - predicted_UHI))         # Mean Absolute Error
  )

# View the error metrics per location
print(error_metrics_by_location)
```



# This block of code is designed to classify and select specific reference sites (`Ref`) based on their prefixes, ensuring that one representative site from each category is included. 
# It guarantees that "CC8" is always selected from the "CC" group, and then randomly selects one reference site from each of the other groups.

```{r}

# Create a column that identifies the `Ref` type based on prefixes
# A new column `Ref_type` is created to classify the references into groups based on their prefixes, such as "CC", "NE", "NW", etc.

long_data_Ref <- long_data %>%
  mutate(Ref_type = case_when(
    str_starts(Ref, "CC") ~ "CC",
    str_starts(Ref, "NE") ~ "NE",  # Check for "NE" first
    str_starts(Ref, "NW") ~ "NW",  # Check for "NW" next
    str_starts(Ref, "N") ~ "N",    # Then check for "N"
    str_starts(Ref, "E") ~ "E",
    str_starts(Ref, "SE") ~ "SE",
    str_starts(Ref, "SW") ~ "SW",  # Corrected case to "SW"
    str_starts(Ref, "S") ~ "S",
    str_starts(Ref, "W") ~ "W",
    TRUE ~ "Other"  # If there are any other types, they will be labeled as "Other"
  ))

# Convert `Ref` to character for selection, and then back to factor after selection
long_data_Ref$Ref <- as.character(long_data_Ref$Ref)

# Initialize an empty vector to store selected references
selected_refs <- c()

# Loop through each group and select 1 unique reference
for (ref_type in unique(long_data_Ref$Ref_type)) {
  
  if (ref_type == "CC") {
    # Ensure "CC8" is always included in the "CC" group
    selected_refs <- c(selected_refs, "CC8")
    
    
  } else {
    # Select 1 unique reference from other groups
    additional_refs <- long_data_Ref %>%
      filter(Ref_type == ref_type) %>%
      distinct(Ref) %>%  # Ensure distinct values
      slice_sample(n = 1) %>%
      pull(Ref)
    
    selected_refs <- c(selected_refs, additional_refs)
  }
}

# Ensure all selected references are unique
# This step removes any potential duplicates in the selected references.
selected_refs <- unique(selected_refs)

# Check the final selected references
# Print the selected references to verify the selection process.
print(selected_refs)


# Convert selected_refs back to factor with the original levels
# The selected references are converted back to a factor, ensuring that they retain the original levels from the `Ref` column.
selected_refs_f <- factor(selected_refs, levels = levels(long_data$Ref))

# Check the selected references
# Print the factorized selected references to verify they have been converted correctly.
print(selected_refs_f)


#In my case the random locations were these:
selected_refs <- c("CC8", "N3",  "NE3", "E2",  "SE2" ,"S6"  ,"SW3", "W5" , "NW5")

```




# This script calculates the Root Mean Squared Error (RMSE) for model predictions across different locations and visualizes these errors using a bar plot.
# It loops through each location, computes RMSE values, and creates a bar plot to compare prediction accuracy among locations.
```{r}
# Initialize an empty data frame to store RMSE values
rmse_df <- data.frame(Location = character(), RMSE = numeric(), stringsAsFactors = FALSE)

# Loop through each location in the selected_locations list
for (ref in selected_refs) {
  
  # Filter the data for the current location
  location_data <- test_set_whole %>% filter(Ref == ref)
  
  # Calculate RMSE for the current location
  rmse <- sqrt(mean((location_data$UHI - location_data$predicted_UHI)^2))
  
  # Store the results in the data frame
  rmse_df <- rmse_df %>% 
    add_row(Location = ref, RMSE = rmse)
}


# Create a bar plot of RMSE values for all locations
ggplot(rmse_df, aes(x = Location, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "black", color = "black", width = 0.6) +
  labs(x = "Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1),
    axis.title.x = element_text(size = 12, margin = margin(t = 10)),  # Set x-axis label size
    axis.title.y = element_text(size = 12, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  

```










# This script calculates the Root Mean Squared Error (RMSE) for model predictions grouped by letter-based categories of locations and visualizes these errors using a bar plot.
# It creates a new column to categorize locations based on their letter prefixes, calculates RMSE for each category, and plots the results.

```{r}
# Create a new column that extracts only the letters from the Ref column
# This helps to group locations based on their letter prefixes (e.g., CC, NE, etc.).
test_set_whole$LetterGroup <- gsub("[0-9]", "", test_set_whole$Ref)

# Calculate RMSE for each LetterGroup
# This summarizes RMSE values for each letter-based group to evaluate model performance across different categories.

error_by_group <- test_set_whole %>%
  group_by(LetterGroup) %>%
  summarise(
    RMSE = sqrt(mean((UHI - predicted_UHI)^2))  # Calculate RMSE for each group
  )

# View the error by group
# Print the calculated RMSE values for each letter-based group
print(error_by_group)



# Save the bar plot as a PNG file
png("RMSE_by_direction_regression.png", width = 8, height = 4, units = "in", res = 300)

# Create a bar plot of RMSE values for all letter-based groups
# This plot helps in comparing prediction errors across different location categories.
ggplot(error_by_group, aes(x = LetterGroup, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "#5e6a6f", color = "#5e6a6f", width = 0.6) +
  labs(x = "Group Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size=12, angle = 0, hjust = 1),
    axis.title.x = element_text(size = 15, margin = margin(t = 10)),  # Set x-axis label size
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(size = 15, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  
# Close the graphics device
dev.off()
```


# This block of code generates and saves combined plots for observed and predicted Urban Heat Island (UHI) intensity over specified sub-periods within the testing data range for a specific location.
# The function `create_combined_uhi_plot` is used to create these plots, which are then saved as image files.

```{r}

# Define the g_legend function to extract the legend from a ggplot
# This helper function extracts the legend from a ggplot object, which will be used to create a unified legend for the combined plots.

g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Define the sub-periods within the testing data range
time_ranges <- list(
  range1 = as.POSIXct(c("2015-06-15", "2015-06-21")),
  range2 = as.POSIXct(c("2015-08-15", "2015-08-21")),
  range3 = as.POSIXct(c("2015-10-15", "2015-10-21")),
  range4 = as.POSIXct(c("2015-12-15", "2015-12-21"))
)

# Function to create a combined plot for the station and time ranges
# This function generates a combined plot of observed and predicted UHI values for a specific station across the defined time ranges.

create_combined_uhi_plot <- function(ref, data, time_ranges) {
  station_data <- data %>% filter(Ref == ref)  # Filter data for the specific location
  
  plots <- lapply(time_ranges, function(time_range) {
    time_filtered_data <- station_data %>% filter(DateTime >= time_range[1] & DateTime <= time_range[2])
    ggplot(time_filtered_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size=0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size=0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
            legend.position = "bottom",
            legend.title = element_text(size = 12),  # Increase the size of the legend title
            legend.text = element_text(size = 12),
            axis.title.x = element_blank(),  # Increase x-axis label size
            axis.title.y = element_blank(),  # Increase y-axis label size
            axis.text.y = element_text(size = 12),   # Increase y-axis text size
            panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
            panel.grid.minor = element_blank())
  })
  
  # Extract legend from the first plot
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and legend into a single layout
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
  # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )
  
  return(combined_plot)
}


# Loop through each location in the selected_locations list
# The function is called in a loop to generate and save combined plots for the specified location(s).
#From the selected refs I will plot N3 because had the worst error among the other directions

for (ref in "N3") {
  
  # Generate and display the combined plot for each location
  combined_plot <- create_combined_uhi_plot(ref, test_set_whole, time_ranges)
  grid.newpage()
  grid.draw(combined_plot)

  ggsave("N3regr.png",plot=combined_plot,  width = 8, height = 4, units = "in", dpi = 300)
}


```
#This block calculates the performance of the regression model for each month in the test set
```{r}
#calculate the residuals
test_set_whole$residuals <- test_set_whole$UHI - test_set_whole$predicted_UHI

# Calculate MAE and RMSE by month
monthly_performancereg <- test_set_whole %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2)),
              nRMSE_by_range = (RMSE/16.05437 )* 100) %>%
    ungroup()

  # Ensure Month column is in Date format
monthly_performancereg <- monthly_performancereg %>%
    mutate(Month = as.Date(Month))

#plot the performance metrics per month
print(monthly_performancereg)
```






# This script generates scatter plots to visualize the relationship between observed and predicted UHI values for both training and test datasets.
# It also calculates and annotates the R-squared values on these plots to assess model performance.

# r2 scatter plot for train and test for the regression model
```{r}

# Generate predictions on the train set
# Using the best model to predict UHI values on the training dataset
train_set_1$predicted_UHI <- predict(best_model, newdata = train_set_1, allow.new.levels = TRUE)

# Calculate R-squared for the train set
# Fit a linear model with observed UHI as the response and predicted UHI as the predictor to get R-squared
R2_train <- summary(lm(UHI ~ predicted_UHI, data = train_set_1))$r.squared

# Scatter plot for the train set
# Plot observed vs. predicted UHI values for the training set and add a best-fit line
r_squared_plot_train <- ggplot(train_set_1, aes(x = UHI, y = predicted_UHI)) +
  geom_point(color = "skyblue", alpha = 0.7) +  # Use skyblue color for points
  geom_smooth(method = "lm", col = "black", se = FALSE) +
  labs(title = paste("Train Set R-squared: ", round(R2_train, 3)),
       x = "Observed UHI",
       y = "Predicted UHI") +
  theme_minimal()

# Display the R-squared plot for the train set
print(r_squared_plot_train)




# Generate predictions on the test set (assuming you already have them, but just in case)
#test_set_whole$predicted_UHI <- predict(best_model, newdata = test_set_whole, allow.new.levels = TRUE)


# Fit a linear model for the test set
# Model observed UHI as a function of predicted UHI to compute R-squared
lm_fit_test <- lm(predicted_UHI ~ UHI, data = test_set_whole)
# Extract the coefficients from the linear model
intercept_test <- coef(lm_fit_test)[1]
slope_test <- coef(lm_fit_test)[2]

# Calculate the R-squared value for the test set
# Measure the proportion of variance in UHI that is predictable from the model's predictions
R2_test <- 1 - sum((test_set_whole$UHI - test_set_whole$predicted_UHI)^2) / sum((test_set_whole$UHI - mean(test_set_whole$UHI))^2)

# Scatter plot for the test set
# Plot observed vs. predicted UHI values for the test set and add a best-fit line with annotations
r_squared_plot_test <- ggplot(test_set_whole, aes(x = UHI, y = predicted_UHI)) +
  geom_point(color = "skyblue", alpha = 0.7) +  # Use skyblue color for points
  geom_smooth(method = "lm", col = "black", se = FALSE) +
  labs(x = "Observed UHI",
       y = "Predicted UHI") +
  theme_minimal()+
  ylim(c(-6, 6)) +
  xlim(c(-6, 10)) +
  annotate("text", x = max(test_set_whole$UHI) - 1, y = max(test_set_whole$predicted_UHI)+0.25, 
           label = paste("y =", round(slope_test, 3), "* x +", round(intercept_test, 3)),
           hjust = 1, vjust = 0, size = 5, color = "black") +  # Formula of best-fit line
  annotate("text", x = min(test_set_whole$UHI)+2 , y = max(test_set_whole$predicted_UHI)+0.70, 
           label = paste("R² =", round(R2_test, 3)),
           hjust = 0, vjust = 1, size = 5, color = "black") +  # R-squared value in top left
  theme(
    plot.background = element_rect(fill = "white", color = "white"),
    panel.background = element_rect(fill = "white", color = "white"),  # Set all text elements to size 12
    axis.title = element_text(size = 15),  # Axis titles to size 12
    axis.text = element_text(size = 12),  # Axis tick labels to size 12
    plot.title = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.15),  # All grid lines the same color
    panel.grid.minor = element_blank()  # Plot title to size 12
  )


# Display the R-squared plot for the train set
print(r_squared_plot_test)

# Save the plot as a PNG file
# Save the test set scatter plot with annotations for R-squared and line formula

ggsave("r_squared_plot_test_regression.png", plot = r_squared_plot_test, width = 8, height = 4, units = "in", dpi = 300)
```







#Randomly find 4 locations, the 1st is CC8 the 2nd is in the selected ref dataset and the other 2 are from the reamining, also keep in mind that are from different directions
```{r}
# Select only the 'Ref' and 'Ref_type' columns from the 'long_data_Ref' dataframe
Refs_df <- dplyr::select(long_data_Ref, Ref, Ref_type)

# Create a dataframe with unique references and their types
Refs_df_unique <- Refs_df %>% distinct()

# Exclude the references that are already in the selected_refs
remaining_refs_df <- Refs_df_unique %>% filter(!Ref %in% selected_refs)

# Define the type of reference 'CC8' to be excluded
cc8_ref_type <- "CC"

# Remove all references of the same type as 'CC8' from the remaining references
remaining_refs_df <- remaining_refs_df[remaining_refs_df$Ref_type != cc8_ref_type, ]

# Set a seed for reproducibility of random sampling
set.seed(111)

# Remove 'CC8' from the set of remaining references
remaining_selected_refs <- setdiff(selected_refs, "CC8")

# Randomly select one reference from the remaining set
random_ref <- sample(remaining_selected_refs, 1)

# Determine the type of the randomly selected reference
random_ref_type <- Refs_df_unique$Ref_type[Refs_df_unique$Ref == random_ref]

# Remove all references of the same type as the randomly selected reference
remaining_refs_df <- remaining_refs_df[remaining_refs_df$Ref_type != random_ref_type, ]

# Get all unique reference types left in the remaining references
unique_ref_types <- unique(remaining_refs_df$Ref_type)

# Randomly select one reference type from the remaining types
first_ref_type_unknown <- sample(unique_ref_types, 1)
  
# Select one random Ref from the first Ref_type
first_ref_unknown <- sample(remaining_refs_df$Ref[remaining_refs_df$Ref_type == first_ref_type_unknown], 1)
  
# Exclude the first Ref_type from the list of remaining Ref_types
remaining_ref_types <- setdiff(unique_ref_types, first_ref_type_unknown)
  
# Randomly select the second Ref_type
second_ref_type_unknown  <- sample(remaining_ref_types, 1)
  
# Select one random Ref from the second Ref_type
second_ref_unknown <- sample(remaining_refs_df$Ref[remaining_refs_df$Ref_type == second_ref_type_unknown], 1)
  
# Combine the two selected Refs
final_refs <- c("CC8",random_ref,first_ref_unknown, second_ref_unknown)

print(final_refs)

#mine randomly sellected final plots
final_refs <- c("CC8" ,"SE1", "E5",  "SW3")

```







#The script takes the above random 4 locations and plot the 4 seasons
# This script generates and saves plots comparing observed and predicted UHI values for selected stations
# across different seasonal date ranges. It creates a combined plot for each season, showing how well the
# model's predictions match the actual UHI values.
```{r}

# Define the specific date ranges for plotting
date_ranges <- list(
  Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-09")),
  Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-09")),
  Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-09")),
  Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-06-04"))
)

# Function to create the combined plots for each date range
create_combined_plots <- function(date_range, season,test_data_with_pred,selected_stations) {
  
  # Filter data for the selected stations and date range
  selected_data <- test_data_with_pred %>%
    filter(Ref %in% selected_stations) %>%
    filter(DateTime >= date_range[1] & DateTime <= date_range[2])
  
  # Determine y-axis limits based on the range of observed and predicted values
  y_limits <- range(c(selected_data$UHI, selected_data$predicted_UHI), na.rm = TRUE)
  
  actual_vs_predicted_plot <- ggplot(selected_data, aes(x = DateTime)) +
    geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
    geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
    scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
    labs(color="Legend") +
    #scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
    #scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) +
    theme_minimal() +
    theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
          axis.text.y = element_text(size=12),
          axis.title.x = element_text(size=15,margin = margin(t = 20)),
          axis.title.y = element_text(size=15),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          legend.background = element_rect(fill = "white", color = "white"),
          legend.position = "bottom",
          legend.title = element_text(size = 12),  # Increase the size of the legend title
          legend.text = element_text(size = 12),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank() )+
    facet_wrap(~ Ref, ncol = 2)
  
  # Save the plot
  #ggsave(filename = paste0("UHI_Predicted_vs_Actual_", season, "_2015.png"), plot = actual_vs_predicted_plot, width = 16, height = 8)
  
  print(actual_vs_predicted_plot)
}

# Loop over each season and create/save the plots
for (season in names(date_ranges)) {
  file_name <- paste0("Plot_", season, "_Regression.png")
  
   # Generate the combined plot for the current season
  p<-create_combined_plots(date_ranges[[season]], season, test_set_whole, final_refs)
  
   # Save the plot to a file
  ggsave(file_name,plot=p,  width = 8, height = 4, units = "in", dpi = 300)

}

```









#Plot the four seasons for each ref separetely
# This script generates combined plots for observed and predicted UHI values
# across various time ranges for selected stations. It produces plots showing
# how well the model's predictions align with actual UHI values over different periods.
```{r}

# Define the sub-periods within the testing data range

time_ranges <- list(
  range1 = as.POSIXct(c("2015-05-27", "2015-06-04")),
  range2 = as.POSIXct(c("2015-08-01", "2015-08-09")),
  range3 = as.POSIXct(c("2015-10-01", "2015-10-09")),
  range4 = as.POSIXct(c("2015-12-01", "2015-12-09"))
)

# Function to extract the legend from a ggplot object
g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Function to create a combined plot for each selected station and time range
create_combined_uhi_plot <- function(ref, data, time_ranges, model_name) {
  # Calculate the y-axis limits based on the combined range of UHI and predicted_UHI across all time ranges
  combined_data <- data %>% filter(Ref == ref & DateTime >= min(sapply(time_ranges, `[`, 1)) & DateTime <= max(sapply(time_ranges, `[`, 2)))
  y_limits <- range(c(combined_data$UHI, combined_data$predicted_UHI), na.rm = TRUE)
  print(y_limits)
  
  # Generate a plot for each time range
  plots <- lapply(seq_along(time_ranges), function(i) {
    time_range <- time_ranges[[i]]
    station_data <- data %>% filter(Ref == ref & DateTime >= time_range[1] & DateTime <= time_range[2])
    p <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
      scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) + # Set consistent y-axis limits and grid lines
      theme_minimal() +
      theme(
        axis.text.x = element_text(size=12,angle = 0, hjust = 1),
        legend.position = "bottom",
        legend.title = element_text(size = 12),  # Increase the size of the legend title
        legend.text = element_text(size = 12),
        axis.title.x = element_blank(),  # Remove x-axis label
        axis.title.y = element_blank(),
        axis.text.y = element_text(size = 12),
        panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
        panel.grid.minor = element_blank()  # Remove minor grid lines
      )
    
    return(p)
  })
  
  # Extract the legend from one of the plots
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and add the legend at the bottom
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
   # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )

  
  return(combined_plot)
}


# Generate and display combined plots
for (ref in final_refs) {
  
  
  combined_plot <- create_combined_uhi_plot(ref, test_set_whole, time_ranges, "Model_Regression")
  grid.newpage()
  grid.draw(combined_plot)
  
}
```







# This function calculates and plots the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)
# on a monthly basis. It creates two plots: one for MAE and one for RMSE, visualizing how the model's
# performance varies over time. The plots are customized and saved as PNG files.
```{r}

calculate_and_plot_monthly_performance <- function(data, df_name) {
  
  # Calculate MAE and RMSE by month
  monthly_performance <- data %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2))) %>%
    ungroup()

  # Ensure Month column is in Date format
  monthly_performance <- monthly_performance %>%
    mutate(Month = as.Date(Month))

  # Plot monthly MAE
  mae_plot <- ggplot(monthly_performance, aes(x = Month, y = MAE)) +
    geom_line(color = "red", size = 0.5) +
    geom_point(color = "red", size = 0.5) +
    labs(x = "Month", y = "Mean Absolute Error (MAE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())

  # Save the MAE plot
  #mae_filename <- paste0(df_name, "_Monthly_MAE.png")
  #ggsave(mae_filename, plot = mae_plot, width = 12, height = 6, units = "in", dpi = 300)

  
  
  # Plot monthly RMSE
  rmse_plot <- ggplot(monthly_performance, aes(x = Month, y = RMSE)) +
    geom_line(color = "green", size = 0.5) +
    geom_point(color = "green", size = 0.5) +
    labs(x = "Month", y = "Root Mean Squared Error (RMSE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())
  
  # Save the RMSE plot
  #rmse_filename <- paste0(df_name, "_Monthly_RMSE.png")
  #ggsave(rmse_filename, plot = rmse_plot, width = 12, height = 6, units = "in", dpi = 300)


  # Print the plots
  print(mae_plot)
  print(rmse_plot)
}

calculate_and_plot_monthly_performance(test_set_whole, "Model Regression")
```






# This function processes a data frame by categorizing model performance based on residuals,
# and then creates and displays combined plots for specified stations and date ranges. It includes
# plots of actual vs. predicted UHI and model performance over time. The function also prints
# information about the data processing and plot creation steps.

```{r}

# Function to process a single data frame
process_data_frame <- function(df, final_refs, df_name) {
  
  # Convert Ref to factor
  df$Ref <- as.factor(df$Ref)

  # Calculate the mean and standard deviation of the residuals
  residual_mean <- mean(df$residuals)
  residual_sd <- sd(df$residuals)

  # Categorize performance using the standard deviation-based thresholds
  df <- df %>%
    mutate(Performance = case_when(
      abs(residuals - residual_mean) > 1.2 * residual_sd ~ "Poor",
      abs(residuals - residual_mean) > residual_sd ~ "Medium",
      TRUE ~ "Good"
    ))

  # Define the specific date ranges for plotting
  date_ranges <- list(
    Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-31")),
    Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-05-31")),
    Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-31")),
    Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-31"))
  )

  # Function to create the combined plots for each station and date range
  create_combined_plots <- function(station_ref, season) {
    print(paste("Creating plots for Station:", station_ref, "in", season, "2015"))

    station_data <- df %>%
      filter(Ref == station_ref) %>%
      filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])

    if (nrow(station_data) == 0) {
      print(paste("No data available for Station:", station_ref, "in", season, "2015"))
      return()
    }

    actual_vs_predicted_plot_period <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Actual UHI"), size = 1) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 1, linetype = "dashed") +
      labs(title = paste(df_name, "- Predicted vs. Actual UHI for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Actual UHI" = "skyblue", "Predicted UHI" = "red"))

    performance_points_plot_period <- ggplot(station_data, aes(x = DateTime, y = UHI, color = Performance)) +
      geom_point(size = 1, alpha = 0.6) +
      labs(title = paste(df_name, "- Model Performance Over Time for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Performance") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Poor" = "red", "Medium" = "orange", "Good" = "green"))
    
    grid.arrange(actual_vs_predicted_plot_period, performance_points_plot_period, ncol = 1)
  }

  # Loop over the selected stations and seasons to create the plots
  for (station_ref in final_refs) {
    for (season in names(date_ranges)) {
      print(paste("Checking data for Station:", station_ref, "in", season, "2015"))
      filtered_data <- df %>%
        filter(Ref == station_ref) %>%
        filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])
      
      print(paste("Number of records for Station:", station_ref, "in", season, "2015:", nrow(filtered_data)))
      
      create_combined_plots(station_ref, season)
    }
  }
}



# Apply the function to each data frame

process_data_frame(test_set_whole, final_refs, "Model Regression")

```







# This script calculates and displays seasonal performance metrics (MAE, MSE, RMSE) for model predictions across different seasons.
```{r}
# Function to calculate additional metrics
calculate_metrics <- function(actual, predicted) {
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  nrmse_range <- (rmse / (max(actual) - min(actual))) * 100
  list(MAE = mae, MSE = mse, RMSE = rmse, nRMSE_by_range = nrmse_range)
}

# Define seasons within the testing data range
seasons <- list(
  Spring = c("2015-05-27", "2015-05-31"),
  Summer = c("2015-06-01", "2015-08-31"),
  Fall = c("2015-09-01", "2015-11-30"),
  Winter = c("2015-12-01", "2015-12-31")
)


# Function to calculate seasonal errors for a single data frame
calculate_seasonal_errors <- function(data, df_name) {
  seasonal_errors <- lapply(names(seasons), function(season) {
    season_data <- data %>%
      filter(DateTime >= as.POSIXct(seasons[[season]][1]) & DateTime <= as.POSIXct(seasons[[season]][2]))
    metrics <- calculate_metrics(season_data$UHI, season_data$predicted_UHI)
    season_errors <- data.frame(DataFrame = df_name, Season = season, MAE = metrics$MAE, MSE = metrics$MSE, RMSE = metrics$RMSE)
    return(season_errors)
  })
  
  # Combine seasonal errors into a single data frame
  seasonal_errors_df <- bind_rows(seasonal_errors)
  return(seasonal_errors_df)
}

# Apply the function to each of the three data frames
regression_seasonal_errors <- calculate_seasonal_errors(test_set_whole, "Model Regression")


# Print the combined seasonal errors
print(regression_seasonal_errors)

```




#This script generates diagnostic plots to assess the residuals from a regression model fitted on the training dataset.
```{r}
# Compute residuals
train_set_1$residuals <-train_set_1$UHI - train_set_1$predicted_UHI

# Save the plots as a PNG file
png("4_plot_train_regression.png", width = 8, height = 4, units = "in", res = 300)

# Set up a 2x2 plotting area
par(mfrow = c(1, 2))  # Set up a 2x2 plotting area

# a) Q-Q Plot of Residuals
qqnorm(train_set_1$residuals, main = "(a)",  ylim = c(-10, 10), cex.axis = 1.2, cex.lab = 1.4)
qqline(train_set_1$residuals, col = "red")

# b) Histogram of Residuals
hist(train_set_1$residuals, main = "(b)", xlab = "Residuals", breaks = 20,cex.axis = 1.2, cex.lab = 1.4)


# Close the PNG device
dev.off()
```



















#Longitudinal models

##Linear Mixed-Effects Models 

The dataset "long_data" consists of hourly UHI measurements collected from various stations over a two-year period. Each station is characterized by specific environmental factors such as Sky View Factor (SVF) and Emissivity Factor (EF). Additionally, several weather-related variables of the rural site are included to capture their potential impact on UHI.

Longitudinal models are particularly suited for this type of data as they allow for the analysis of repeated measurements over time, accounting for within-station correlations. By fitting multiple longitudinal mixed-effects models, I can account for both the fixed effects of these predictors and the random effects associated with station-specific variations, providing a comprehensive understanding of the dynamics influencing the UHI effect across different locations and times.


```{r}
# Convert the 'Ref' column to a factor in the 'long_data_norm' data frame
long_data_norm$Ref <- as.factor(long_data_norm$Ref)

# Convert the 'Ref' column to a factor in the 'long_data' data frame
long_data$Ref <- as.factor(long_data$Ref)

# Display the structure 
str(long_data_norm)
str(long_data)

```



#This script converts the 'DateTime' column to POSIXct format, extracts date and time components, performs sine and cosine transformations for cyclic features, and sorts the data by 'DateTime'.
```{r}

# Convert DateTime to POSIXct format
long_data_norm$DateTime <- as.POSIXct(long_data_norm$DateTime)

# Extract Year, Month, Day, and Hour as factors and as numeric for sine/cosine transformations
long_data_norm <- long_data_norm %>%
  mutate(
    Year = factor(format(DateTime, "%Y")),
    Month = factor(format(DateTime, "%m")),
    Day = factor(format(DateTime, "%d")),
    Hour = factor(format(DateTime, "%H")),
    hour = hour(DateTime),
    day = yday(DateTime),
    sin_hour = sin(2 * pi * hour / 24),
    cos_hour = cos(2 * pi * hour / 24),
    sin_day = sin(2 * pi * day / 365.25),
    cos_day = cos(2 * pi * day / 365.25)
  )

# Ensure the data is sorted by DateTime
long_data_norm <- long_data_norm %>% arrange(DateTime)

```








#choose one sites from each location randomly (we did it for the previous model). We use the same as before for consistency
```{r}
# Check the selected refs
print(selected_refs)

```





#This script filters the dataset to include only selected references, splits the data into training and testing sets based on a date split, and checks the date ranges of the resulting datasets.
```{r}

# Filter the original dataset to include only the selected `Refs`
filtered_data <- long_data_norm %>%
  filter(Ref %in% selected_refs)

# Ensure the data is sorted by DateTime
filtered_data <- filtered_data %>% arrange(DateTime)
long_data_norm <- long_data_norm %>% arrange(DateTime)

# Define the split point by `DateTime`, e.g., 70% for training
split_date <- filtered_data$DateTime[round(0.7 * nrow(filtered_data))]
split_date_full <-long_data_norm$DateTime[round(0.7 * nrow(long_data_norm))]

# Split the data based on `DateTime`
train_data<- filtered_data %>% filter(DateTime <= split_date)
test_data <- filtered_data %>% filter(DateTime > split_date)

# Display the range of 'DateTime' in the training and testing datasets
range(train_data$DateTime)
range(test_data$DateTime)

# Split the original dataset based on 'DateTime' using the full range
train_data_full<- long_data_norm %>% filter(DateTime <= split_date_full)
test_data_full <- long_data_norm %>% filter(DateTime > split_date_full)

# Display the range of 'DateTime' in the full training and testing datasets
range(train_data_full$DateTime)
range(test_data_full$DateTime)
```





##Linear mixed model with sites (Ref) as the grouping factor

```{r}

#model 1 development
model1 <- lmer(UHI ~ 1 + (1|Ref), data = train_data, REML = FALSE, control = lmerControl(optimizer = "bobyqa"))
summary(model1)
ranova(model1)

```
The linear mixed model was fitted to the UHI data with urban sites (Ref) as the grouping factor. The fixed effect of the intercept was estimated to be 0.094870, indicating the average normalized UHI effect across all sites.

The random intercept for each urban site (Ref) allows the baseline UHI effect to vary between different sites. The variance of 0.00167 indicates the extent of this variability. The standard deviation of 0.04082 suggests that the baseline UHI effect can differ by approximately ±0.04082 units between different urban sites on the normalized scale. The estimated variance of the error term reflecting within-subject variability is 0.013997, and the standard deviation is 0.11831. This indicates that the UHI effect within the same urban site can vary by approximately ±0.11831 units over time, on the normalized scale.


The ICC can be intepretated as the proportion of the total variance that is “explained” by between-subject variability (in this case, the variability between different sites (Ref)). The ICC of 0.108 means that approximately 10.8% of the total variance in UHI can be attributed to differences between the urban sites (Ref).
I performed an ANOVA-like test to evaluate the significance of the random intercept for urban sites (Ref) in model1.The highly significant p-value (< 2.2e-16) suggests that including the random intercept for Ref (Urban sites) significantly improves the model fit. This supports the choice of using a mixed-effects model over a fixed-effects model, as it accounts for the variability between different urban sites.

# This script fits a linear mixed-effects model to the training data, compares it with another model, and summarizes the results.
```{r}

model2 <- lmer( UHI ~  1 + sin_hour + cos_hour + sin_day + cos_day + (1 | Ref)  , data = train_data, REML=FALSE, control = lmerControl(optimizer = "bobyqa"))
summary(model2)
anova(model1,model2)

```





#This script performs model selection for a linear mixed-effects model using the `buildmer` package. It starts with a maximal model formula and uses a forward selection approach based on Bayesian Information Criterion (BIC).
```{r}

# Define the maximal model formula including various predictors and interactions
maximal_model_formula_1 <- UHI ~  1 + sin_hour + cos_hour + sin_day + cos_day + wind_direction + wind_speed * cld_ttl_amt_id + cld_base_ht + air_temperature + rltv_hum + stn_pres + glbl_irad_amt + visibility + wetb_temp + SVF + EF + (1|Ref)

# Perform model selection using Maximum Likelihood (ML)
optimal_model_1 <- buildmer(
  formula = maximal_model_formula_1,
  data = train_data,
  buildmerControl = buildmerControl(
    direction = 'forward',
    crit = 'BIC',
    calc.anova = TRUE,
    include = "SVF + EF + (1 | Ref)",
    REML = FALSE,
    args=list(control=lmerControl(optimizer='bobyqa'))
  )
)


```




#This script extracts the final model from the `buildmer` object, calculates its Bayesian Information Criterion (BIC), and prints both the BIC value and the model summary.
```{r}

# Extract the final model from the buildmer object
model3_BIC <- optimal_model_1@model

# Calculate BIC
model_bic <- BIC(model3_BIC)

# Print the BIC value
cat("BIC of the optimal model:", model_bic, "\n")

# Print the summary of the optimal model
print(summary(model3_BIC))

```


#Construction of the model 5 on subset
```{r}

model5 <- lmer( UHI ~ 1 + sin_hour + cos_hour + sin_day + cos_day + air_temperature + cld_ttl_amt_id * wind_speed  + cld_base_ht + rltv_hum + stn_pres + glbl_irad_amt + visibility + wetb_temp + wind_direction + SVF + EF + (1 | Ref) , data = train_data, control = lmerControl(optimizer = "bobyqa"))
```



#Redo the above process the entire dataset
```{r}
model1_full <- lmer(UHI ~ 1 + (1|Ref), data = train_data_full, REML = FALSE, control = lmerControl(optimizer = "bobyqa"))
summary(model1_full)
ranova(model1_full)

model2_full <- lmer( UHI ~  1 + sin_hour + cos_hour + sin_day + cos_day + (1 | Ref)  , data = train_data_full, REML=FALSE, control = lmerControl(optimizer = "bobyqa"))
summary(model2_full)
anova(model1_full,model2_full)


maximal_model_formula_1_full <- UHI ~  1 + sin_hour + cos_hour + sin_day + cos_day + wind_direction + wind_speed * cld_ttl_amt_id + cld_base_ht + air_temperature + rltv_hum + stn_pres + glbl_irad_amt + visibility + wetb_temp + SVF + EF + (1|Ref)

# Perform model selection using Maximum Likelihood (ML)
optimal_model_1_full <- buildmer(
  formula = maximal_model_formula_1_full,
  data = train_data_full,
  buildmerControl = buildmerControl(
    direction = 'forward',
    crit = 'BIC',
    calc.anova = TRUE,
    include = "SVF + EF + (1 | Ref)",
    REML = FALSE,
    args=list(control=lmerControl(optimizer='bobyqa'))
  )
)
# Extract the final model from the buildmer object
model3_BIC_full <- optimal_model_1_full@model
# Calculate BIC
model_bic_full <- BIC(model3_BIC_full)
# Print the BIC value
cat("BIC of the optimal model:", model_bic_full, "\n")
# Print the summary of the optimal model
print(summary(model3_BIC_full))



model5_full <- lmer( UHI ~ 1 + sin_hour + cos_hour + sin_day + cos_day + air_temperature + cld_ttl_amt_id * wind_speed  + cld_base_ht + rltv_hum + stn_pres + glbl_irad_amt + visibility + wetb_temp + wind_direction + SVF + EF + (1 | Ref) , data = train_data_full, control = lmerControl(optimizer = "bobyqa"))

```

model5_full is our Final best LLM model.





## Define the model using nlme::lme, including AR(1) autocorrelation structure

```{r}
#write.csv(train_data, file = "train_data_corerrors.csv", row.names = FALSE)

# Define the model formula
model_formula <- UHI ~ sin_hour + cos_hour + sin_day + cos_day + 
  air_temperature + cld_ttl_amt_id * wind_speed + cld_base_ht + 
  rltv_hum + stn_pres + glbl_irad_amt + visibility + wind_direction + wetb_temp  + SVF + EF

# Fit the linear mixed-effects model with AR(1) autocorrelation structure
model_ar<- lme(
  fixed = model_formula,
  random = ~ 1 | Ref,  # Random intercept for each Ref
  correlation = corAR1(form = ~ DateTime | Ref),  # Autocorrelated errors within each Ref
  data = train_data,
  method = "REML"  # Use Restricted Maximum Likelihood
)

# Print the model summary
summary(model_ar)

# Save the model to a file
saveRDS(model_ar, file = "model_ar.rds")

```



#This script fits a linear mixed-effects model to the training data, performs model comparison using ANOVA, and calculates the Intraclass Correlation Coefficient (ICC).

```{r}
# Fit the linear mixed-effects model
model5_lme <- lme(
  fixed = UHI ~ 1 + sin_hour + cos_hour + sin_day + cos_day + air_temperature + cld_ttl_amt_id * wind_speed  + cld_base_ht + rltv_hum + stn_pres + glbl_irad_amt + visibility + wetb_temp + wind_direction + SVF + EF,
  random = ~ 1 | Ref,
  data = train_data,
  method = "REML",
  control = lmeControl(opt = "optim")
)

anova(model_ar,model5_lme)


# Calculate the ICC
# Extract variance components
var_random <- as.numeric(VarCorr(model_ar)[1, "Variance"])

# Extract residual variance, adjusted for autocorrelation
var_residual <- model_ar$sigma^2

# Extract autocorrelation parameter (phi)
phi <- coef(model_ar$modelStruct$corStruct, unconstrained = FALSE)

# Calculate the effective residual variance considering autocorrelation
effective_residual_variance <- var_residual / (1 - phi^2)

# Calculate ICC
icc <- var_random / (var_random + effective_residual_variance)
cat("Intraclass Correlation Coefficient (ICC):", icc, "\n")

```



##Time-series Cross-Validation for  model5 fitted on subset and entire dataset

```{r}
# Calculate the initial window size and horizon
initial_window <- round(0.5 * nrow(train_data))  # 50% of train_data
horizon <- round(0.1 * nrow(train_data))         # 10% of train_data 


initial_window_full <- round(0.5 * nrow(train_data_full))  # 50% of train_data_full
horizon_full <- round(0.1 * nrow(train_data_full))         # 10% of train_data_full


model5_formula <- UHI ~ 1 + sin_hour + cos_hour + sin_day + cos_day + 
                   air_temperature + cld_ttl_amt_id * wind_speed + 
                   cld_base_ht + rltv_hum + stn_pres + 
                   glbl_irad_amt + visibility + wetb_temp + wind_direction + 
                   SVF + EF + (1 | Ref)


time_series_cv <- function(data, model_formula, model_name, initial_window, horizon, fixed_window = TRUE) {
  results <- list()
  
  # Calculate the number of folds
  n <- nrow(data)
  n_folds <- floor((n - initial_window) / horizon)
  
  for (i in 1:n_folds) {
    # Define the train and validation sets
    train_index <- 1:(initial_window + (i - 1) * horizon)
    validation_index <- (initial_window + (i - 1) * horizon + 1):(initial_window + i * horizon)
    
    train_set <- data[train_index, ]
    validation_set <- data[validation_index, ]
    
    # Ensure the random effects are included in both train and validation sets
    train_refs <- unique(train_set$Ref)
    validation_set <- validation_set %>% filter(Ref %in% train_refs)
    
    # Train the model on the train set
    model <- lmer(model_formula, data = train_set, REML = FALSE, control = lmerControl(optimizer = "bobyqa"))
    
    # Make predictions on the validation set
    predictions <- predict(model, newdata = validation_set, allow.new.levels = TRUE)
    
    # Calculate the metrics
    mae <- mean(abs(validation_set$UHI - predictions))
    mse <- mean((validation_set$UHI - predictions)^2)
    rmse <- sqrt(mse)
    
    # Store the results
    results[[i]] <- list(train_set = train_set, validation_set = validation_set,
                         mae = mae, mse = mse, rmse = rmse, model = model)
  }
  
  # Calculate mean MAE, MSE, and RMSE across all folds
  mean_mae <- mean(sapply(results, function(x) x$mae))
  mean_mse <- mean(sapply(results, function(x) x$mse))
  mean_rmse <- mean(sapply(results, function(x) x$rmse))
  
  return(list(results = results, mean_mae = mean_mae, mean_mse = mean_mse, mean_rmse = mean_rmse))
}


# Function to print CV results
print_cv_results <- function(cv_results, model_name) {
  cat(paste("Results for", model_name, ":\n"))
  cat(paste("Mean MAE for", model_name, ":", cv_results$mean_mae, "\n"))
  cat(paste("Mean MSE for", model_name, ":", cv_results$mean_mse, "\n"))
  cat(paste("Mean RMSE for", model_name, ":", cv_results$mean_rmse, "\n"))
}


# Example usage:
cv_results_model5 <- time_series_cv(train_data, model5_formula, "model5", initial_window, horizon)
print_cv_results(cv_results_model5, "model5")

cv_results_model5_full <- time_series_cv(train_data_full, model5_formula, "model5_full", initial_window_full, horizon_full)
print_cv_results(cv_results_model5_full, "model5_full")


```


##Cross-Validation for model_ar
```{r}

time_series_cv_ar1 <- function(data, model_formula, initial_window, horizon, fixed_window = TRUE) {
  results <- list()
  
  # Calculate the number of folds
  n <- nrow(data)
  n_folds <- floor((n - initial_window) / horizon)
  
  for (i in 1:n_folds) {
    # Define the train and validation sets
    train_index <- 1:(initial_window + (i - 1) * horizon)
    validation_index <- (initial_window + (i - 1) * horizon + 1):(initial_window + i * horizon)
    
    train_set <- data[train_index, ]
    validation_set <- data[validation_index, ]
    
    # Ensure the random effects are included in both train and validation sets
    train_refs <- unique(train_set$Ref)
    validation_set <- validation_set %>% filter(Ref %in% train_refs)
    
    # Train the model on the train set
    model <- lme(
      fixed = model_formula,
      random = ~ 1 | Ref,
      correlation = corAR1(form = ~ DateTime | Ref),
      data = train_set,
      method = "REML"
    )
    
    # Make predictions on the validation set
    predictions <- predict(model, newdata = validation_set, allow.new.levels = TRUE)
    
    # Calculate the metrics
    mae <- mean(abs(validation_set$UHI - predictions))
    mse <- mean((validation_set$UHI - predictions)^2)
    rmse <- sqrt(mse)
    
    # Store the results
    results[[i]] <- list(train_set = train_set, validation_set = validation_set,
                         mae = mae, mse = mse, rmse = rmse, model = model)
  }
  
  # Calculate mean MAE, MSE, and RMSE across all folds
  mean_mae <- mean(sapply(results, function(x) x$mae))
  mean_mse <- mean(sapply(results, function(x) x$mse))
  mean_rmse <- mean(sapply(results, function(x) x$rmse))
  
  return(list(results = results, mean_mae = mean_mae, mean_mse = mean_mse, mean_rmse = mean_rmse))
}

# Example usage:
cv_results_model_ar1 <- time_series_cv_ar1(train_data, model_formula, initial_window, horizon)
print_cv_results(cv_results_model_ar1, "model_ar")

```







##Predictions on test_data 
## This script evaluates multiple models on the test data by calculating various performance metrics including MAE, MSE, RMSE, R-squared, AIC, and BIC.
```{r}


# Function to calculate additional metrics
calculate_metrics <- function(actual, predicted) {
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  nrmse_range <- (rmse / (max(actual) - min(actual))) * 100
  list(MAE = mae, MSE = mse, RMSE = rmse, nRMSE_by_range = nrmse_range)
}

# Evaluate model function including additional metrics
evaluate_model <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data, allow.new.levels = TRUE)
  metrics <- calculate_metrics(test_data$UHI, predictions)
  r_squared <- r.squaredGLMM(model)
  aic <- AIC(model)
  bic <- BIC(model)
  list(predictions = predictions, MAE = metrics$MAE, MSE = metrics$MSE, RMSE = metrics$RMSE, 
      R2_marginal = r_squared[1], R2_conditional = r_squared[2], 
       AIC = aic, BIC = bic)
}


# Evaluate models
results_model5 <- evaluate_model(model5, test_data_full)
cat("Model5 Metrics:\n")
cat("MAE:", results_model5$MAE, "\n")
cat("MSE:", results_model5$MSE, "\n")
cat("RMSE:", results_model5$RMSE, "\n")
cat("R2_marginal:", results_model5$R2_marginal, "\n")
cat("R2_conditional:", results_model5$R2_conditional, "\n")
cat("AIC:", results_model5$AIC, "\n")
cat("BIC:", results_model5$BIC, "\n\n")

results_model5_full <- evaluate_model(model5_full, test_data_full)
cat("Model5_full Metrics:\n")
cat("MAE:", results_model5_full$MAE, "\n")
cat("MSE:", results_model5_full$MSE, "\n")
cat("RMSE:", results_model5_full$RMSE, "\n")
cat("R2_marginal:", results_model5_full$R2_marginal, "\n")
cat("R2_conditional:", results_model5_full$R2_conditional, "\n")
cat("AIC:", results_model5_full$AIC, "\n")
cat("BIC:", results_model5_full$BIC, "\n\n")

results_model_ar <- evaluate_model(model_ar, test_data)
cat("Model_ar Metrics:\n")
cat("MAE:", results_model_ar$MAE, "\n")
cat("MSE:", results_model_ar$MSE, "\n")
cat("RMSE:", results_model_ar$RMSE, "\n")
cat("R2_marginal:", results_model_ar$R2_marginal, "\n")
cat("R2_conditional:", results_model_ar$R2_conditional, "\n")
cat("AIC:", results_model_ar$AIC, "\n")
cat("BIC:", results_model_ar$BIC, "\n\n")



```


## Calculate and Evaluate Predictions
```{r}

# Function to calculate MSE
calculate_mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

# Make predictions for your best model
predictions_model5_full <- predict(model5_full, newdata = test_data_full, allow.new.levels = TRUE)


#denormalized predictions to compare with regression and gam models
uhi_mean <- mean(long_data$UHI, na.rm = TRUE)
uhi_sd <- sd(long_data$UHI, na.rm = TRUE)
predictions_model5_full_denorm <- predictions_model5_full*uhi_sd + uhi_mean
test_data_full$UHI <- test_data_full$UHI*uhi_sd + uhi_mean


#combine predictions of models and the test_data_full
results_model5_full <- test_data_full %>%
  mutate(predicted_UHI = predictions_model5_full_denorm)



errors_model5_full_denom <-calculate_metrics(results_model5_full$UHI,results_model5_full$predicted_UHI )
print(errors_model5_full_denom)


 
# This block of code calculates performance metrics (MSE, RMSE, R², MAE) for the linear regression model at each location in the test set.
# It provides a detailed breakdown of the model's predictive accuracy by location, allowing for a more granular assessment of model performance.




# Calculate RMSE, MAE and R² for each location
error_metrics_by_location_model5_full <- results_model5_full %>%
  group_by(Ref) %>%
  summarise(
    MSE = mean((UHI - predicted_UHI)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(UHI - predicted_UHI))
  )

# View the error metrics per location
print(error_metrics_by_location_model5_full)



```




# This script calculates the Root Mean Squared Error (RMSE) for model predictions across different locations and visualizes these errors using a bar plot.
# It loops through each location, computes RMSE values, and creates a bar plot to compare prediction accuracy among locations.
```{r}
# Initialize an empty data frame to store RMSE values
rmse_df <- data.frame(Location = character(), RMSE = numeric(), stringsAsFactors = FALSE)

# Loop through each location in the selected_locations list
for (ref in selected_refs) {
  
  # Filter the data for the current location
  location_data <- results_model5_full %>% filter(Ref == ref)
  
  # Calculate RMSE for the current location
  rmse <- sqrt(mean((location_data$UHI - location_data$predicted_UHI)^2))
  
  # Store the results in the data frame
  rmse_df <- rmse_df %>% 
    add_row(Location = ref, RMSE = rmse)
}


# Create a bar plot of RMSE values for all locations
ggplot(rmse_df, aes(x = Location, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "black", color = "black", width = 0.6) +
  labs(x = "Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1),
    axis.title.x = element_text(size = 12, margin = margin(t = 10)),  # Set x-axis label size
    axis.title.y = element_text(size = 12, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  

```





# This script calculates the Root Mean Squared Error (RMSE) for model predictions grouped by letter-based categories of locations and visualizes these errors using a bar plot.
# It creates a new column to categorize locations based on their letter prefixes, calculates RMSE for each category, and plots the results.

```{r}
# Create a new column that extracts only the letters from the Ref column
# This helps to group locations based on their letter prefixes (e.g., CC, NE, etc.).
results_model5_full$LetterGroup <- gsub("[0-9]", "", results_model5_full$Ref)

# Calculate RMSE for each LetterGroup
# This summarizes RMSE values for each letter-based group to evaluate model performance across different categories.

error_by_group <- results_model5_full %>%
  group_by(LetterGroup) %>%
  summarise(
    RMSE = sqrt(mean((UHI - predicted_UHI)^2))  # Calculate RMSE for each group
  )

# View the error by group
# Print the calculated RMSE values for each letter-based group
print(error_by_group)



# Save the bar plot as a PNG file
png("RMSE_by_direction_LLMM.png", width = 8, height = 4, units = "in", res = 300)

# Create a bar plot of RMSE values for all letter-based groups
# This plot helps in comparing prediction errors across different location categories.
ggplot(error_by_group, aes(x = LetterGroup, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "#5e6a6f", color = "#5e6a6f", width = 0.6) +
  labs(x = "Group Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size=12, angle = 0, hjust = 1),
    axis.title.x = element_text(size = 15, margin = margin(t = 10)),  # Set x-axis label size
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(size = 15, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  
# Close the graphics device
dev.off()
```


# This block of code generates and saves combined plots for observed and predicted Urban Heat Island (UHI) intensity over specified sub-periods within the testing data range for a specific location.
# The function `create_combined_uhi_plot` is used to create these plots, which are then saved as image files.

```{r}

# Define the g_legend function to extract the legend from a ggplot
# This helper function extracts the legend from a ggplot object, which will be used to create a unified legend for the combined plots.

g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Define the sub-periods within the testing data range
time_ranges <- list(
  range1 = as.POSIXct(c("2015-06-15", "2015-06-21")),
  range2 = as.POSIXct(c("2015-08-15", "2015-08-21")),
  range3 = as.POSIXct(c("2015-10-15", "2015-10-21")),
  range4 = as.POSIXct(c("2015-12-15", "2015-12-21"))
)

# Function to create a combined plot for the station and time ranges
# This function generates a combined plot of observed and predicted UHI values for a specific station across the defined time ranges.

create_combined_uhi_plot <- function(ref, data, time_ranges) {
  station_data <- data %>% filter(Ref == ref)  # Filter data for the specific location
  
  plots <- lapply(time_ranges, function(time_range) {
    time_filtered_data <- station_data %>% filter(DateTime >= time_range[1] & DateTime <= time_range[2])
    ggplot(time_filtered_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size=0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size=0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
            legend.position = "bottom",
            legend.title = element_text(size = 12),  # Increase the size of the legend title
            legend.text = element_text(size = 12),
            axis.title.x = element_blank(),  # Increase x-axis label size
            axis.title.y = element_blank(),  # Increase y-axis label size
            axis.text.y = element_text(size = 12),   # Increase y-axis text size
            panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
            panel.grid.minor = element_blank())
  })
  
  # Extract legend from the first plot
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and legend into a single layout
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
  # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )
  
  return(combined_plot)
}


# Loop through each location in the selected_locations list
# The function is called in a loop to generate and save combined plots for the specified location(s).
#From the selected refs I will plot N3 to compare it with the MLR model


for (ref in "N3") {
  
  # Generate and display the combined plot for each location
  combined_plot <- create_combined_uhi_plot(ref, results_model5_full, time_ranges)
  grid.newpage()
  grid.draw(combined_plot)

  ggsave("N3llmm.png",plot=combined_plot,  width = 8, height = 4, units = "in", dpi = 300)
}
```





#This block calculates the performance of the regression model for each month in the test set
```{r}
#calculate the residuals
results_model5_full$residuals <- results_model5_full$UHI - results_model5_full$predicted_UHI

# Calculate MAE and RMSE by month
monthly_performancellm <- results_model5_full %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2)),
              nRMSE_by_range = (RMSE/16.05437 )* 100) %>%
    ungroup()

  # Ensure Month column is in Date format
monthly_performancellm <- monthly_performancellm %>%
    mutate(Month = as.Date(Month))

#plot the performance metrics per month
print(monthly_performancellm)
```




# r2 scatter plot for train and test for the model 5 full
```{r}

# Generate predictions on the train set
train_data_full$predicted_UHI <- predict(model5_full, newdata = train_data_full, allow.new.levels = TRUE)

#denormalized predictions to compare with regression and gam models
train_data_full$UHI <- train_data_full$UHI*uhi_sd + uhi_mean
train_data_full$predicted_UHI <- train_data_full$predicted_UHI*uhi_sd + uhi_mean

lm_fit <- lm(predicted_UHI ~ UHI, data = train_data_full)
# Extract the coefficients from the linear model
intercept <- coef(lm_fit)[1]
slope <- coef(lm_fit)[2]

# Calculate the R-squared value
R2_train <- 1 - sum((train_data_full$UHI - train_data_full$predicted_UHI)^2) / sum((train_data_full$UHI - mean(train_data_full$UHI))^2)

# Scatter plot for the train set
r_squared_plot_train <- ggplot(train_data_full, aes(x = UHI, y = predicted_UHI)) +
  geom_point(color = "skyblue", alpha = 0.7) +  # Use skyblue color for points
  geom_smooth(method = "lm", col = "black", se = FALSE) +
  labs(x = "Observed UHI",
       y = "Predicted UHI") +
  theme_minimal()+
  ylim(c(-6, 6)) +
  xlim(c(-6, 10)) +
  annotate("text", x = max(train_data_full$UHI) - 1, y = max(train_data_full$predicted_UHI)+0.25, 
           label = paste("y =", round(slope, 2), "* x +", round(intercept, 2)),
           hjust = 1, vjust = 0, size = 5, color = "black") +  # Formula of best-fit line
  annotate("text", x = min(train_data_full$UHI) , y = max(train_data_full$predicted_UHI)+0.75, 
           label = paste("R² =", round(R2_train, 2)),
           hjust = 0, vjust = 1, size = 5, color = "black") +  # R-squared value in top left
  theme(
    plot.background = element_rect(fill = "white", color = "white"),
    panel.background = element_rect(fill = "white", color = "white"),  # Set all text elements to size 12
    axis.title = element_text(size = 12),  # Axis titles to size 12
    axis.text = element_text(size = 12),  # Axis tick labels to size 12
    plot.title = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.15),  # All grid lines the same color
    panel.grid.minor = element_blank()  # Plot title to size 12
  )


# Display the R-squared plot for the train set
print(r_squared_plot_train)






# Generate predictions on the test set (assuming you already have them, but just in case)
#test_set_whole$predicted_UHI <- predict(best_model, newdata = test_set_whole, allow.new.levels = TRUE)

lm_fit_test <- lm(predicted_UHI ~ UHI, data = results_model5_full)
# Extract the coefficients from the linear model
intercept_test <- coef(lm_fit_test)[1]
slope_test <- coef(lm_fit_test)[2]

# Calculate the R-squared value
R2_test <- 1 - sum((results_model5_full$UHI - results_model5_full$predicted_UHI)^2) / sum((results_model5_full$UHI - mean(results_model5_full$UHI))^2)

# Scatter plot for the test set
r_squared_plot_test <- ggplot(results_model5_full, aes(x = UHI, y = predicted_UHI)) +
  geom_point(color = "skyblue", alpha = 0.7) +  # Use skyblue color for points
  geom_smooth(method = "lm", col = "black", se = FALSE) +
  labs(x = "Observed UHI",
       y = "Predicted UHI") +
  theme_minimal()+
  ylim(c(-6, 6)) +
  xlim(c(-6, 10)) +
  annotate("text", x = max(results_model5_full$UHI) - 1, y = max(results_model5_full$predicted_UHI)+0.25, 
           label = paste("y =", round(slope_test, 3), "* x +", round(intercept_test, 3)),
           hjust = 1, vjust = 0, size = 5, color = "black") +  # Formula of best-fit line
  annotate("text", x = min(results_model5_full$UHI)+2 , y = max(results_model5_full$predicted_UHI)+0.70, 
           label = paste("R² =", round(R2_test, 3)),
           hjust = 0, vjust = 1, size = 5, color = "black") +  # R-squared value in top left
  theme(
    plot.background = element_rect(fill = "white", color = "white"),
    panel.background = element_rect(fill = "white", color = "white"),  # Set all text elements to size 12
    axis.title = element_text(size = 15),  # Axis titles to size 12
    axis.text = element_text(size = 12),  # Axis tick labels to size 12
    plot.title = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.15),  # All grid lines the same color
    panel.grid.minor = element_blank()  # Plot title to size 12
  )


# Display the R-squared plot for the train set
print(r_squared_plot_test)

# Save the plot
ggsave("r_squared_plot_test_model5full.png", plot = r_squared_plot_test, width = 8, height = 4, units = "in", dpi = 300)


```





#Just like previously plots the four random selected final refs for four seasons, each season is a different plot

```{r}

# Define the specific date ranges for plotting
date_ranges <- list(
  Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-09")),
  Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-09")),
  Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-09")),
  Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-06-04"))
)

# Function to create the combined plots for each date range
create_combined_plots <- function(date_range, season,test_data_with_pred,selected_stations) {
  
  selected_data <- test_data_with_pred %>%
    filter(Ref %in% selected_stations) %>%
    filter(DateTime >= date_range[1] & DateTime <= date_range[2])
  
  y_limits <- range(c(selected_data$UHI, selected_data$predicted_UHI), na.rm = TRUE)
  
  actual_vs_predicted_plot <- ggplot(selected_data, aes(x = DateTime)) +
    geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
    geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
    scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
    labs(color="Legend") +
    #scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
    #scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) +
    theme_minimal() +
    theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
          axis.text.y = element_text(size=12),
          axis.title.x = element_text(size=15,margin = margin(t = 20)),
          axis.title.y = element_text(size=15),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          legend.background = element_rect(fill = "white", color = "white"),
          legend.position = "bottom",
          legend.title = element_text(size = 12),  # Increase the size of the legend title
          legend.text = element_text(size = 12),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank() )+
    facet_wrap(~ Ref, ncol = 2)
  
  # Save the plot
  #ggsave(filename = paste0("UHI_Predicted_vs_Actual_", season, "_2015.png"), plot = actual_vs_predicted_plot, width = 16, height = 8)
  
  print(actual_vs_predicted_plot)
}

# Loop over each season and create/save the plots
for (season in names(date_ranges)) {
  file_name <- paste0("Plot_", season, "_LLMM.png")
  
  p<-create_combined_plots(date_ranges[[season]], season, results_model5_full, final_refs)
  
  ggsave(file_name,plot=p,  width = 8, height = 4, units = "in", dpi = 300)

}

```








#Just like previously plots the four random selected final refs for four seasons, each ref is a different plot
```{r}

# Define the sub-periods within the testing data range

time_ranges <- list(
  range1 = as.POSIXct(c("2015-05-27", "2015-06-04")),
  range2 = as.POSIXct(c("2015-08-01", "2015-08-09")),
  range3 = as.POSIXct(c("2015-10-01", "2015-10-09")),
  range4 = as.POSIXct(c("2015-12-01", "2015-12-09"))
)

g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Function to create a combined plot for each selected station and time range
create_combined_uhi_plot <- function(ref, data, time_ranges, model_name) {
  # Calculate the y-axis limits based on the combined range of UHI and predicted_UHI across all time ranges
  combined_data <- data %>% filter(Ref == ref & DateTime >= min(sapply(time_ranges, `[`, 1)) & DateTime <= max(sapply(time_ranges, `[`, 2)))
  y_limits <- range(c(combined_data$UHI, combined_data$predicted_UHI), na.rm = TRUE)
  print(y_limits)
  
  
  plots <- lapply(seq_along(time_ranges), function(i) {
    time_range <- time_ranges[[i]]
    station_data <- data %>% filter(Ref == ref & DateTime >= time_range[1] & DateTime <= time_range[2])
    p <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
      scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) + # Set consistent y-axis limits and grid lines
      theme_minimal() +
      theme(
        axis.text.x = element_text(size=12,angle = 0, hjust = 1),
        legend.position = "bottom",
        legend.title = element_text(size = 12),  # Increase the size of the legend title
        legend.text = element_text(size = 12),
        axis.title.x = element_blank(),  # Remove x-axis label
        axis.title.y = element_blank(),
        axis.text.y = element_text(size = 12),
        panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
        panel.grid.minor = element_blank()  # Remove minor grid lines
      )
    
    return(p)
  })
  
  # Extract the legend from one of the plots
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and add the legend at the bottom
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
   # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )

  
  return(combined_plot)
}


# Generate and display combined plots
for (ref in final_refs) {
  
  
  combined_plot <- create_combined_uhi_plot(ref, results_model5_full, time_ranges, "Model_LLMM")
  grid.newpage()
  grid.draw(combined_plot)
  
  print(combined_plot)
}
```




# This function calculates and plots the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)
# on a monthly basis. It creates two plots: one for MAE and one for RMSE, visualizing how the model's
# performance varies over time. The plots are customized and saved as PNG files.
```{r}

calculate_and_plot_monthly_performance <- function(data, df_name) {
  
  # Calculate MAE and RMSE by month
  monthly_performance <- data %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2))) %>%
    ungroup()

  # Ensure Month column is in Date format
  monthly_performance <- monthly_performance %>%
    mutate(Month = as.Date(Month))

  # Plot monthly MAE
  mae_plot <- ggplot(monthly_performance, aes(x = Month, y = MAE)) +
    geom_line(color = "red", size = 0.5) +
    geom_point(color = "red", size = 0.5) +
    labs(x = "Month", y = "Mean Absolute Error (MAE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())

  # Save the MAE plot
  #mae_filename <- paste0(df_name, "_Monthly_MAE.png")
  #ggsave(mae_filename, plot = mae_plot, width = 12, height = 6, units = "in", dpi = 300)

  
  
  # Plot monthly RMSE
  rmse_plot <- ggplot(monthly_performance, aes(x = Month, y = RMSE)) +
    geom_line(color = "green", size = 0.5) +
    geom_point(color = "green", size = 0.5) +
    labs(x = "Month", y = "Root Mean Squared Error (RMSE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())
  
  # Save the RMSE plot
  #rmse_filename <- paste0(df_name, "_Monthly_RMSE.png")
  #ggsave(rmse_filename, plot = rmse_plot, width = 12, height = 6, units = "in", dpi = 300)


  # Print the plots
  print(mae_plot)
  print(rmse_plot)
}

calculate_and_plot_monthly_performance(results_model5_full, "Model 5 full")



```




# This function processes a data frame by categorizing model performance based on residuals,
# and then creates and displays combined plots for specified stations and date ranges. It includes
# plots of actual vs. predicted UHI and model performance over time. The function also prints
# information about the data processing and plot creation steps.



```{r}

# Function to process a single data frame
process_data_frame <- function(df, final_refs, df_name) {
  
  # Convert Ref to factor
  df$Ref <- as.factor(df$Ref)

  # Calculate the mean and standard deviation of the residuals
  residual_mean <- mean(df$residuals)
  residual_sd <- sd(df$residuals)

  # Categorize performance using the standard deviation-based thresholds
  df <- df %>%
    mutate(Performance = case_when(
      abs(residuals - residual_mean) > 1.2 * residual_sd ~ "Poor",
      abs(residuals - residual_mean) > residual_sd ~ "Medium",
      TRUE ~ "Good"
    ))

  # Define the specific date ranges for plotting
  date_ranges <- list(
    Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-31")),
    Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-05-31")),
    Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-31")),
    Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-31"))
  )

  # Function to create the combined plots for each station and date range
  create_combined_plots <- function(station_ref, season) {
    print(paste("Creating plots for Station:", station_ref, "in", season, "2015"))

    station_data <- df %>%
      filter(Ref == station_ref) %>%
      filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])

    if (nrow(station_data) == 0) {
      print(paste("No data available for Station:", station_ref, "in", season, "2015"))
      return()
    }

    actual_vs_predicted_plot_period <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Actual UHI"), size = 1) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 1, linetype = "dashed") +
      labs(title = paste(df_name, "- Predicted vs. Actual UHI for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Actual UHI" = "skyblue", "Predicted UHI" = "red"))

    performance_points_plot_period <- ggplot(station_data, aes(x = DateTime, y = UHI, color = Performance)) +
      geom_point(size = 1, alpha = 0.6) +
      labs(title = paste(df_name, "- Model Performance Over Time for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Performance") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Poor" = "red", "Medium" = "orange", "Good" = "green"))
    
    grid.arrange(actual_vs_predicted_plot_period, performance_points_plot_period, ncol = 1)
  }

  # Loop over the selected stations and seasons to create the plots
  for (station_ref in final_refs) {
    for (season in names(date_ranges)) {
      print(paste("Checking data for Station:", station_ref, "in", season, "2015"))
      filtered_data <- df %>%
        filter(Ref == station_ref) %>%
        filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])
      
      print(paste("Number of records for Station:", station_ref, "in", season, "2015:", nrow(filtered_data)))
      
      create_combined_plots(station_ref, season)
    }
  }
}



# Apply the function to each data frame
process_data_frame(results_model5_full, final_refs, "Model 5 full")


```

# This script calculates and displays seasonal performance metrics (MAE, MSE, RMSE) for model predictions across different seasons.

```{r}
# Function to calculate additional metrics
calculate_metrics <- function(actual, predicted) {
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  nrmse_range <- (rmse / (max(actual) - min(actual))) * 100
  list(MAE = mae, MSE = mse, RMSE = rmse, nRMSE_by_range = nrmse_range)
}

# Define seasons within the testing data range
seasons <- list(
  Spring = c("2015-05-27", "2015-05-31"),
  Summer = c("2015-06-01", "2015-08-31"),
  Fall = c("2015-09-01", "2015-11-30"),
  Winter = c("2015-12-01", "2015-12-31")
)


# Function to calculate seasonal errors for a single data frame
calculate_seasonal_errors <- function(data, df_name) {
  seasonal_errors <- lapply(names(seasons), function(season) {
    season_data <- data %>%
      filter(DateTime >= as.POSIXct(seasons[[season]][1]) & DateTime <= as.POSIXct(seasons[[season]][2]))
    metrics <- calculate_metrics(season_data$UHI, season_data$predicted_UHI)
    season_errors <- data.frame(DataFrame = df_name, Season = season, MAE = metrics$MAE, MSE = metrics$MSE, RMSE = metrics$RMSE)
    return(season_errors)
  })
  
  # Combine seasonal errors into a single data frame
  seasonal_errors_df <- bind_rows(seasonal_errors)
  return(seasonal_errors_df)
}

# Apply the function to each of the three data frames
llmm_seasonal_errors <- calculate_seasonal_errors(results_model5_full, "Model LLMM")


# Print the combined seasonal errors
print(llmm_seasonal_errors)

```


#This script generates diagnostic plots to assess the residuals from a regression model fitted on the training dataset.
```{r}

train_data_full$predicted_UHI <- predict(model5_full, newdata = train_data_full, allow.new.levels = TRUE)
train_data_full$residuals <-train_data_full$UHI - train_data_full$predicted_UHI

png("4_plot_train_model5_fulln.png", width = 8, height = 4, units = "in", res = 300)

par(mfrow = c(1, 2))  # Set up a 2x2 plotting area

# a) Q-Q Plot of Residuals
qqnorm(train_data_full$residuals,main = "(a)",  ylim = c(-10, 10), cex.axis = 1.2, cex.lab = 1.4)
qqline(train_data_full$residuals, col = "red")

# b) Histogram of Residuals
hist(train_data_full$residuals, main = "(b)", xlab = "Residuals", breaks = 20,cex.axis = 1.2, cex.lab = 1.4)

dev.off()


```















# Function to generate and plot random effects for a given model
```{r}

# Function to generate and plot random effects for a given model
plot_random_effects <- function(model, model_name) {
  # Generate random effects simulations for the model
  REsimulations <- REsim(model)
  REsim_df <- as.data.frame(REsimulations)
  
  # Plot the interval estimates for Ref group
  plot_ref <- ggplot(subset(REsim_df, groupFctr == "Ref"), aes(x = reorder(groupID, mean), y = mean)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - 1.96 * sd, ymax = mean + 1.96 * sd), width = 0.2) +
    geom_hline(yintercept = 0, color = "red") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(size=15,angle = 90, hjust = 1, vjust = 0.5),
      axis.text.y = element_text(size=15),
      plot.background = element_rect(fill = "white", color = "white"),
      panel.background = element_rect(fill = "white", color = "white"),
      axis.title.x = element_text(size = 18),  # Set x-axis label size to 12
      axis.title.y = element_text(size = 18),  # Set y-axis label size to 12
      panel.grid.minor = element_blank(),  # Remove minor grid lines
      panel.grid.major = element_line(color = "gray", size = 0.3)  # Customize major grid lines
    ) +
    labs(x = "Ref Group",
         y = "Effect Range")
  
  # Save the Ref group plot using the title as the filename
  ref_filename <- paste0(model_name, "_Random_Intercept_Ref_Group.png")
  ggsave(ref_filename, plot = plot_ref, width = 12, height = 6, units = "in", dpi = 300)
  
  # Plot the interval estimates for Hour
  plot_hour <- ggplot(subset(REsim_df, groupFctr == "Hour"), aes(x = reorder(groupID, mean), y = mean)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - 1.96 * sd, ymax = mean + 1.96 * sd), width = 0.2) +
    geom_hline(yintercept = 0, color = "red") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
      plot.background = element_rect(fill = "white", color = "white"),
      panel.background = element_rect(fill = "white", color = "white"),
      axis.title.x = element_text(size = 12),  # Set x-axis label size to 12
      axis.title.y = element_text(size = 12),  # Set y-axis label size to 12
      panel.grid.minor = element_blank(),  # Remove minor grid lines
      panel.grid.major = element_line(color = "gray", size = 0.3)  # Customize major grid lines
    ) +
    labs(x = "Hour",
         y = "Effect Range")
  
  # Save the Hour plot using the title as the filename
  hour_filename <- paste0(model_name, "_Random_Intercept_Hour.png")
  ggsave(hour_filename, plot = plot_hour, width = 12, height = 6, units = "in", dpi = 300)
  
  # Print the plots
  print(plot_ref)
  print(plot_hour)
}




# Apply the function to each model

plot_random_effects(model5_full, "Model 5 full")



```

The graph of random intercept interval estimates from the mixed-effects model4 reveals significant variability in Urban Heat Island (UHI) effects across different Ref groups/sites. Each black dot represents the estimated random intercept for a site, with vertical lines indicating the 95% confidence intervals for these estimates. There are sites with positive intercepts and confidence intervals entirely above the red horizontal line, indicating significantly higher UHI effects likely due to factors such as dense urban infrastructure. Conversely, there are sites with negative intercepts and confidence intervals entirely below the red line, indicating significantly lower UHI effects, possibly benefiting from higher vegetation cover or better urban planning. Sites with confidence intervals crossing the red line exhibit UHI effects that are not significantly different from the overall average. The width of the confidence intervals provides insight into the precision of the estimates, with narrower intervals indicating more reliable estimates. This visualization underscores the importance of accounting for site-specific variations in UHI studies.

Some of the sites that have significantly higher UHI effects examples: SW12, SW13, cc11, cc6 
some of the sites that have significantly lower UHI effects examples: NE6, NE5, N5
some of the sites that have UHI effects that are not significantly different from the average: CC7, N1, SE5, SE6 





















