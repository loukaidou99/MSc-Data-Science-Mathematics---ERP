---
title: "UHI"
author: 'StudentID: 11455972'
date: "2024-03-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
library(readxl)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(imputeTS)
library(readr)
library(tidyverse)
library(reshape2)
library(sf)
library(leaflet)
library(RColorBrewer)
library(scales)
library(lattice)
library(rgl)
library(car)
library(sp)
library(spacetime)
library(xts)
library(spatstat)
library(mgcv)
library(plotly)
library(akima)
library(reticulate)
library(gganimate)
library(animation)
library(viridis)
library(gridExtra)
library(grid)
```

# Data Preprocessing

```{r}
# Set the directory path
directory <- "C:\\Users\\user\\Desktop"

# Define the paths to the Excel and CSV files
excel_path <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\AllSites2014and2015 (1).xlsx")
excel_path2 <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\RostherneRadiation2014_15 (1).xlsx")
excel_path3 <- paste0(directory, "\\MSc-Data-Science---ERP\\data\\locations_svf_ef.csv")

# Read data from the first Excel file (AllSites2014and2015 (1).xlsx)
# Skip initial non-data rows (6 rows for Allsites, 1 row for Rostherne2014/2015)
Allsites <- read_excel(excel_path, sheet = "Allsites", skip = 6)
Rostherne2014 <- read_excel(excel_path, sheet = "Rostherne2014", skip = 1)
Rostherne2015 <- read_excel(excel_path, sheet = "Rostherne2015", skip = 1)

# Read data from the second Excel file (RostherneRadiation2014_15 (1).xlsx)
# Skip initial non-data rows (2 rows for radiation data)
RostherneRadiation2014 <- read_excel(excel_path2, sheet = "RostherneRadiation2014", skip = 2)
RostherneRadiation2015 <- read_excel(excel_path2, sheet = "RostherneRadiation2015", skip = 2)


# Read the CSV file containing locations, SVF, and EF data
locations_svf_ef <- read_csv(excel_path3)
```



# Data Cleaning
# In this block, we perform basic data cleaning tasks including removing rows and columns that contain only NA values,and renaming a column for clarity.

```{r}
# Removing rows where all elements are NA
# This ensures that only rows with some data are kept in the dataset
Allsites <- Allsites[rowSums(is.na(Allsites)) != ncol(Allsites), ]

# Removing columns where all elements are NA
# This step removes columns that do not contain any useful data across all rows
Allsites <- Allsites[, colSums(is.na(Allsites)) != nrow(Allsites)]

# Rename the 'Date/Time' column to 'DateTime'
# Standardizes the column name to make it easier to reference in future analysis
colnames(Allsites)[colnames(Allsites) == "Date/Time"] <- "DateTime"

# Print the cleaned data to verify the changes
print(Allsites)

```



# Data Integration
# In this block, we will merge solar irradiation data from the `RostherneRadiation2014` dataset into the `Rostherne2014` dataset.
# This involves converting the time columns to a compatible format and performing a left join.

```{r}
# Convert the 'ob_end_time' column in RostherneRadiation2014 to POSIXct format
# This ensures that the time format matches with 'ob_time' in Rostherne2014 for accurate merging
RostherneRadiation2014$ob_end_time <- as.POSIXct(RostherneRadiation2014$ob_end_time, format="%Y-%m-%d %H:%M")


# Perform a left join to add the 'glbl_irad_amt' (global solar irradiation amount) column from RostherneRadiation2014
# The join is based on matching the 'ob_time' column in Rostherne2014 with the 'ob_end_time' column in RostherneRadiation2014
Rostherne2014 <- merge(Rostherne2014, RostherneRadiation2014[c('ob_end_time', 'glbl_irad_amt')],
                       by.x = 'ob_time', by.y = 'ob_end_time', all.x = TRUE)

# Check the first few rows of the updated Rostherne2014 to verify that the join was successful
head(Rostherne2014)


```




# The Rostherne2014 dataset now includes solar irradiation data from the RostherneRadiation2014 dataset.




# Data Integration for 2015
# In this block, we will merge solar irradiation data from the `RostherneRadiation2015` dataset into the `Rostherne2015` dataset.
# This involves converting the time columns to a compatible format and performing a left join.

```{r}

# Convert 'ob_end_time' in RostherneRadiation2015 from character to POSIXct format
# This ensures that the time format in both datasets matches for accurate merging
RostherneRadiation2015$ob_end_time <- as.POSIXct(RostherneRadiation2015$ob_end_time, format="%Y-%m-%d %H:%M")

# Perform the left join to add the 'glbl_irad_amt' (global solar irradiation amount) column from RostherneRadiation2015
# The join matches 'ob_time' in Rostherne2015 with 'ob_end_time' in RostherneRadiation2015, including all rows from Rostherne2015
Rostherne2015 <- merge(Rostherne2015, RostherneRadiation2015[c('ob_end_time', 'glbl_irad_amt')], 
                                by.x = 'ob_time', by.y = 'ob_end_time', all.x = TRUE)

# Check the first few rows of the result to verify that the join was successful
head(Rostherne2015)


```

# The Rostherne2015 dataset now includes solar irradiation data from the RostherneRadiation2015 dataset.




# Combine and Analyze 2014 and 2015 Data
# In this block, we will combine the datasets from 2014 and 2015, sort the combined data by time,
# and analyze the combined dataset to identify the presence of missing values.

```{r}

# Combine the Rostherne2014 and Rostherne2015 datasets
# The rbind() function is used to stack the datasets on top of each other
Rostherne_combined <- rbind(Rostherne2014, Rostherne2015)

# Sort the combined dataset by 'ob_time' to ensure chronological order
Rostherne_combined <- Rostherne_combined[order(Rostherne_combined$ob_time), ]

# Convert 'ob_time' to POSIXct format if it is not already in this format
# This step ensures that time-related operations can be performed accurately
Rostherne_combined$ob_time <- as.POSIXct(Rostherne_combined$ob_time)

# Calculate the total number of rows in the combined dataset
# This helps in determining the scale of the dataset and is used for further analysis
total_rows_combined <- nrow(Rostherne_combined)

# Recalculate the number of missing values in each column
# This identifies the extent of missing data across different variables in the combined dataset
missing_values_combined <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Calculate the percentage of missing values for each column
# This provides a relative measure of missingness, useful for data cleaning decisions
missing_percentage_combined <- (missing_values_combined / total_rows_combined) * 100

# Create a dataframe to summarize the status of missing values in the combined dataset
# The dataframe includes the column names, the count of missing values, and the percentage of missing data
missing_data_frame_combined <- data.frame(
  Column = names(missing_values_combined),
  MissingValues = missing_values_combined,
  MissingPercentage = missing_percentage_combined
)

# Print the dataframe to review the missing data status in the combined dataset
print(missing_data_frame_combined)


```

# The combined dataset is now ready, and an analysis of missing values has been conducted.







# Function to Analyze Missing Data
# In this block, we define a function to calculate and print the number and percentage of missing values 
# for each column in a given dataset. This function is then applied to the 'Allsites' dataset.


```{r}
# Define a function to calculate and print missing values
# The function takes a dataset and its name as arguments, calculates the number and percentage of missing values, 
# and prints a summary for each column in the dataset
print_missing_values <- function(data, dataset_name) {
  missing_values <- sapply(data, function(x) sum(is.na(x)))
  missing_percentage <- (missing_values / nrow(data)) * 100
  
  # Print the dataset name followed by the missing data summary for each column
  cat(paste("Missing Data for", dataset_name, ":\n"))
  for (i in 1:length(missing_values)) {
    cat(names(missing_values)[i], ": ", missing_values[i], " missing values (", round(missing_percentage[i], 2), "%)\n", sep = "")
  }
}

# Apply the function to the 'Allsites' dataset
# This will calculate and print the missing values for each column in the 'Allsites' dataset
print_missing_values(Allsites, "Allsites")
 
```








# The function provides a clear summary of missing data in the 'Allsites' dataset, which is useful for further data cleaning.

Remove stations with very high numbers of missing values (N6 with 53.77% and NE4 with 74.99%)



# Data Cleaning: Removing Stations with High Missing Values
# In this block, we identify and remove specific stations (columns) from the 'Allsites' dataset
# that have a very high number of missing values.

```{r}
# Define the stations (columns) with very high numbers of missing values to be removed
# These stations are specified by their column names
stations_to_remove <- c("NE4", "N6")

# Remove the specified stations from the 'Allsites' dataset
# This operation excludes the columns listed in 'stations_to_remove' from the dataset
Allsites <- Allsites[, !(names(Allsites) %in% stations_to_remove)]
```

# Summary of Cleaned Dataset
# In this block, we generate a summary of the cleaned 'Allsites' dataset after removing columns with high missing values.

```{r}
# Generate and print a summary of the 'Allsites' dataset
# The summary provides a quick overview of the dataset, including min, max, mean, and missing values for each column
summary(Allsites)
```


# The summary gives an insight into the current state of the 'Allsites' dataset after removing problematic stations.







# Visualizing NA Gaps
# In this block, we define a function to visualize gaps (missing values) in each column of the 'Allsites' dataset.
# The function is then applied to all columns that contain NA values.
```{r}

# Function to visualize NA gaps for a specific column in the dataset
# The function converts the column to a time series object and uses ggplot to create a visualization of the NA gaps
visualize_na_gaps <- function(data, column_name) {
  ts_data <- ts(data[[column_name]])  # Convert to time series object
  plot_title <- paste("NA Gaps for", column_name)
  plot <- ggplot_na_gapsize(ts_data) + ggtitle(plot_title)
  print(plot)
}

# Identify columns in 'Allsites' that contain NA values
# This helps target only those columns that need visualization of missing data
columns_with_na <- colnames(Allsites)[colSums(is.na(Allsites)) > 0]

# Visualize NA gaps for each column with NA values
# Loop through each column identified and apply the 'visualize_na_gaps' function to visualize missing data
for (column in columns_with_na) {
  visualize_na_gaps(Allsites, column)
}


```


# This block provides visualizations of NA gaps for all columns in the 'Allsites' dataset that contain missing values, helping to understand the pattern of missing data.








# Filtering and Finding Closest Stations
# In this block, we first filter out specific stations from the locations dataset. 
# Then, we define and apply a function to find the two closest stations for each station with missing data in the 'Allsites' dataset.
```{r}
# Create a copy of the locations data frame to preserve the original data
locations_svf_ef_filtered <- locations_svf_ef

# Define the list of stations that need to be removed due to high missing values or other issues
stations_to_remove <- c("CC1", "CC10", "N6", "NE4", "NW1", "NW4", "NE6", "NE5")

# Filter out the specified stations from the locations dataset
# This step removes the stations listed in 'stations_to_remove' from the dataset
locations_svf_ef_filtered <- locations_svf_ef_filtered %>% 
  filter(!Ref %in% stations_to_remove)

# Verify the removal of the specified stations by printing the filtered dataset
print(locations_svf_ef_filtered)

# Define a function to find the closest stations based on Euclidean distance
# This function will be used to identify the two closest stations for stations with missing data in 'Allsites'
find_closest_stations <- function(data, locations) {
  # Identify stations with missing values, excluding NE5 and NE6
  stations_with_na <- setdiff(colnames(data)[colSums(is.na(data)) > 0], c("NE5", "NE6"))
  
  # Initialize a list to store the closest stations
  closest_stations_list <- list()
  
  # Define a helper function to calculate Euclidean distance between two points
  euclidean_distance <- function(lon1, lat1, lon2, lat2) {
    sqrt((lon1 - lon2)^2 + (lat1 - lat2)^2)
  }
  
  # For each station with missing data, find the two closest stations based on their geographic coordinates
  for (station in stations_with_na) {
    current_location <- locations %>% filter(Ref == station)
    other_locations <- locations %>% filter(Ref != station)
    
    # Calculate distances from the current station to all other stations and sort by distance
    other_locations <- other_locations %>%
      mutate(distance = euclidean_distance(current_location$Longitude, current_location$Latitude, Longitude, Latitude)) %>%
      arrange(distance)
    
    # Identify the two closest stations
    closest_stations <- other_locations$Ref[1:2]
    closest_stations_list[[station]] <- closest_stations
  }
  
  return(closest_stations_list)
}

# Apply the 'find_closest_stations' function to find the closest stations using the filtered locations dataset
closest_stations <- find_closest_stations(Allsites, locations_svf_ef_filtered)
# Print the list of closest stations for stations with missing data
print(closest_stations)



```

# This block effectively filters out unwanted stations and then finds the two closest stations 
# for those stations in 'Allsites' that have missing data, facilitating further analysis or imputation.






```{r}

# Define the closest stations
closest_stations <- list(
  CC3 = c("CC4", "CC8"),
  CC4 = c("CC3", "CC8"),
  CC5 = c("CC6", "CC8"),
  CC6 = c("CC7", "CC8"),
  CC8 = c("CC2", "CC6"),
  CC9 = c("CC2", "CC11"),
  CC11 = c("CC9", "S1"),
  E1 = c("SE1", "NE1"),
  E2 = c("SE2", "E1"),
  S6 = c("S5", "S4"),
  SW5 = c("SW6", "SW4"),
  NE5= c("NE3", "NE6")
)
```



# Manual Imputation of Missing Values
# In this block, we define and apply a function to manually impute missing values for specific stations in the 'Allsites' dataset.
# The imputation is based on the average of values from the two closest stations identified earlier.

```{r}
# Define a function to manually impute missing values for a specific station
# The function imputes missing values in a given station using the mean of its closest neighboring stations
impute_station <- function(data, station, neighbors) {
  data[[station]] <- ifelse(is.na(data[[station]]), 
                            rowMeans(data[, neighbors], na.rm = TRUE), 
                            data[[station]])
  return(data)
}


# Apply the manual imputation for each station with missing values
# Iterate through each station identified as having missing values and apply the imputation function

for (station in names(closest_stations)) {
  Allsites <- impute_station(Allsites, station, closest_stations[[station]])
}

# The 'Allsites' dataset is now updated with missing values imputed using data from the closest stations.
```
# This block effectively handles missing data by imputing values based on nearby stations, which helps in maintaining the integrity of the dataset for further analysis.




# Visualizing Air Temperature Comparison
# In this block, we convert the 'DateTime' column to POSIXct format and create a plot to compare air temperatures 
# between two stations, CC2 and CC8, over time.
```{r}
# Convert the 'DateTime' column to POSIXct format
# This conversion ensures that the time series data is in the correct format for plotting and analysis
Allsites$DateTime <- as.POSIXct(Allsites$DateTime, format="%Y-%m-%d %H:%M:%S")

# Plot air temperatures for stations CC2 and CC8 to compare their variations over time
# The plot uses ggplot2 to create a line graph with DateTime on the x-axis and air temperature on the y-axis
ggplot(Allsites, aes(x = DateTime)) +
  geom_line(aes(y = CC2, color = "CC2")) +
  geom_line(aes(y = CC8, color = "CC8")) +
  labs(title = "Comparison of Air Temperatures: CC2 vs CC8",
       x = "Date/Time",
       y = "Air Temperature",
       color = "Legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
# This block generates a visual comparison of air temperatures between the two stations, CC2 and CC8, 
# helping to identify patterns or discrepancies in the temperature data over time.





#Missing values-Rostherne_combined (columns where more than 50% of the values are missing)

```{r}

# Calculate the number of missing values in each column
missing_values <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Create a dataframe to display the column names and the count of missing values
missing_data_frame_combined <- data.frame(Column = names(missing_values), MissingValues = missing_values)

# Print the dataframe that shows columns and their respective number of missing values
print(missing_data_frame_combined)

```




```{r}
# Calculate the number of missing values in each column
# This identifies the extent of missing data across all columns in the 'Rostherne_combined' dataset

column_missing_values <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Calculate the total number of rows in the dataset
# This value is used to compute the percentage of missing data for each column
total_rows <- nrow(Rostherne_combined)

# Calculate the percentage of missing values in each column
# The percentage provides a relative measure of how much data is missing in each column

column_missing_percentage <- (column_missing_values / total_rows) * 100

# Identify columns where more than 50% of the values are missing
# This step filters out columns with a significant amount of missing data for further investigation

columns_over_50_missing <- names(column_missing_percentage[column_missing_percentage > 50])

# Print the names of the columns, the number of missing values, and the percentage
# This loop provides a detailed report of the columns with over 50% missing data, making it easier to decide on next steps
print("Columns with more than 50% missing values:")
for (col in columns_over_50_missing) {
    print(paste("Column:", col, 
                "- Missing Values:", column_missing_values[col], 
                "- Percentage:", column_missing_percentage[col], "%"))
}


```




# This block of code is focused on cleaning the 'Rostherne_combined' dataset by removing columns that have more than 50% missing data.
# After removing these columns, the code recalculates the missing values to assess the impact of the cleaning process.


```{r}

# Remove the columns with more than 50% missing values from the dataframe
# This step helps in cleaning the dataset by excluding columns with excessive missing data

Rostherne_combined <- Rostherne_combined[, !(names(Rostherne_combined) %in% columns_over_50_missing)]

# Print the updated dataframe to verify that the columns have been removed
# This allows you to inspect the dataframe and ensure the removal was successful
print(Rostherne_combined)

# Recalculate the number of missing values in each column of the updated dataframe
# This step checks the status of missing data after the problematic columns have been removed
missing_values_updated <- sapply(Rostherne_combined, function(x) sum(is.na(x)))

# Update the missing values dataframe to reflect the current state of the dataset
# This updated dataframe provides an overview of missing data in the cleaned dataset
missing_data_frame_201415 <- data.frame(Column = names(missing_values_updated), MissingValues = missing_values_updated)

# Print the updated missing values dataframe
# This output helps to verify the current missing data situation after column removal
print(missing_data_frame_201415)

```




# This block of code is focused on cleaning the 'Rostherne_combined' dataset by removing columns that have names ending with '_q' or '_j'.
# These columns may represent quality flags or other auxiliary data that are not needed for further analysis.

```{r}
# Remove columns that end with '_q' and '_j' from Rostherne_combined
# The dplyr::select function is used with the matches() function to identify and remove columns
# ending with '_q' (likely indicating quality flags) and '_j' from the dataset.
Rostherne_combined <- Rostherne_combined %>%
  dplyr::select(-matches(".*_q$"), -matches(".*_j$"))


```
# The resulting dataset, 'Rostherne_combined', no longer contains these auxiliary columns, making it more streamlined for analysis.



# This block of code is designed to streamline the 'Rostherne_combined' dataset by removing specific columns (2 through 9),
# which are presumed to be unnecessary ID columns, making the dataset more focused on the relevant data for analysis.
```{r}
# Remove ID columns (columns 2 through 9) from the 'Rostherne_combined' dataset
# The dplyr::select function is used with a negative index to drop columns 2 through 9
# These columns are likely ID columns or other metadata that are not needed for further analysis
Rostherne_combined <- Rostherne_combined %>%
  dplyr::select(-c(2:9))  # negative to drop columns 2 through 9


```
# The resulting 'Rostherne_combined' dataset is now streamlined, excluding unnecessary ID columns, and ready for further analysis.



# This block of code is focused on refining the 'Rostherne_combined' dataset by removing specific columns that are deemed unnecessary for the analysis.
# After removing these columns, a summary of the dataset is generated to review its current state.

```{r}
# Remove specific unnecessary columns from 'Rostherne_combined'
# The dplyr::select function is used to drop the following columns:
# - 'cld_amt_id_1' and 'cld_base_ht_id_1': Likely related to cloud amount and base height, which may not be relevant.
# - 'meto_stmp_time' and 'midas_stmp_etime': Timestamps or metadata columns that may not be needed for analysis.
Rostherne_combined <- Rostherne_combined %>%
    dplyr::select(-cld_amt_id_1, -cld_base_ht_id_1, -meto_stmp_time, -midas_stmp_etime)

```

# Generate a summary of the updated 'Rostherne_combined' dataset
# This summary provides an overview of the dataset's structure, including descriptive statistics for each column.
```{r}
summary(Rostherne_combined)

```


# This block of code is focused on visualizing the gaps (missing values) in specific columns of the 'Rostherne_combined' dataset.
# By generating plots for each column with NA values, the code helps to understand the pattern and extent of missing data.
```{r}

# List of columns to visualize NA gaps
# These columns are selected based on their relevance and the presence of missing values that need to be visualized
columns_with_na <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                     "visibility", "msl_pressure", "air_temperature", "dewpoint",
                     "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                     "drv_hr_sun_dur", "glbl_irad_amt")

# Function to visualize NA gaps for each column
# This function converts the selected column to a time series object and creates a plot
# showing the gaps (missing values) within the time series data
visualize_na_gaps <- function(data, column_name) {
  ts_data <- ts(data[[column_name]])  # Convert to time series object
  plot_title <- paste("NA Gaps for", column_name)
  plot <- ggplot_na_gapsize(ts_data) + ggtitle(plot_title)
  print(plot)
}

# Loop through the list of columns with NA values and visualize the gaps for each
# This loop applies the 'visualize_na_gaps' function to each specified column in the 'Rostherne_combined' dataset
for (column in columns_with_na) {
  visualize_na_gaps(Rostherne_combined, column)
}

# The plots generated in this block provide a visual understanding of the missing data patterns in key columns.
```






# This block of code is focused on handling missing values for specific variables in the 'Rostherne_combined' dataset.
# It uses linear interpolation to fill in small gaps in selected variables where the number of missing values is minimal.
 
```{r}
# List of variables to be interpolated
# These variables are chosen because they have small gaps and a small number of missing values, making them suitable for linear interpolation
variables_interpolation <- c("wind_direction", "wind_speed")

# Loop through each specified variable and apply linear interpolation
# The na_interpolation function is used with the 'linear' option to fill in missing values in a straightforward manner
for (col in variables_interpolation) {
  if (col %in% names(Rostherne_combined)) {
    Rostherne_combined[[col]] <- na_interpolation(Rostherne_combined[[col]], option = "linear")
  }
}


```




# This block of code is focused on handling missing values for variables with larger gaps or a higher number of missing values in the 'Rostherne_combined' dataset.
# Kalman smoothing with auto ARIMA is applied to these variables to fill in the missing data more effectively.
```{r}

# Function to apply Kalman smoothing with auto ARIMA
# This function uses the na_kalman function with the 'auto.arima' model to smooth and interpolate missing values
kalman_auto_arima <- function(series) {
  series_kalman <- na_kalman(series, model = "auto.arima")
  return(series_kalman)
}

# Identify columns with missing values, excluding those already interpolated
# This ensures that Kalman smoothing is only applied to variables that haven't been handled by linear interpolation
variables_kalman <- setdiff(names(Rostherne_combined), variables_interpolation)

# Apply Kalman smoothing with auto ARIMA to the remaining variables
# The loop iterates through the remaining columns and applies the Kalman smoothing if any missing values are present
for (col in variables_kalman) {
  if (any(is.na(Rostherne_combined[[col]]))) {
    Rostherne_combined[[col]] <- kalman_auto_arima(Rostherne_combined[[col]])
  }
}

# Display the result
# This prints the updated 'Rostherne_combined' dataset, showing the effect of the Kalman smoothing
print(Rostherne_combined)




```





# This block is intended to provide a comprehensive summary of the 'Rostherne_combined' dataset after applying the interpolation and Kalman smoothing techniques.
# The summary function will give an overview of the current state of the dataset, including descriptive statistics for each variable.

```{r}
# Generate a summary of the 'Rostherne_combined' dataset
# This summary includes key statistics such as minimum, maximum, mean, and the number of missing values for each column
summary(Rostherne_combined)
```


Relative humidity values calculated from standard weather instruments range from as low as near 1 percent when the evaporation rate greatly exceeds the condensation rate (a huge difference between temperature and dew point), to 100 percent when the evaporation rate equals the condensation rate (temperature and dew point are equal).

```{r}

Rostherne_combined <- Rostherne_combined %>%
  # Replace negative values in glbl_irad_amt with zero
  
  mutate(glbl_irad_amt = ifelse(glbl_irad_amt < 0, 0, glbl_irad_amt)) %>%
  # Ensure rltv_hum values are not above 100
  
  mutate(rltv_hum = ifelse(rltv_hum > 100, 100, rltv_hum)) %>%
  
  # Round cld_ttl_amt_id values and convert to integers
  
  mutate(cld_ttl_amt_id = as.integer(round(cld_ttl_amt_id))) %>%

# Replace negative values in drv_hr_sun_dur with zero

  mutate(drv_hr_sun_dur = ifelse(drv_hr_sun_dur < 0, 0, drv_hr_sun_dur))

summary(Rostherne_combined)
```


```{r}

# Select the weather variables you want to plot
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                  "visibility", "msl_pressure", "air_temperature", "dewpoint",
                  "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                  "drv_hr_sun_dur", "glbl_irad_amt")

# Subset the data to include only the weather variables
weather_data <- Rostherne_combined %>% dplyr::select(all_of(weather_vars))

# Convert the data from wide to long format
weather_data_long <- weather_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Create the boxplots
ggplot(weather_data_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  labs(title = "Boxplots of Weather Variables in Rostherne_combined",
       x = "Weather Variables",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```# This block of code is focused on visualizing the distribution of various weather variables in the 'Rostherne_combined' dataset.
# By creating boxplots, you can easily compare the spread, central tendency, and presence of outliers across different weather variables.

```{r}


# Select the weather variables you want to plot
# These variables are chosen for their relevance to the analysis and are stored in the 'weather_vars' list
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                  "visibility", "msl_pressure", "air_temperature", "dewpoint",
                  "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                  "drv_hr_sun_dur", "glbl_irad_amt")

# Subset the data to include only the selected weather variables
# This step extracts only the relevant columns from the 'Rostherne_combined' dataset
weather_data <- Rostherne_combined %>% dplyr::select(all_of(weather_vars))

# Convert the data from wide to long format for plotting
# The pivot_longer function reshapes the data so that each variable's values are in a single column, 
# making it easier to plot multiple variables in one graph
weather_data_long <- weather_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Create the boxplots for each weather variable
# ggplot2 is used to generate boxplots, which help visualize the distribution and identify outliers for each variable
ggplot(weather_data_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  labs(title = "Boxplots of Weather Variables in Rostherne_combined",
       x = "Weather Variables",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```




# This block of code creates a scatter plot to visualize the variation of global radiation over time in the 'Rostherne_combined' dataset.
# The plot helps in identifying patterns, trends, or anomalies in global radiation values throughout the observation period.

```{r}

# Create a scatter plot for global radiation over time
# The scatter plot uses 'ob_time' as the x-axis (time) and 'glbl_irad_amt' as the y-axis (global radiation amount)
ggplot(Rostherne_combined, aes(x = ob_time, y = glbl_irad_amt)) +
  geom_point(color = "blue", alpha = 0.5) +
  labs(title = "Scatter Plot of Global Radiation Over Time",
       x = "Time",
       y = "Global Radiation (glbl_irad_amt)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability


```




```{r}
# Rename columns for clarity and consistency
colnames(Rostherne_combined)[colnames(Rostherne_combined) == "ob_time"] <- "DateTime"

```

The datasets are merged on the DateTime column using a left join, meaning all rows from Allsites are kept, and matching rows from Rostherne_combined are added.

```{r}

# Merge the datasets based on DateTime

combined_data <- merge(Allsites, Rostherne_combined, by = "DateTime", all.x = TRUE)

```

#Calculate UHI Intensities and Replace Urban Temperatures

#I calculate the Urban Heat Island (UHI) intensities by subtracting the rural temperature (air_temperature) from each urban sensor's temperature and then I replace the urban temperature values with the calculated UHI intensities.

```{r}
# Define the pattern to identify sensor columns
# The pattern matches column names starting with CC, N, NE, E, SE, S, SW, W, or NW, which are likely to be sensor locations
sensor_patterns <- "^CC|^N|^NE|^E|^SE|^S|^SW|^W|^NW"
sensor_columns <- grep(sensor_patterns, names(combined_data), value = TRUE)

# Calculate UHI intensity for each sensor
# The loop iterates over each sensor column, calculates the UHI intensity by subtracting the 'air_temperature' (rural temperature),
# and then renames the column to indicate it now represents UHI intensity
for (sensor in sensor_columns) {
  combined_data[[sensor]] <- combined_data[[sensor]] - combined_data$air_temperature
  # Rename the column to include "UHI_intensity"
  names(combined_data)[names(combined_data) == sensor] <- paste0(sensor, "_UHI")
}
# Display the first few rows of the updated dataset
# The head function is used to verify the changes made to the dataset
head(combined_data)

```


# This block of code processes the 'combined_data' dataset by converting the DateTime column, reshaping the data to a long format,
# cleaning up the 'Ref' column, and arranging the data based on a specified order of sensor references. This prepares the dataset
# for further analysis or visualization by organizing it in a consistent and interpretable format.

```{r}
# Convert DateTime to datetime format
combined_data$DateTime <- as.POSIXct(combined_data$DateTime, format="%Y-%m-%d %H:%M:%S", tz="UTC")

#Reshape the dataset to long format
long_data <- pivot_longer(combined_data, cols = ends_with("_UHI"), names_to = "Ref", values_to = "UHI")

# Remove the "_UHI" suffix from the 'Ref' column
long_data$Ref <- sub("_UHI", "", long_data$Ref)

# Specify the desired order of 'Ref'
desired_order <- c("CC2", "CC3", "CC4", "CC5", "CC6", "CC7", "CC8", "CC9", "CC11", 
                   "N1", "N2", "N3", "N4", "N5", "NE1", "NE2", "NE3", "NE5", "NE6", 
                   "E1", "E2", "E3", "E4", "E5", "E6", "SE1", "SE2", "SE3", "SE4", 
                   "SE5", "SE6", "S1", "S2", "S3", "S4", "S5", "S6", "S7", "S8", 
                   "SW1", "SW2", "SW3", "SW4", "SW5", "SW6", "W1", "W2", "W3", 
                   "W4", "W5", "NW0", "NW2", "NW3", "NW5", "NW6")


# Create a custom sorting index and arrange the data
long_data <- long_data %>%
  mutate(Ref_order = match(Ref, desired_order)) %>%
  arrange(Ref_order, DateTime) %>%
  dplyr::select(-Ref_order)

```



```{r}

# Reorder the columns to have DateTime, Ref, UHI, and other observations together
long_data <- long_data %>% dplyr::select(DateTime, Ref, UHI, everything())

```




# This block of code enhances the 'long_data' dataset by performing a left join with the 'locations_svf_ef' dataset.
# The join adds geographical and other relevant information from 'locations_svf_ef' to the UHI data based on the 'Ref' column.

```{r}

# Perform the left join with the locations data
# The left_join function merges 'long_data' with 'locations_svf_ef' using the 'Ref' column as the key
# This adds location-specific information such as coordinates or SVF to each UHI observation in 'long_data'
long_data <- long_data %>% left_join(locations_svf_ef, by = "Ref")

# Print the first few rows of the resulting dataset
# This helps in verifying that the join was successful and that the data has been merged correctly
print(head(long_data))

# Print the structure of the resulting dataset
# The str function provides an overview of the dataset's structure, including data types and the presence of any new columns
print(str(long_data))

```

# This block of code is designed to identify and handle any mismatches between the 'long_data' dataset and the 'locations_svf_ef' dataset.
# It first checks for any unmatched references in both datasets and then filters out the rows in 'long_data' that do not have corresponding location data.

```{r}
# Identify and print location references in 'locations_svf_ef' that do not match any in 'long_data'
# The anti_join function is used to find rows in 'locations_svf_ef' that don't have a matching 'Ref' in 'long_data'
unmatched_locations <- anti_join(locations_svf_ef, long_data, by = "Ref")
print(unmatched_locations$Ref)

# Identify and print references in 'long_data' that do not have corresponding entries in 'locations_svf_ef'
# Another anti_join is performed to find rows in 'long_data' that lack a matching 'Ref' in 'locations_svf_ef'
missing_refs <- anti_join(long_data, locations_svf_ef, by = "Ref")
print(unique(missing_refs$Ref))

# Filter out rows in 'long_data' that do not have corresponding references in 'locations_svf_ef'
# The filter function removes any rows from 'long_data' that have a 'Ref' value identified as missing in the previous step
long_data <- long_data %>%
  filter(!Ref %in% missing_refs$Ref)

# The resulting 'long_data' dataset now only includes rows with valid location references, ensuring consistency between the datasets.
```





# This block of code is focused on identifying rows in the 'long_data' dataset that have missing values in the 'Latitude' or 'Longitude' columns.
# These rows are important to identify as they may indicate issues with location data, which is critical for spatial analysis.

```{r}

# Identify rows with NA values in 'Latitude' and 'Longitude' columns
# The filter function is used to select rows where either 'Latitude' or 'Longitude' is missing

na_rows <- long_data %>%
  filter(is.na(Latitude) | is.na(Longitude))


# Extract and print the unique 'Ref' values associated with these rows
# The unique function ensures that each reference is listed only once, making it easier to identify problematic entries
na_refs <- unique(na_rows$Ref)
print(na_refs)


```


# This block of code is designed to identify potential outliers in the Urban Heat Island (UHI) data by calculating the minimum and maximum UHI values for each station.
# Outliers can be identified by examining these extreme values across the different stations.
```{r}
# Function to find minimum and maximum UHI values for each station
# This function groups the data by station ('Ref') and calculates the minimum and maximum UHI values for each group
find_min_max_per_station <- function(data) {
  data %>%
    group_by(Ref) %>%
    summarise(
      min_UHI = min(UHI, na.rm = TRUE),
      max_UHI = max(UHI, na.rm = TRUE)
    )
}

# Find and print the minimum and maximum UHI values for each station
# This step applies the 'find_min_max_per_station' function to the 'long_data' dataset
min_max_per_station <- find_min_max_per_station(long_data)
print(min_max_per_station)
# Print the result to identify stations with extreme UHI values
```





# This block of code is used to determine the overall range of UHI values in the 'long_data' dataset.
# The range provides a quick overview of the minimum and maximum UHI values across all stations, which is useful for understanding the data distribution and identifying potential outliers.
```{r}
# Print the overall range of UHI values in the dataset
# The range function returns the minimum and maximum values of the 'UHI' column, giving an overview of the spread of UHI values
print(range(long_data$UHI))
```




# This block of code creates a histogram to visualize the distribution of Urban Heat Island (UHI) values in the 'long_data' dataset.
# The histogram helps to understand the frequency of different UHI values and can highlight the presence of any skewness or outliers in the data.
```{r}
ggplot(long_data, aes(x = UHI)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of UHI Values in Manchester", x = "UHI", y = "Frequency") +
  theme_minimal()
```





#Correlation Analysis 

# This block of code creates a heatmap to visualize the correlation matrix of selected weather variables in the 'combined_data' dataset.
# The heatmap provides an intuitive way to understand the relationships between different weather variables, with color intensity indicating the strength of the correlation.
```{r}
# Define the weather variables to include in the correlation matrix
# These variables are selected for their relevance to the analysis and are stored in the 'weather_vars' vector
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht", 
                  "visibility", "msl_pressure", "air_temperature", "dewpoint", 
                  "wetb_temp", "stn_pres", "wmo_hr_sun_dur", "rltv_hum", 
                  "drv_hr_sun_dur", "glbl_irad_amt")


# Melt the correlation matrix for better visualization
# The melt function reshapes the correlation matrix into a long format that is suitable for plotting with ggplot
# Plot the heatmap with correlation values
ggplot(data = melt( cor(combined_data[, weather_vars], use = "complete.obs")  ), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name="Correlation") +
  geom_text(aes(label = sprintf("%.2f", value)), color = "black", size = 2.5) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1)) +
  coord_fixed()


```


High Positive Correlations:

wetb_temp and air_temperature (0.96): Wet bulb temperature is highly positively correlated with air temperature. This makes sense as wet bulb temperature is influenced by the air temperature and humidity.

dewpoint and air_temperature (0.85): Dewpoint is also strongly correlated with air temperature.

dewpoint and wetb_temp (0.95):Dewpoint is also strongly correlated with Wet bulb temperature.

wmo_hr_sun_dur and drv_hr_sun_dur (0.94): The World Meteorological Organization (WMO) hourly sunshine duration is highly correlated with derived hourly sunshine duration, indicating consistency between these measurements.

wmo_hr_sun_dur and glbl_irad_amt (0.73):Both WMO measured and derived sunshine durations are strongly correlated with global solar irradiation .
drv_hr_sun_dur and glbl_irad_amt (0.72):Both WMO measured and derived sunshine durations are strongly correlated with global solar irradiation .

stn_pres and msl_pressure (1.00): Station pressure is perfectly correlated with mean sea level pressure.

High Negative Correlations:

glbl_irad_amt and rltv_hum (-0.65): global solar irradiation  is strongly negatively correlated with relative humidity. This indicates that higher irradiance is associated with lower humidity levels.

cld_ttl_amt_id and cld_base_ht (-0.60): Cloud total amount is negatively correlated with WMO hourly sunshine duration. More clouds reduce the amount of sunshine.






# This block of code is focused on refining the 'long_data' dataset by removing specific columns that are not needed for further analysis.
# After removing these columns, the code displays the first few rows of the updated dataset to verify the changes.
```{r}
# Remove the specified columns from 'long_data'
# The dplyr::select function is used to drop the 'msl_pressure', 'wmo_hr_sun_dur', and 'drv_hr_sun_dur' columns, which may be redundant or irrelevant for the analysis
long_data <- long_data %>%
  dplyr::select(-msl_pressure, -wmo_hr_sun_dur, -drv_hr_sun_dur)
```

```{r}
# Display the first few rows of the updated dataset
# The head function is used to check the structure of 'long_data' after the specified columns have been removed
head(long_data)
```







#Exploratory Data Analysis (EDA) 


# This block of code refines the 'locations_svf_ef' dataset by removing specific locations and any rows with missing coordinates.
# It then converts the data to a spatial format and visualizes it using a leaflet map, where locations are color-coded based on their EF (emissivity factor) values.
```{r}
# Define the list of locations to be removed
# These locations are specified for removal based on criteria such as redundancy, irrelevance, or poor data quality
locations_to_remove <- c("CC1", "CC10", "N6", "NE4", "NW1", "NW4", "S7", "S8", "NE5", "NE6")

# Remove rows with missing coordinates and the specified locations
# The filter function removes rows where either 'Longitude' or 'Latitude' is missing and excludes the locations listed in 'locations_to_remove'
locations_svf_ef <- locations_svf_ef %>%
  filter(!is.na(Longitude) & !is.na(Latitude) & !Ref %in% locations_to_remove)

# Convert the dataset to an sf (spatial features) object
# This conversion is necessary for spatial operations and visualization, setting the coordinate system to WGS 84 (EPSG:4326)
data_sf <- st_as_sf(locations_svf_ef, coords = c("Longitude", "Latitude"), crs = 4326)

# Create a color palette for visualizing EF values
# The colorNumeric function generates a color scale from yellow to red, corresponding to the range of EF values
pal <- colorNumeric(palette = "YlOrRd", domain = data_sf$EF)

# Create the leaflet map for visualizing locations
# The map includes base tiles, circles representing locations with colors based on EF values, and labels for each location reference (Ref)
leaflet(data_sf) %>% 
  addTiles(options = tileOptions(opacity = 0.45)) %>% 
  addCircles(
    color = "red", 
    fillColor = ~pal(EF), 
    fillOpacity = 0.01, 
    radius = 50, 
    popup = ~paste("Ref: ", Ref, "<br>")
  ) %>% 
  addLabelOnlyMarkers(
    label = ~Ref,
    labelOptions = labelOptions(noHide = TRUE, direction = 'top', textOnly = TRUE)
  ) %>%
  addMiniMap()


```



Typically the temperature difference between rural and urban areas is larger at night than during the day, is most obvious when winds are weak and skies are clear, and is greater in the summer than in the winter (Oke, 1987).

First, I aimed to examine the frequency distribution of Urban Heat Island (UHI) intensities for all sites during the daytime in the summer months (June, July, August) for the years 2014 and 2015. Additionally, nocturnal UHI intensities for Brown Street and University Place were also selected for a similar histogram analysis. This was done because Brown Street has the smallest sky view factor (Great Ducie street has a much lower sky view factor, however, it is not a canyon, the monitoring station was located under a bridge) and University Place has the largest among the canyons.


# This block of code calculates the mean Urban Heat Island (UHI) intensity by month for the years 2014 and 2015.
# It then visualizes the monthly mean UHI values using a line plot, with separate facets for each year to compare the seasonal patterns.
```{r}

# Calculate mean UHI by month for both 2014 and 2015
# The mutate function is used to extract the year and month from the DateTime column, then the data is grouped by Year and Month
# The summarise function calculates the mean UHI for each month, excluding NA values
monthly_uhi <- long_data %>%
  mutate(Year = year(DateTime),
         Month = floor_date(DateTime, "month")) %>%
  group_by(Year, Month) %>%
  summarise(Mean_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Convert Month to Date format
# This step ensures that the Month column is in Date format, which is necessary for accurate plotting along the x-axis
monthly_uhi$Month <- as.Date(monthly_uhi$Month)

# Plot the mean UHI by month, faceted by year
# The ggplot function creates a line plot of mean UHI values over time, with separate lines for each year
# The facet_wrap function is used to create separate plots for 2014 and 2015, allowing for easy comparison between the years
ggplot(monthly_uhi, aes(x = Month, y = Mean_UHI, color = factor(Year))) +
  geom_line() +
  labs(title = 'Mean UHI Intensity by Month (2014 and 2015)',
       x = 'Month', y = 'Mean UHI Intensity (Â°C)',
       color = 'Year') +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  facet_wrap(~ Year, scales = 'free_x')


```







# This block of code calculates the mean Urban Heat Island (UHI) intensity by month, combining data from both 2014 and 2015.
# It then creates and customizes a line plot to visualize the monthly mean UHI values, enhancing readability by adjusting label and text sizes.
# Finally, the plot is saved as a PNG file with high resolution.
```{r}

# Calculate mean UHI by month, combining data from both 2014 and 2015
# The mutate function extracts the month from the DateTime column, and the data is grouped by month
# The summarise function calculates the mean UHI for each month across both years
monthly_uhi_combined <- long_data %>%
  mutate(Month = month(DateTime, label = FALSE)) %>%  # Extract the month as a number
  group_by(Month) %>%  # Group by the month number
  summarise(Mean_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')  # Calculate the mean UHI for each month

# Plot the mean UHI by month for the combined data
# The ggplot function is used to create a line plot, with additional customization for colors, labels, and text sizes
p<-ggplot(monthly_uhi_combined, aes(x = Month, y = Mean_UHI)) +
  geom_line(color = "#4F6272") +  # Medium Sea Green
  geom_point(color = "#4F6272", size = 2) +
  labs(x = 'Month', y = 'Mean UHI Intensity (Â°C)') +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +  # Use month abbreviations as labels
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p.png", plot = p, width = 8, height = 4, units = "in", dpi = 300)
```








# This block of code processes the 'long_data' dataset to filter for daytime hours and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensity for the years 2014 and 2015. The plot is customized for readability and saved as a PNG file.
```{r}


# Ensure the DateTime column is in the correct format and add Year and Hour columns
# The mutate function adds 'Year' (as a factor) and 'Hour' (as an integer) columns for further filtering and analysis
combined_data <- long_data %>%
  mutate(Year = as.factor(year(DateTime)),  # Convert Year to a factor
         Hour = hour(DateTime))  # Extract the hour to filter by daytime and nighttime

# Filter for daytime hours (6 AM to 6 PM)
# This step filters the data to include only observations between 6 AM and 6 PM, which are considered daytime hours
daytime_data <- combined_data %>%
  filter(Hour >= 6 & Hour < 18)

# Create the frequency distribution plot for both 2014 and 2015 side by side (Daytime)
# The ggplot function creates a histogram showing the distribution of UHI values, with separate plots for 2014 and 2015
p1<-ggplot(daytime_data, aes(x = UHI, fill = Year)) +
  geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
  scale_y_continuous(labels = percent) +
  labs(x = "Urban Heat Island Intensity (Â°C)",
       y = "% of time") +
  scale_fill_manual(values = c("2014" = "#FF9999", "2015" = "#9999FF")) +
  theme_minimal() +
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )+
  facet_wrap(~ Year, scales = "free_y")



# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p1.png", plot = p1, width = 8, height = 4, units = "in", dpi = 300)

```











# This block of code filters the UHI data for nighttime hours and creates a frequency distribution plot of UHI intensities for 2014 and 2015.
# The plot is customized for better readability and saved as a PNG file.
```{r}

# Filter for nighttime hours (6 PM to 6 AM)
# This step filters the data to include only observations between 6 PM and 6 AM, which are considered nighttime hours
nighttime_data <- combined_data %>%
nighttime_data <- combined_data %>%
  filter(Hour >= 18 | Hour < 6)

# Create the frequency distribution plot for both 2014 and 2015 side by side (Nighttime)
# The ggplot function creates a histogram showing the distribution of UHI values during nighttime, with separate plots for 2014 and 2015
p2 <- ggplot(nighttime_data, aes(x = UHI, fill = Year)) +
  geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
  scale_y_continuous(labels = percent) +
  labs(title = "Frequency Distribution of Nighttime UHI Intensities (2014 vs 2015)",
       x = "Urban Heat Island Intensity (Â°C)",
       y = "% of time") +
  scale_fill_manual(values = c("2014" = "#4F6272", "2015" = "#5E9EA0")) +
  theme_minimal() +theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )+
  facet_wrap(~ Year, scales = "free_y")



# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p2.png", plot = p2, width = 8, height = 4, units = "in", dpi = 300)


```





## Frequency distribution for daytime UHI Intensities during Summer

# Overall Comment:
# This block of code defines a function to filter data for summer months and daytime hours, and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensities for a specified year. The function is then used to generate plots for the summers of 2014 and 2015.

```{r}

# Function to filter data, create UHI columns, and plot the histogram
# This function filters the data for summer months (June, July, August) and daytime hours (6 AM to 6 PM) for a given year.
# It then creates a histogram to visualize the distribution of UHI intensities during these periods.
plot_uhi_distribution <- function(data, year) {
  # Filter for summer months (June, July, August) in the specified year and daytime hours (6 AM to 6 PM)
  summer_daytime_data <- data %>%
    filter(year(DateTime) == year, month(DateTime) %in% c(6, 7, 8), hour(DateTime) >= 6, hour(DateTime) < 18)
  
  # Create the frequency distribution plot
  ggplot(summer_daytime_data, aes(x = UHI)) +
    geom_histogram(binwidth = 0.5, fill = "red", color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = percent) +
    labs(title = paste("Frequency Distribution for Daytime UHI Intensities During Summer", year),
         x = "Urban Heat Island Intensity (Â°C)",
         y = "% of time") +
    theme_minimal()
}

# Plot for summer 2014
# This call to the function generates and displays the UHI distribution plot for the summer months of 2014
plot_uhi_distribution(long_data, 2014)

# Plot for summer 2015
# This call to the function generates and displays the UHI distribution plot for the summer months of 2015
plot_uhi_distribution(long_data, 2015)



```





## Frequency distribution for nocturnal UHI Intensities during Summer


# This block of code defines a function to filter data for summer months and nocturnal hours, and then creates a frequency distribution plot
# of Urban Heat Island (UHI) intensities for both 2014 and 2015. The function is used to generate a comparative plot for nocturnal UHI intensities during the summer.

```{r}

# Function to filter data, create UHI columns, and plot the histogram for nocturnal hours
# This function filters the data for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM) across both 2014 and 2015.
# It then creates a histogram to visualize the distribution of UHI intensities during these periods, with a side-by-side comparison between the years.
plot_uhi_distribution_nocturnal <- function(data) {
  # Filter for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM)
  summer_nocturnal_data <- data %>%
    filter(month(DateTime) %in% c(6, 7, 8), hour(DateTime) < 6 | hour(DateTime) >= 18)
  
  # Create the frequency distribution plot
  ggplot(summer_nocturnal_data, aes(x = UHI, fill = as.factor(year(DateTime)))) +
    geom_histogram(binwidth = 0.5, color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = percent) +
    labs(title = "Frequency Distribution for Nocturnal UHI Intensities During Summer (2014 vs 2015)",
         x = "Urban Heat Island Intensity (Â°C)",
         y = "% of time",
         fill = "Year") +
    scale_fill_manual(values = c("2014" = "#6A5ACD",  # Medium Slate Blue
                                 "2015" = "#20B2AA")) +  # Light Sea Green
    theme_minimal() +
    facet_wrap(~ year(DateTime), scales = "free_y")
}

# Plot for summer 2014 and 2015 nocturnal side by side
# This call to the function generates and displays the UHI distribution plot for the nocturnal hours during the summer months of 2014 and 2015
plot_uhi_distribution_nocturnal(long_data)



```






## Frequency distribution for nocturnal UHI Intensities for University Place during Summer
# This block of code defines a function to filter data for nocturnal hours during the summer for a specific sensor,
# and then creates a frequency distribution plot of Urban Heat Island (UHI) intensities. The function is applied to data for University Place (sensor CC11)
# for the summers of 2014 and 2015.

```{r}
# Function to plot nocturnal UHI intensities for a specific sensor
# This function filters the data for summer months (June, July, August) and nocturnal hours (6 PM to 6 AM) for a given year and sensor.
# It then creates a histogram to visualize the distribution of UHI intensities for the specified location and year.
plot_uhi_distribution_for_sensor <- function(data, year, sensor_name, location_name) {
  
  # Filter for summer months (June, July, August) in the specified year and nocturnal hours (6 PM to 6 AM)
  summer_nocturnal_data <- data %>%
    filter(year(DateTime) == year & month(DateTime) %in% c(6, 7, 8) & (hour(DateTime) < 6 | hour(DateTime) >= 18) & Ref == sensor_name)
  
  # Create the frequency distribution plot
  ggplot(summer_nocturnal_data, aes(x = UHI)) +
    geom_histogram(binwidth = 0.5, fill = "red", color = "black", aes(y = after_stat(count)/sum(after_stat(count)))) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = paste("Frequency distribution for nocturnal UHI Intensities for", location_name, "during Summer", year),
         x = "Urban Heat Island Intensity (Â°C)",
         y = "% of time") +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_text(size = 12)
    )
}

# Plot for summer 2014 nocturnal for University Place (CC11)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2014 at University Place (sensor #CC11)
plot_uhi_distribution_for_sensor(long_data, 2014, "CC11", "University Place")

# Plot for summer 2015 nocturnal for University Place (CC11)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2015 at University Place (sensor #CC11)
plot_uhi_distribution_for_sensor(long_data, 2015, "CC11", "University Place")
```



## Frequency distribution for nocturnal UHI Intensities for Brown Street during Summer
# This block of code generates frequency distribution plots of nocturnal Urban Heat Island (UHI) intensities during summer for Brown Street (sensor CC8).
# The plots are created for the years 2014 and 2015, allowing for a comparison of nocturnal UHI patterns across these two years.
```{r}
# Plot for summer 2014 nocturnal for Brown Street (CC8)
# This call to the previously defined function generates and displays the UHI distribution plot for the summer nocturnal hours of 2014 at Brown #Street (sensor CC8_
plot_uhi_distribution_for_sensor(long_data, 2014, "CC8", "Browns Street")

# Plot for summer 2015 nocturnal for Brown Street (CC8)
# This call to the function generates and displays the UHI distribution plot for the summer nocturnal hours of 2015 at Brown Street (sensor #CC8)
plot_uhi_distribution_for_sensor(long_data, 2015, "CC8", "Browns Street")
```






## Graphs showing the mean UHI intensity daily profile  
# This block of code calculates the mean Urban Heat Island (UHI) intensity for each hour of the day across the entire year.
# It then visualizes this daily profile using a combination of scatter points and a smoothed line plot. The plot is customized for readability and saved as a PNG file.
```{r}

# Calculate the mean UHI intensity for each hour of the day for the entire year
# The data is grouped by hour, and the mean UHI intensity is calculated for each hour across all observations
p3 <- ggplot(long_data %>%
  mutate(hour = hour(DateTime)) %>%
  group_by(hour) %>%
  summarise(UHI_intensity = mean(UHI, na.rm = TRUE)), aes(x = hour, y = UHI_intensity)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_smooth(method = "loess", color = "black") +
  scale_x_continuous(breaks = seq(0, 23, by = 1), labels = seq(0, 23, by = 1)) +
  labs(x = "Time of day",
       y = "UHI Intensity (Â°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
    
  )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution
ggsave("p3.png", plot = p3, width = 8, height = 4, units = "in", dpi = 300)



```

The UHI intensity is highest during the nighttime and early morning hours, particularly from around midnight (0:00) to 5:00 and again from 18:00 to 23:00. This is because urban areas retain and release the heat absorbed during the day more slowly than rural areas, leading to higher nighttime temperatures in urban areas. The peak UHI intensity is around 2Â°C, indicating that urban areas are significantly warmer than rural areas during these hours.
The UHI intensity starts to decline in the early morning hours, reaching its lowest point around noon (12:00). The lowest UHI intensity is around 0.5Â°C , suggesting that the temperature difference is minimal during these hours.
The UHI intensity begins to increase again in the late afternoon, around 15:00, and continues to rise into the evening. 

The graph confirms that the UHI effect is most significant during nighttime and early morning hours.


# This block of code filters the dataset for city center stations in 2014, calculates the monthly average UHI for each station,
# and then visualizes the results using a lattice plot. The plot is designed to show the monthly UHI effect for each city center station in 2014.

```{r}

# Filter the dataset for city center stations, the year 2014, and calculate monthly averages in one pipeline
cc_data_monthly_avg <- long_data %>%
  filter(grepl("^CC", Ref)) %>%                                 # Filter for city center stations
  filter(DateTime >= as.POSIXct("2014-01-01") & DateTime < as.POSIXct("2015-01-01")) %>%  # Filter for the year 2014
  mutate(Month = format(DateTime, "%Y-%m")) %>%                # Extract month from DateTime
  group_by(Ref, Month) %>%                                     # Group by station and month
  summarise(UHI = mean(UHI, na.rm = TRUE)) %>%                 # Calculate monthly average UHI
  ungroup() %>%                                                # Ungroup the data
  mutate(Month = as.Date(paste0(Month, "-01")))                # Convert Month to Date object

# Check the number of unique city center stations
# This step calculates the number of unique city center stations to adjust the plot layout accordingly
unique_stations <- unique(cc_data_monthly_avg$Ref)
num_stations <- length(unique_stations)
print(paste("Number of city center stations:", num_stations))

# Create the lattice plot with enhanced clarity
# The xyplot function from the lattice package is used to create a plot showing the monthly UHI effect for each city center station
xyplot(UHI ~ Month | Ref, data = cc_data_monthly_avg, type = c("p", "r"),
       pch = 16, cex = 0.7, col = "blue", # Enhance point clarity
       layout = c(5, ceiling(num_stations / 5)), # Adjust layout to fit the number of stations
       as.table = TRUE, 
       xlab = "Month", 
       ylab = "UHI Effect", 
       main = "Average Monthly UHI Effect by City Center Station (2014)",
       scales = list(x = list(rot = 45))) # Rotate x-axis labels for better readability

```



The differences that develop between an urbanizing area and the rural landscape greatly depend on the synoptic conditions. They are in essence a differentiation of topoclimates and as such depend on dissimilarity of radiative fluxes and turbulent exchanges. These contrasts are largest in clear, calm conditions. They tend to disappear in cloudy and windy weather.(Landsberg, H. E. (1981))

Hence, in order to observe the maximum urban heat island intensity, air temperature data was filtered according to different weather parameters, as the literature has already suggested that the urban heat island would be a maximum under clear and calm nights. As a result, air temperature data was selected for clear and calm conditions with the following criteria:

1. Rostherne wind speed less than or equal to 5 knots
2. Rostherne cloud level less than or equal to 2 oktas











## Graphs showing the Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for City Center Stations
# This block of code filters the dataset for clear and calm weather conditions during the summer months, 
# then calculates and visualizes the mean Urban Heat Island (UHI) intensity daily profile for city center stations.
# The resulting graph shows how UHI intensity varies throughout the day under specific weather conditions.

```{r}

# Filter the data for clear and calm conditions
# The data is filtered to include only observations with wind speeds <= 5 m/s and cloud cover <= 2 oktas, which are considered clear and calm conditions
clear_calm_data <- long_data %>%
  filter(wind_speed <= 5 & cld_ttl_amt_id <= 2)

# Filter for summer months (June, July, August)
# The data is further filtered to include only observations from the summer months (June, July, August)
summer_data <- clear_calm_data %>% filter(month(DateTime) %in% c(6, 7, 8))

# Extract hour of the day
# The Hour column is extracted from the DateTime to facilitate grouping by time of day
summer_data$Hour <- format(summer_data$DateTime, "%H:%M:%S")

# Calculate mean UHI intensity for each hour of the day for city center stations
# The data is filtered for city center stations (stations with references starting with "CC") and grouped by hour
# The mean UHI intensity is calculated for each hour across all relevant observations
mean_uhi_hourly <- summer_data %>%
  filter(grepl("^CC", Ref)) %>%
  group_by(Hour) %>%
  summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# Plot the mean UHI intensity daily profile for city center stations
# The ggplot function creates a plot showing the mean UHI intensity throughout the day, with time on the x-axis and UHI intensity on the y-axis
ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
  geom_point(color = "red", alpha = 0.5) +
  labs(title = 'Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for City Center Stations',
       x = 'Time of day',
       y = 'UHI Intensity (Â°C)') +
  theme_minimal() +
  scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```







## Graphs showing the Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for all stations
# This block of code calculates and visualizes the mean Urban Heat Island (UHI) intensity daily profile for all stations during the summer months under clear and calm conditions.
# The resulting graph provides insight into how UHI intensity varies throughout the day across all stations.
```{r}
# Calculate mean UHI intensity for each hour of the day for all stations
# The data is grouped by hour, and the mean UHI intensity is calculated for each hour across all stations under the specified conditions

mean_uhi_hourly <- summer_data %>%
  group_by(Hour) %>%
  summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# Plot the mean UHI intensity daily profile for all stations
# The ggplot function creates a plot showing the mean UHI intensity throughout the day, with time on the x-axis and UHI intensity on the y-axis
ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
  geom_point(color = "blue", alpha = 0.5) +
  labs(title = 'Mean UHI Intensity Daily Profile of Clear and Calm Conditions During Summer for All Stations',
       x = 'Time of day',
       y = 'UHI Intensity (Â°C)') +
  theme_minimal() +
  scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```




# This block of code defines a function to calculate and plot the mean Urban Heat Island (UHI) intensity daily profile for city center stations during a specified season.
# The function is then applied to different seasons to visualize how UHI intensity varies throughout the day in winter, spring, and fall under clear and calm conditions.

```{r}

# Define a function to plot UHI intensity for a given season for city center stations
# This function filters the data for the specified season and city center stations, calculates the mean UHI intensity for each hour, and plots the daily profile.
plot_uhi_intensity_cc <- function(data, season_months, season_name, color) {
  # Filter the data for the given season and city center stations
  season_data <- data %>%
    filter(month(DateTime) %in% season_months & grepl("^CC", Ref))
  
  # Extract hour of the day
  season_data$Hour <- format(season_data$DateTime, "%H:%M:%S")
  
  # Calculate mean UHI intensity for each hour of the day
  mean_uhi_hourly <- season_data %>%
    group_by(Hour) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))
  
  # Plot the mean UHI intensity daily profile
  # The ggplot function creates a plot showing the mean UHI intensity throughout the day for the specified season
  ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
    geom_point(color = color, alpha = 0.5) +
    labs(title = paste('Mean UHI Intensity Daily Profile of Clear and Calm Conditions During', season_name, 'for City Center Stations'),
         x = 'Time of day',
         y = 'UHI Intensity (Â°C)') +
    theme_minimal() +
    scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Filter the data for clear and calm conditions
# The data is filtered to include only observations with wind speeds <= 5 m/s and cloud cover <= 2 oktas, which are considered clear and calm conditions
clear_calm_data <- long_data %>%
  filter(wind_speed <= 5 & cld_ttl_amt_id <= 2)

# Plot UHI intensity for each season for city center stations
# The function is called with different seasonal parameters to generate UHI intensity profiles for winter, spring, and fall
plot_uhi_intensity_cc(clear_calm_data, c(12, 1, 2), "Winter", "blue")
plot_uhi_intensity_cc(clear_calm_data, c(3, 4, 5), "Spring", "green")
plot_uhi_intensity_cc(clear_calm_data, c(9, 10, 11), "Fall", "orange")


```








# This block of code defines a function to calculate and plot the mean Urban Heat Island (UHI) intensity daily profile for all stations during a specified season.
# The function is then applied to different seasons to visualize how UHI intensity varies throughout the day in winter, spring, and fall under clear and calm conditions across all stations.

```{r}

# Define a function to plot UHI intensity for a given season for all stations
# This function filters the data for the specified season, calculates the mean UHI intensity for each hour, and plots the daily profile for all stations.
plot_uhi_intensity_all <- function(data, season_months, season_name, color) {
  # Filter the data for the given season
  # The data is filtered by the specified months
  season_data <- data %>% filter(month(DateTime) %in% season_months)
  
  # Extract hour of the day
  # The Hour column is extracted to facilitate grouping by time of day
  season_data$Hour <- format(season_data$DateTime, "%H:%M:%S")
  
  # Calculate mean UHI intensity for each hour of the day
  # The mean UHI intensity is calculated for each hour across all relevant observations
  mean_uhi_hourly <- season_data %>%
    group_by(Hour) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))
  
  # Plot the mean UHI intensity daily profile
  # The ggplot function creates a plot showing the mean UHI intensity throughout the day for the specified season
  ggplot(mean_uhi_hourly, aes(x = as.POSIXct(Hour, format="%H:%M:%S"), y = mean_UHI)) +
    geom_point(color = color, alpha = 0.5) +  # Plot points with the specified color and semi-transparency
    labs(title = paste('Mean UHI Intensity Daily Profile of Clear and Calm Conditions During', season_name, 'for All Stations'),
         x = 'Time of day',
         y = 'UHI Intensity (Â°C)') +
    theme_minimal() +
    scale_x_datetime(date_labels = "%H:%M", date_breaks = "2 hour") +  # Format the x-axis to show time in hours and minutes
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
}

# Plot UHI intensity for each season for all stations
# The function is called with different seasonal parameters to generate UHI intensity profiles for winter, spring, and fall across all stations
plot_uhi_intensity_all(clear_calm_data, c(12, 1, 2), "Winter", "blue")
plot_uhi_intensity_all(clear_calm_data, c(3, 4, 5), "Spring", "green")
plot_uhi_intensity_all(clear_calm_data, c(9, 10, 11), "Fall", "orange")

```






##SVF


# This block of code analyzes the relationship between Urban Heat Island (UHI) intensity and Sky View Factor (SVF) during clear and calm nights across different seasons.
# It filters the data by season, calculates the average UHI for each site, performs a linear regression with SVF as the predictor, and visualizes the results.

```{r}
# Filter the data for clear and calm conditions
# The data is filtered to include only observations with cloud cover <= 2 oktas and wind speeds <= 5 m/s, which are considered clear and calm conditions

clear_calm_conditions <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5)

# Define the seasons
# The seasons are defined by specifying the corresponding months for spring, summer, autumn, and winter

seasons <- list(
  spring = c(3, 4, 5),
  summer = c(6, 7, 8),
  autumn = c(9, 10, 11),
  winter = c(12, 1, 2)
)

# Function to process data and create plots for each season
# This function filters the data for a specific season and night hours, calculates the average UHI for each site, 
# performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship
process_season <- function(season_name, months) {
  
  season_nights <- clear_calm_conditions %>%
    filter(month(DateTime) %in% months & (hour(DateTime) >= 18 | hour(DateTime) < 6))
  
  # Calculate the overall average UHI for each site
  season_avg <- season_nights %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 
  lm_season <- lm(avg_UHI ~ SVF, data = season_avg)
  summary_season <- summary(lm_season)
  equation_season <- paste("y = ", round(coef(lm_season)[1], 2), " + ", round(coef(lm_season)[2], 2), "x", sep = "")
  r_squared_season <- round(summary_season$r.squared, 3)
  
  # Plot 
  plot <- ggplot(season_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(title = paste("Average UHI Intensity vs Sky View Factor (SVF) during", season_name, "Nights"),
         x = "Sky View Factor (SVF)", y = "Average UHI Intensity (Â°C)") +
    annotate("text", x = 0.25, y = 1.5, 
             label = paste(equation_season, "\nRÂ² = ", r_squared_season), 
             hjust = 0, vjust = 0, size = 4, color = "black") +
    theme_minimal() +
    theme(legend.position = "none")
  
  return(plot)
}

# Generate plots for each season before removing the point
# The process_season function is applied to each season, generating a plot for each
plots_before <- lapply(names(seasons), function(season_name) {
  process_season(season_name, seasons[[season_name]])
})

# Display the plots for each season
# The plots for spring, summer, autumn, and winter are displayed
plots_before[[1]] # Spring 
plots_before[[2]] # Summer 
plots_before[[3]] # Autumn 
plots_before[[4]] # Winter 

```




# Identify the station with the smallest SVF and print its details
```{r}
# Identify the station with the smallest SVF and print its details
smallest_svf_station <- long_data[which.min(long_data$SVF), ]
print(smallest_svf_station)
```



# This block of code removes the station with the smallest Sky View Factor (SVF) from the analysis and then re-evaluates the relationship between Urban Heat Island (UHI) intensity and SVF during clear and calm nights across different seasons.
# The updated analysis is visualized with new plots that exclude the removed station (CC5).

```{r}
# Remove the point with the smallest SVF
# The dataset is filtered to exclude the station 'CC5', which has the smallest SVF
long_data <- long_data %>% filter(Ref != 'CC5')

# Filter the data for clear and calm conditions after removing CC5
# The data is filtered to include only observations with cloud cover <= 2 oktas and wind speeds <= 5 m/s, after excluding the station with the #smallest SVF
clear_calm_conditions_filtered <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5)

# Function to process data and create plots for each season after removing CC5
# This function filters the data for a specific season and night hours, calculates the average UHI for each site, performs a linear regression with SVF as the predictor,
# and creates a plot to visualize the relationship, after the removal of the station with the smallest SVF
process_season_filtered <- function(season_name, months) {

  # Filter the data for the given season and nighttime hours (6 PM to 6 AM)
  season_nights <- clear_calm_conditions_filtered %>%
    filter(month(DateTime) %in% months & (hour(DateTime) >= 18 | hour(DateTime) < 6))
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  season_avg <- season_nights %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 # Perform linear regression of average UHI against SVF
  lm_season <- lm(avg_UHI ~ SVF, data = season_avg)
  summary_season <- summary(lm_season)
  equation_season <- paste("y = ", round(coef(lm_season)[1], 2), " + ", round(coef(lm_season)[2], 2), "x", sep = "")
  r_squared_season <- round(summary_season$r.squared, 3)
  
  # Plot the updated relationship between UHI and SVF after removing CC5
  plot <- ggplot(season_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(title = paste("Average UHI Intensity vs Sky View Factor (SVF) during", season_name, "Nights (After Removal)"),
         x = "Sky View Factor (SVF)", y = "Average UHI Intensity (Â°C)") +
    annotate("text", x = 0.25, y = 1.5, 
             label = paste("Equation: ", equation_season, "\nRÂ² = ", r_squared_season), 
             hjust = 0, vjust = 0, size = 5, color = "black") +
    theme_minimal() +
    theme(legend.position = "none")
  
  return(plot)
}

# Generate plots for each season after removing the point
# The process_season_filtered function is applied to each season, generating a plot for each after the removal of CC5
plots_after <- lapply(names(seasons), function(season_name) {
  process_season_filtered(season_name, seasons[[season_name]])
})

# Display the plots for each season after removal
# The updated plots for spring, summer, autumn, and winter are displayed
plots_after[[1]] # Spring after removal
plots_after[[2]] # Summer after removal
plots_after[[3]] # Autumn after removal
plots_after[[4]] # Winter after removal


```







#Averaged UHI intensity against Sky View Factor on clear and calm nights
# This block of code analyzes the relationship between averaged Urban Heat Island (UHI) intensity and Sky View Factor (SVF) on clear and calm nights.
# It filters the dataset for clear and calm nighttime conditions, calculates the average UHI for each site, performs a linear regression, and visualizes the results.

```{r} 

# Define clear and calm nights
# The dataset is filtered to include only observations with cloud cover <= 2 oktas, wind speeds <= 5 m/s, and nighttime hours (6 PM to 6 AM)

clear_calm_nights <- long_data %>%
  filter(cld_ttl_amt_id <= 2 & wind_speed <= 5 & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Function to create the plot for SVF
# This function calculates the average UHI for each site, performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship

create_svf_plot <- function(data) {
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  svf_avg <- data %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
  # Perform linear regression of average UHI against SVF
  lm_svf <- lm(avg_UHI ~ SVF, data = svf_avg)
  summary_svf <- summary(lm_svf)
  equation_svf <- paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x", sep = "")
  r_squared_svf <- round(summary_svf$r.squared, 3)
  
  # Create a plot to visualize the relationship between UHI and SVF
  plot <- ggplot(svf_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(x = "Sky View Factor (SVF)", 
      y = "Average UHI Intensity (Â°C)"
    ) +
    annotate("text", x = 0.45, y = max(svf_avg$avg_UHI, na.rm = TRUE) * 0.5, 
             label = paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x\nRÂ² = ", round(summary(lm_svf)$r.squared, 3)), 
             size = 5, color = "black", fontface = "italic") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size
      axis.title.x = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
      panel.grid.minor = element_blank()
    )
  
  return(plot)
}

# Generate the SVF plot
# The create_svf_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and SVF

svf_plot <- create_svf_plot(clear_calm_nights)

# Display the plot
print(svf_plot)


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p4.png", plot = svf_plot, width = 8, height = 4, units = "in", dpi = 300)



```








# This block of code focuses on analyzing the relationship between averaged Urban Heat Island (UHI) intensity and Sky View Factor (SVF) for sites with SVF values less than 0.65.
# It filters the dataset for clear and calm nighttime conditions and SVF < 0.65, calculates the average UHI for each site, performs a linear regression, and visualizes the results.

```{r}

# Function to create the plot for SVF with SVF < 0.65
# This function filters the data for SVF values less than 0.65, calculates the average UHI for each site, performs a linear regression of UHI against SVF, and creates a plot to visualize the relationship

create_svf_plot <- function(data) {
  
  # Filter for SVF < 0.65
  # The dataset is filtered to include only observations with SVF values less than 0.65
  filtered_data <- data %>% 
    filter(SVF < 0.65)
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and SVF, and the average UHI is calculated for each group
  svf_avg <- filtered_data %>%
    group_by(Ref, SVF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
 # Perform linear regression of average UHI against SVF
  lm_svf <- lm(avg_UHI ~ SVF, data = svf_avg)
  summary_svf <- summary(lm_svf)
  
  # Create a plot to visualize the relationship between UHI and SVF for SVF < 0.65
  plot <- ggplot(svf_avg, aes(x = SVF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
    labs(x = "Sky View Factor (SVF)", 
      y = "Average UHI Intensity (Â°C)"
    ) +
    annotate("text", x = 0.35, y = max(svf_avg$avg_UHI, na.rm = TRUE) * 0.75, 
             label = paste("y = ", round(coef(lm_svf)[1], 2), " + ", round(coef(lm_svf)[2], 2), "x\nRÂ² = ", round(summary(lm_svf)$r.squared, 3)), 
             size = 5, color = "black", fontface = "italic") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size
      axis.title.x = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 15, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 12, face = "plain"),  # Axis text size and not bold
      panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
      panel.grid.minor = element_blank()
      )
  
  return(plot)
}

# Generate the SVF plot with SVF < 0.65
# The create_svf_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and SVF for SVF < 0.65

svf_plot <- create_svf_plot(clear_calm_nights)

# Display the plot
print(svf_plot)


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p5.png", plot = svf_plot, width = 8, height = 4, units = "in", dpi = 300)

```








#EF

#Averaged UHI intensity against evapotranspiration fraction on clear and calm nights

```{r}

# Function to create the plot for EF
# This function calculates the average UHI intensity for each site based on the Evapotranspiration Fraction (EF), 
# performs a linear regression of UHI against EF, and creates a plot to visualize the relationship.

create_ef_plot <- function(data) {
  
  # Calculate the overall average UHI for each site
  # The data is grouped by site (Ref) and EF, and the average UHI is calculated for each group.
  ef_avg <- data %>%
    group_by(Ref, EF) %>%
    summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')
  
  # Perform linear regression of average UHI against EF
  lm_ef <- lm(avg_UHI ~ EF, data = ef_avg)
  summary_ef <- summary(lm_ef)
  equation_ef <- paste("y = ", round(coef(lm_ef)[1], 2), " + ", round(coef(lm_ef)[2], 2), "x", sep = "")
  r_squared_ef <- round(summary_ef$r.squared, 3)
  
  # Create a plot to visualize the relationship between UHI and EF
  plot <- ggplot(ef_avg, aes(x = EF, y = avg_UHI, color = Ref)) +
    geom_point(alpha = 0.7, size = 3) +  # Plot points with transparency and size
    geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +  # Add a linear regression line
    labs(
      title = "Averaged UHI intensity against Evapotranspiration Fraction on clear and calm nights", 
      x = "Evapotranspiration Fraction (EF)", 
      y = "Average UHI Intensity (Â°C)"
    ) +
    annotate("text", x = 0.7, y = max(ef_avg$avg_UHI, na.rm = TRUE) * 0.9, 
             label = paste("y = ", round(coef(lm_ef)[1], 2), " + ", round(coef(lm_ef)[2], 2), "x\nRÂ² = ", round(summary(lm_ef)$r.squared, 3)), 
             size = 4, color = "black", fontface = "italic") +  # Annotate the plot with the regression equation and R-squared value
    theme_minimal() +
    theme(
      legend.position = "none",  # Remove the legend for clarity
      plot.title = element_text(hjust = 0.5, size = 12, face = "plain"),  # Adjusted title size and formatting
      axis.title.x = element_text(size = 10, face = "plain"),  # Axis title size and not bold
      axis.title.y = element_text(size = 10, face = "plain"),  # Axis title size and not bold
      axis.text.x = element_text(size = 8, face = "plain"),  # Axis text size and not bold
      axis.text.y = element_text(size = 8, face = "plain")  # Axis text size and not bold
    )
  
  return(plot)
}

# Generate the EF plot
# The create_ef_plot function is applied to the filtered dataset to generate a plot showing the relationship between UHI and EF
ef_plot <- create_ef_plot(clear_calm_nights)

# Display the plot
print(ef_plot)



```









# This block of code analyzes the relationship between wind speed and averaged Urban Heat Island (UHI) intensity.
# It calculates the average UHI for each wind speed value, creates a plot to visualize the relationship, and saves the plot as a PNG file.

```{r}


# Calculate average UHI intensity for each wind speed value
# The data is grouped by wind speed, and the average UHI is calculated for each group, excluding NA values

avg_uhi_by_wind_speed <- long_data %>%
  group_by(wind_speed) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE)) %>%
  ungroup()

# Create a plot to visualize the relationship between wind speed and average UHI intensity
# The ggplot function creates a scatter plot with points and a line connecting them, as well as a smooth curve to highlight trends
p6 <- ggplot(avg_uhi_by_wind_speed, aes(x = wind_speed, y = avg_UHI)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  geom_smooth(method = "loess", se = FALSE) +  
  labs(x = "Wind Speed (knots)", y = "Average UHI Intensity (Â°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
  )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution

ggsave("p6.png", plot = p6, width = 8, height = 4, units = "in", dpi = 300)



```







```{r}

# Filter data for clear nights (all stations)
clear_nights <- long_data %>%
  filter(cld_ttl_amt_id <= 2  & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Calculate hourly averages for each wind speed value during clear nights
hourly_avg <- clear_nights %>%
  mutate(hour = floor_date(DateTime, "hour")) %>%
  group_by(hour, wind_speed) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Linear regression model
linear_model <- lm(avg_UHI ~ wind_speed, data = hourly_avg)
linear_model_summary <- summary(linear_model)
linear_coeff <- coefficients(linear_model)
linear_eq <- paste0("y = ", round(linear_coeff[1], 2), " + ", round(linear_coeff[2], 2), "x")

# Plot with stronger colors: vibrant blue for points and deep black for the line
ggplot(hourly_avg, aes(x = wind_speed, y = avg_UHI)) +
  geom_point(alpha = 0.8, color = "#1E90FF") +  # Dodger Blue points
  geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +  # Black line
  labs(title = "Averaged UHI Intensity against Wind Speed on Clear Nights",
       x = "Wind Speed (knots)", y = "UHI Intensity (Â°C)") +
  annotate("text", x = max(hourly_avg$wind_speed) * 0.6, y = max(hourly_avg$avg_UHI) - 1, 
           label = paste("Linear Fit:\nRÂ² = ", round(linear_model_summary$r.squared, 4), "\n", linear_eq), color = "black", hjust = 0) +  
  theme_minimal()

print(linear_model_summary)
```


Cloud cover is measured in oktas, which are units of eighths of the sky covered by clouds. For example, 0 oktas means no cloud cover, 4 oktas means half of the sky is covered with clouds, and 8 oktas means the sky is fully covered with clouds. If the value is 9, it indicates that the sky is completely obscured, meaning that it's not possible to see the sky to determine the cloud cover.











# This block of code analyzes the relationship between cloud cover levels (in oktas) and averaged Urban Heat Island (UHI) intensity.
# It handles special cases where the sky is obscured, calculates the average UHI for each cloud cover bin, visualizes the results, and saves the plot as a PNG file.

```{r}

# Handle the special value 9 for sky obscured
# A new column 'cloud_cover_bin' is created to handle the special value '9' which indicates that the sky is obscured. This value is labeled separately.

combined_data <- long_data %>%
  mutate(cloud_cover_bin = ifelse(cld_ttl_amt_id == 9, "9 (Sky Obscured)", as.character(cld_ttl_amt_id)))

# Calculate average UHI intensity for each cloud cover bin, excluding "9 (Sky Obscured)"
# The data is filtered to exclude the "9 (Sky Obscured)" category, grouped by cloud cover level, and the average UHI is calculated for each group.
avg_uhi_by_cloud_bin <- combined_data %>%
  filter(cloud_cover_bin != "9 (Sky Obscured)") %>%  # Exclude the "9 (Sky Obscured)" category
  group_by(cloud_cover_bin) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE))

# Convert cloud_cover_bin to a factor with levels in the correct order
# The cloud cover bins are converted to a factor to ensure that they are ordered correctly on the x-axis.
avg_uhi_by_cloud_bin$cloud_cover_bin <- factor(avg_uhi_by_cloud_bin$cloud_cover_bin, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8"))

# Plot the data
# The ggplot function creates a scatter plot with points and a line connecting them, visualizing the relationship between cloud cover level and average UHI intensity.
p7 <- ggplot(avg_uhi_by_cloud_bin, aes(x = cloud_cover_bin, y = avg_UHI)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  labs(x = "Cloud Cover Level (oktas)", y = "UHI Intensity (Â°C)") +
  theme_minimal()+
  theme(
    legend.title = element_text(size = 13),  # Increase the size of the legend title
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 15),  # Increase x-axis label size
    axis.title.y = element_text(size = 15),  # Increase y-axis label size
    axis.text.x = element_text(size = 12),   # Increase x-axis text size
    axis.text.y = element_text(size = 12),   # Increase y-axis text size
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank()
  )


# Save the plot with increased label and text sizes
# The ggsave function saves the plot as a PNG file with specified dimensions and resolution.
ggsave("p7.png", plot = p7, width = 8, height = 4, units = "in", dpi = 300)


# The 'cloud_cover_bin' column is removed from the combined dataset if it's no longer needed.
combined_data <- combined_data %>%
  dplyr::select(-cloud_cover_bin)

```








# This block of code analyzes the relationship between cloud cover levels and averaged Urban Heat Island (UHI) intensity during calm nights.
# It filters the dataset for calm nights, calculates hourly average UHI for each cloud cover level, fits a linear regression model, and visualizes the results.

```{r}

# Filter data for calm nights (all sites, all year) with wind speed <= 5 knots
# The dataset is filtered to include only nighttime observations (6 PM to 6 AM) with wind speeds <= 5 knots, considered calm nights.

calm_nights <- long_data %>%
  filter(wind_speed <= 5 & (hour(DateTime) >= 18 | hour(DateTime) < 6))

# Calculate hourly averages for each cloud cover level without averaging cloud cover
# The DateTime column is rounded to the nearest hour, and the data is grouped by hour and cloud cover level (cld_ttl_amt_id).
# The average UHI is then calculated for each group.
hourly_avg <- calm_nights %>%
  mutate(hour = floor_date(DateTime, "hour")) %>%
  group_by(hour, cld_ttl_amt_id) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Linear regression model
# A linear regression model is fitted to the data, with average UHI as the dependent variable and cloud cover level as the independent variable.

linear_model <- lm(avg_UHI ~ cld_ttl_amt_id, data = hourly_avg)
linear_coeffs <- coefficients(linear_model)
linear_model_summary <- summary(linear_model)

# Plot
ggplot(hourly_avg, aes(x = as.numeric(cld_ttl_amt_id), y = avg_UHI)) +
  geom_point(alpha = 0.7, color = "#696969") +  # Dim grey points for nighttime data
  geom_smooth(method = "lm", se = FALSE, color = "black", formula = y ~ x) +
  scale_x_continuous(breaks = seq(0, 9, 1), limits = c(0, 9)) +
  labs(title = "Averaged UHI Intensity against Cloud Cover Level on Calm Nights",
       x = "Cloud Level (oktas)", y = "UHI Intensity (Â°C)") +
  annotate("text", x = 6, y = max(hourly_avg$avg_UHI) - 0.5, 
           label = paste("Linear Fit:\nRÂ² = ", round(linear_model_summary$r.squared, 4), 
                         "\nEquation: y = ", round(linear_coeffs[1], 2), " + ", round(linear_coeffs[2], 4), "x"), 
           color = "black") +
  theme_minimal()

# Print the summary of the linear regression model
print(linear_model_summary)

```








# This block of code categorizes the wind speed and cloud cover levels, calculates the average Urban Heat Island (UHI) intensity for each combination of these categories, and filters the data to remove NA values.
```{r}
# Categorize wind speed and cloud cover levels
# The wind_speed is categorized into bins (e.g., "0-3", "3-6", etc.), and the cloud cover levels (cld_ttl_amt_id) are converted to factors.
avg_uhi_data <- long_data %>%
  mutate(wind_speed_cat = cut(wind_speed, 
                              breaks = c(0, 3, 6, 9, 12, 15, Inf), 
                              labels = c("0-3", "3-6", "6-9", "9-12", "12-15", "15+")),
         cloud_cat = as.factor(cld_ttl_amt_id))



# Calculate average UHI for each combination of cloud cover and wind speed category
# The data is grouped by cloud cover category and wind speed category, and the average UHI is calculated for each group, excluding NA values in UHI.
avg_uhi_data <- avg_uhi_data %>%
  filter(!is.na(UHI)) %>%  # Ensure no NA values in UHI
  group_by(cloud_cat, wind_speed_cat) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Filter out rows where wind_speed_cat is NA
# The data is further filtered to remove rows where the wind speed category is NA.
avg_uhi_data <- avg_uhi_data %>%
  filter(!is.na(wind_speed_cat))
```













# This block of code analyzes the relationship between an interaction term (wind speed multiplied by cloud cover) and averaged Urban Heat Island (UHI) intensity.
# It creates a new interaction variable, calculates the average UHI for each interaction level, and visualizes the relationship using a scatter plot.

```{r}

# Create an interaction term by multiplying wind speed and cloud cover
# A new variable 'wind_cloud_interaction' is created by multiplying wind speed and cloud cover levels.
data_with_interaction <- long_data %>%
  mutate(wind_cloud_interaction = wind_speed * cld_ttl_amt_id)

# Group by the interaction term and calculate the average UHI
# The data is grouped by the interaction term, and the average UHI is calculated for each group.
avg_uhi_interaction <- data_with_interaction %>%
  group_by(wind_cloud_interaction) %>%
  summarise(avg_UHI = mean(UHI, na.rm = TRUE), .groups = 'drop')

# Plot the relationship between the interaction term and UHI
# The ggplot function creates a scatter plot showing the relationship between the wind speed-cloud cover interaction and average UHI intensity.
ggplot(avg_uhi_interaction, aes(x = wind_cloud_interaction, y = avg_UHI)) +
  geom_point(color = "#1E90FF") +  # Plot points with a specific color
  labs(title = "Mean UHI against Interaction of Wind Speed and Cloud Cover",
       x = "Wind Speed * Cloud Cover",
       y = "UHI Intensity (Â°C)") +
  theme_minimal()

```
# Save the dataset
```{r}

write.csv(long_data, file = "long_data.csv", row.names = FALSE)

```




## Apply log transformation to "glbl_irad_amt" and create a new dataset "long_data_norm" that is the normalized version of "long_data" dataset using Z-Score standardization

```{r}


# Copy the dataset for normalization
long_data_norm <- long_data

# Apply log transformation to glbl_irad_amt
long_data_norm$glbl_irad_amt <- log1p(long_data_norm$glbl_irad_amt)  # log1p to handle zero values

# Function to apply Z-score standardization
z_score <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Identify the numeric columns to be standardized, excluding Latitude and Longitude
numeric_columns <- names(long_data_norm)[sapply(long_data_norm, is.numeric)]
columns_to_standardize <- setdiff(numeric_columns, c("Latitude", "Longitude"))

# Apply Z-score standardization to the selected numeric columns
long_data_norm <- long_data_norm %>%
  mutate(across(all_of(columns_to_standardize), ~ z_score(.)))

# Print the resulting dataframe
print(long_data_norm)



```



# This block of code prepares and visualizes the distribution of various weather variables from the normalized dataset 'long_data_norm'.
# It selects specific weather-related variables, reshapes the data for plotting, and generates boxplots to illustrate the distribution and identify any outliers or patterns within these variables.

```{r}

# Select the weather variables you want to plot
# Define a vector of weather variable names that will be included in the analysis and plotting.
weather_vars <- c("wind_direction", "wind_speed", "cld_ttl_amt_id", "cld_base_ht",
                  "visibility", "air_temperature", "dewpoint",
                  "wetb_temp", "stn_pres", "rltv_hum", 
                   "glbl_irad_amt", "UHI")

# Subset the data to include only the weather variables
# Extract only the specified weather variables from the normalized dataset 'long_data_norm' for further analysis.
weather_data <- long_data_norm %>% dplyr::select(all_of(weather_vars))

# Convert the data from wide to long format
# Reshape the data from wide format (multiple columns for variables) to long format (two columns: 'Variable' and 'Value') to facilitate plotting with ggplot2.
weather_data_long <- weather_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Create the boxplots
ggplot(weather_data_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  labs(title = "Boxplots of Weather Variables in Rostherne_combined",
       x = "Weather Variables",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability


```



# This block of code filters the dataset to include only specific reference sites (Ref) and then creates a boxplot to visualize the distribution of Urban Heat Island (UHI) intensity for these sites.

```{r}

# Specify the specific Ref sites
# A vector of specific reference site identifiers is defined to filter the data for these particular locations.
specific_refs <- c("CC6", "E6", "N1", "NE5", "NW0", "S6", "SE2", "SW7", "W7")


# Filter the dataset to include only the specified Ref sites
# The dataset 'long_data' is filtered to retain only the rows corresponding to the specified reference sites.

filtered_data <- long_data %>%
  filter(Ref %in% specific_refs)

# Create a boxplot for UHI by site for the specific Ref sites
ggplot(filtered_data, aes(x = Ref, y = UHI)) +
  geom_boxplot() +
  labs(x = "Site (Ref)", y = "Urban Heat Island (UHI)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```


##Multicollinearity Assessment

To ensure that multicollinearity is not adversely affecting the estimates of the coefficients in the model, I assessed the Variance Inflation Factor (VIF) for all independent variables. VIF values indicate the degree to which the variance of an estimated regression coefficient increases due to multicollinearity. 
```{r}
# Fit a linear regression model to predict UHI intensity
# The model includes weather variables (e.g., wind direction, air temperature) and site-related variables (e.g., SVF, EF).

model0 <- lm(UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + cld_base_ht + 
            visibility + air_temperature + dewpoint + wetb_temp + stn_pres + 
            rltv_hum + glbl_irad_amt + SVF + EF, data = long_data_norm)

# Calculate VIF values
# Variance Inflation Factor (VIF) values are calculated to detect multicollinearity among the predictors in the model. 
# High VIF values indicate potential multicollinearity issues.
vif_values <- vif(model0)

# Print the VIF values
# The calculated VIF values are printed to identify which variables may be contributing to multicollinearity.
print(vif_values)

# Identify variables with high VIF values and calculate pairwise correlations
# Variables with high VIF values are selected, and their pairwise correlations are calculated to explore the extent of multicollinearity between them.

high_vif_vars <- long_data_norm %>%
  dplyr::select(air_temperature, dewpoint, wetb_temp, rltv_hum)

# Calculate the correlation matrix for the selected variables
# The correlation matrix provides insights into how strongly these variables are correlated with each other.
cor_matrix <- cor(high_vif_vars, use = "complete.obs")
print(cor_matrix)


```








To address the high multicollinearity, I removed the variables with the highest VIF values, specifically dewpoint and wetb_temp. The pairwise correlation matrix among variables with high VIF values confirmed high correlations, justifying their removal. 

```{r}

model_refined <- lm(UHI ~ wind_direction + wind_speed + cld_ttl_amt_id + cld_base_ht + 
                    visibility + air_temperature + rltv_hum + stn_pres + wetb_temp +
                    glbl_irad_amt + SVF + EF, data = long_data_norm)

# Calculate VIF values for the refined model
vif_values_refined <- vif(model_refined)

# Print the VIF values
print(vif_values_refined)

```

All the VIF values are below the threshold of 10, indicating that multicollinearity is not a significant issue after the removal of dewpoint and wetb_temp. Therefore, the selected variables can be included in the linear mixed model without concerns for multicollinearity affecting the reliability of the coefficient estimates.













#finalizing the dataset, splitting into train and test dataframes

```{r}
# Convert Ref to a factor
long_data$Ref <- as.factor(long_data$Ref)
str(long_data)
```

#DateTime tranformation to be in a good format
```{r}
# Extract the month, day, and hour as factors for potential seasonal and daily effects

long_data <- long_data %>%
  mutate(Month = factor(format(DateTime, "%m")),
         Day = factor(format(DateTime, "%d")),
         Year = factor(format(DateTime, "%Y")),
         Hour = factor(format(DateTime, "%H")))


long_data$DateTime <- as.POSIXct(long_data$DateTime)


# Ensure the data is sorted by DateTime
long_data <- long_data %>% arrange(DateTime)

```





#remove the locations that dont have coordinates

```{r}

# Removing rows with Ref == "NE5" or Ref == "NE6" from long_data
long_data_gam <- long_data[!(long_data$Ref %in% c("NE5", "NE6")), ]


# Verify the removal
print(unique(long_data_gam$Ref))


# Convert DateTime to POSIXct
long_data_gam$DateTime <- as.POSIXct(long_data_gam$DateTime, format="%Y-%m-%d %H:%M:%S", tz="UTC")


```







SPATIO-TEMPORAL
## Prepare Spatial Data for Analysis
```{r}


# Drop unused levels from Ref
long_data_gam$Ref <- droplevels(long_data_gam$Ref)

# Verify that the Ref factor now only contains the used levels
levels(long_data_gam$Ref)

# Ensure that Latitude and Longitude are numeric
long_data_gam$Latitude <- as.numeric(long_data_gam$Latitude)
long_data_gam$Longitude <- as.numeric(long_data_gam$Longitude)

# Extract unique spatial points
locations <- unique(long_data_gam[, c("Ref","Latitude", "Longitude")])

# Create SpatialPoints object
spatial_points <- SpatialPoints(locations[, c("Longitude", "Latitude")], proj4string = CRS("+proj=longlat +datum=WGS84"))

```






## Extract Unique Time Points from Data
```{r}

# Extract unique time points
time_points <- unique(long_data_gam$DateTime)
```



## Create and Analyze Spatio-Temporal Data

```{r}
# Ensure data is sorted by DateTime and Ref
long_data_gam <- long_data_gam[order(long_data_gam$DateTime, long_data_gam$Ref), ]

# Create the STFDF object
stfdf <- STFDF(sp=spatial_points, time=time_points, data=long_data_gam[, "UHI", drop = FALSE])

# View the STFDF object to confirm
print(stfdf)


# Summarize the STFDF object
summary(stfdf)

# Plot the spatio-temporal data
plot(stfdf)

```


#Select subset for plot

```{r}
subset_stfdf <- stfdf[1:5, "2014-01/2014-06"]
plot(subset_stfdf)
```
X-axis (time): Represents the time component, ranging from 2014 to 2016.
Y-axis (space): Represents the spatial locations, numbered from 1 to 50 (since you have 50 locations).
The black area indicates that there are UHI values for every combination of location and time, but it's too dense to distinguish individual points.



#3D plot
```{r}

df_3d <- long_data_gam

# Calculate the average UHI for each location
# Assuming your dataframe has columns: Longitude, Latitude, UHI
avg_uhi <- df_3d %>%
  group_by(Ref, Longitude, Latitude) %>%
  summarize(Average_UHI = mean(UHI, na.rm = TRUE))

# Interpolate the data onto a grid
interp_result <- with(avg_uhi, interp(x = Longitude, y = Latitude, z = Average_UHI, duplicate = "mean"))

# Convert the interpolated results to a data frame
interp_df <- expand.grid(Longitude = interp_result$x, Latitude = interp_result$y)
interp_df$Average_UHI <- as.vector(interp_result$z)

# Remove any NA values
interp_df <- na.omit(interp_df)


# Create a 3D scatter plot
plot_ly(avg_uhi, x = ~Longitude, y = ~Latitude, z = ~Average_UHI, 
        color = ~Average_UHI, colors = colorRamp(c("blue", "red")), 
        type = 'scatter3d', mode = 'markers', marker = list(size = 5)) %>%
  colorbar(title = "Average UHI") %>%
  layout(scene = list(xaxis = list(title = 'Longitude'),
                      yaxis = list(title = 'Latitude'),
                      zaxis = list(title = 'Average UHI')))




# Interpolate the data onto a grid
interp_result <- with(avg_uhi, interp(x = Longitude, y = Latitude, z = Average_UHI, duplicate = "mean"))

# Prepare the data for lattice
grid_data <- expand.grid(Longitude = interp_result$x, Latitude = interp_result$y)
grid_data$Average_UHI <- as.vector(interp_result$z)


# Save the plot with larger dimensions
png("3d_UHI_surface_plot.png", width = 12, height = 8, units = "in", res = 300)


# Create the 3D wireframe plot with adjusted label positions
wireframe(Average_UHI ~ Longitude * Latitude, data = grid_data, 
          drape = TRUE, col.regions = rev(heat.colors(100)),
          scales = list(arrows = FALSE, 
                        x = list(rot = 0, just = "right", tck = c(1,0), cex = 1.2),
                        y = list(rot = 0, just = "top", tck = c(1,0), cex = 1.2),
                        z = list(rot = 90), cex = 1.2),  # Rotate the Z-axis label
          xlab = list("Longitude", rot = 25, just = "center", 
                      cex = 1.5),  # Adjust position of x-axis label
          ylab = list("Latitude", rot = -45, just = "center", 
                      offset = 1, cex = 1.5),  # Adjust position of y-axis label
          zlab = list("Average UHI", rot = 90, cex = 1.5),  # Rotate and position z-axis label
          colorkey = list(space = "right", height = 0.5, width = 3.0,labels = list(cex = 1.2)),  # Place color bar inside, adjust size
          aspect = c(1, 0.75),  # Adjust aspect ratio for a bigger plot
          screen = list(z = 30, x = -60),
          par.settings = list(axis.line = list(col = "transparent"))

)


dev.off()

```



#Heatmap
```{r}

# Convert the STFDF object to a data frame
df <- as.data.frame(stfdf)
# Inspect the data frame to ensure it has the correct structure
str(df)

# Create a heatmap using levelplot
levelplot(UHI ~ DateTime * Ref, data = long_data_gam, cuts = 50, col.regions = rev(heat.colors(100)),
          xlab = "Time", ylab = "Location", main = "Heatmap of UHI values")

```

#Select a subset for visualization
```{r}
# Subset the data for the first 5 locations (e.g., "CC2" to "CC11") and the first 3 months of 2014
subset_df <- long_data_gam[long_data_gam$Ref %in% c("CC2", "CC3", "CC4", "CC6", "CC7", "CC8", "CC9", "CC11") & 
                           long_data_gam$DateTime >= as.POSIXct("2014-01-01") & 
                           long_data_gam$DateTime < as.POSIXct("2014-02-01"), ]

# Create a heatmap using levelplot with the subset data
levelplot(UHI ~ DateTime * Ref, data = subset_df, cuts = 50, col.regions = rev(heat.colors(100)),
          xlab = "Time", ylab = "Location", main = "Heatmap of UHI values (Subset: First 5 Locations, Jan-Mar 2014)")


```



#Create animation plot
```{r}

# Create the ggplot object for animation
p <- ggplot(long_data_gam, aes(x = Longitude, y = Latitude, fill = UHI)) +
     geom_tile(width = 0.005, height = 0.005) +  # Use tiles to represent UHI values across the grid
     scale_fill_gradient(low = "yellow", high = "red", limits = range(long_data_gam$UHI), na.value = "white") +  # Color gradient from low to high UHI
     transition_time(DateTime) +  # Animate over time
     labs(title = 'DateTime: {frame_time}', x = 'Longitude', y = 'Latitude') +
     theme_minimal() +  # Minimal theme for clean visuals
     ease_aes('linear')  # Smooth transition between frames

# Animate the plot
animate(p, duration = 30, fps = 10, width = 800, height = 600, renderer = gifski_renderer("UHI_animation.gif"))

```




## Plot UHI Values at a Specific Time Point

```{r}
specific_time <- as.POSIXct("2014-01-01 00:00:00", tz = "UTC")
# Plot a single time point to check if UHI values are correctly mapped
single_time <- long_data_gam[long_data_gam$DateTime == specific_time, ]

ggplot(single_time, aes(x = Longitude, y = Latitude, fill = UHI)) +
    geom_tile(width = 0.005, height = 0.005) +
    scale_fill_gradient(low = "yellow", high = "red", limits = range(single_time$UHI, na.rm = TRUE)) +
    labs(title = 'UHI Values on 2014-01-01 00:00:00', x = 'Longitude', y = 'Latitude', fill = 'UHI') +
    theme_minimal()


```



#Aggregate data plotting for any locations
```{r}
# Define the subset of locations and time range
selected_locations <- c("CC2", "CC3", "CC4")
start_date <- as.POSIXct("2014-01-01")
end_date <- as.POSIXct("2014-12-31")

# Filter the dataset
subset_data <- long_data_gam %>%
    filter(Ref %in% selected_locations & DateTime >= start_date & DateTime <= end_date)

# Convert DateTime to a year-month format
subset_data$Month <- format(subset_data$DateTime, "%Y-%m")

# Aggregate the data by Month and Location (Ref), calculating the mean UHI for each group
monthly_aggregated_data <- subset_data %>%
    group_by(Month, Ref) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# Convert Month back to a Date format for better plotting
monthly_aggregated_data$Month <- as.Date(paste0(monthly_aggregated_data$Month, "-01"))

# Plot the aggregated data
ggplot(monthly_aggregated_data, aes(x = Month, y = mean_UHI, color = Ref, group = Ref)) +
    geom_line() +
    labs(title = 'Monthly Aggregated UHI Values', x = 'Month', y = 'Mean UHI', color = 'Location') +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


```



## Analyze and Plot Average UHI for Selected Locations
```{r}
# Define the first 10 locations you want to analyze
selected_locations <- c("CC2", "CC3", "CC4")

# Filter the dataset to include only the selected locations
subset_data1 <- long_data_gam %>%
    filter(Ref %in% selected_locations)


# Calculate the average UHI value over time for the selected locations
avg_uhi <- subset_data1 %>%
    group_by(DateTime) %>%
    summarize(mean_UHI = mean(UHI, na.rm = TRUE))

# View the first few rows to check the result
head(avg_uhi)

# Plot the average UHI values over time
ggplot(avg_uhi, aes(x = DateTime, y = mean_UHI)) +
    geom_line(color = "blue") +
    labs(title = "Average UHI over Time ", x = "Time", y = "Average UHI") +
    theme_minimal()


```



## Plot UHI Values Over Time for Selected Locations and Time Period
```{r}
# Define the locations you want to analyze
selected_locations <- c("CC2", "CC3", "CC4")

# Define the time period you want to analyze
start_date <- as.POSIXct("2014-01-01")
end_date <- as.POSIXct("2014-03-31")

# Filter the dataset to include only the selected locations and the specified time period
specific_time_data <- long_data_gam %>%
    filter(Ref %in% selected_locations & DateTime >= start_date & DateTime <= end_date)


# Plot the UHI values over time for the selected locations within the specified time period
ggplot(specific_time_data, aes(x = DateTime, y = UHI, color = Ref, group = Ref)) +
    geom_line(size = 1) +
    labs(title = "UHI over Time for Multiple Locations (Jan-Mar 2014)", x = "Time", y = "UHI", color = "Location") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


```



## Scatter Plot of UHI Values at a Specific Time
```{r}
# Define the specific time you want to analyze
specific_time <- as.POSIXct("2014-01-01 00:00:00", tz = "UTC")

# Filter the dataset to include only the data for the specific time
time_data <- long_data_gam %>%
    filter(DateTime == specific_time)

# Create a scatter plot of UHI values for the specific time
ggplot(time_data, aes(x = Longitude, y = Latitude, color = UHI)) +
    geom_point(size = 2) +
    scale_color_gradient(low = "yellow", high = "red") +
    labs(title = paste("UHI Values on", specific_time), x = "Longitude", y = "Latitude", color = "UHI") +
    theme_minimal()


```




## Faceted Line Plot of UHI Values Over Time
```{r}
# Optional: Filter data for specific locations or time period
selected_locations <- c("CC2", "CC3", "CC4", "CC6")
start_date <- as.POSIXct("2014-01-01")
end_date <- as.POSIXct("2014-12-31")

# Filter the dataset
faceted_data <- long_data_gam %>%
    filter(Ref %in% selected_locations & DateTime >= start_date & DateTime <= end_date)

# Create the faceted plot
ggplot(faceted_data, aes(x = DateTime, y = UHI)) +
    geom_line(color = "blue") +
    labs(title = "UHI Values Over Time for Selected Locations", x = "Time", y = "UHI") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for readability
    facet_wrap(~ Ref, ncol = 2, scales = "free_y")  # Facet by location, with free y scales

```



## Faceted Plot of UHI Values by Season for a Specific Location
```{r}
# Define the location you want to analyze
selected_location <- "CC2"
start_date <- as.POSIXct("2014-12-11")
end_date <- as.POSIXct("2015-11-30")

# Filter the dataset to include only the selected location
location_data <- long_data_gam %>%
    filter(Ref == selected_location& DateTime >= start_date & DateTime <= end_date)

# Function to categorize DateTime into seasons
get_season <- function(date) {
  month <- as.numeric(format(date, "%m"))
  if (month %in% c(12, 1, 2)) {
    return("Winter")
  } else if (month %in% c(3, 4, 5)) {
    return("Spring")
  } else if (month %in% c(6, 7, 8)) {
    return("Summer")
  } else {
    return("Fall")
  }
}

# Apply the function to create a Season column
location_data$Season <- sapply(location_data$DateTime, get_season)


# Create the faceted plot by season
ggplot(location_data, aes(x = DateTime, y = UHI)) +
    geom_line(color = "blue") +
    labs(title = paste("UHI Values for Location", selected_location, "by Season"), 
         x = "Time", y = "UHI") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for readability
    facet_wrap(~ Season, ncol = 2, scales = "free_x")  # Facet by season with free x scales


```




#Heatmap

```{r}
  
# Create a density heatmap
ggplot(long_data_gam, aes(x = Longitude, y = Latitude)) +
  stat_density2d(aes(fill = ..level..), geom = "polygon", color = "white") +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "UHI Density Heatmap", x = "Longitude", y = "Latitude", fill = "Density")


```
Given that you have the same number of UHI data points over time for all locations, the density heatmap you generated might not be the best representation of the UHI values themselves. This is because the density heatmap primarily shows the concentration of measurement locations rather than the actual UHI values or their distribution across space.

What the Current Heatmap Shows:
Same Number of UHI Data Points: Since each location has the same number of UHI data points, the density heatmap primarily shows where your measurements are spatially concentrated. This explains why some areas show higher densities than othersâitâs a reflection of where your measurement points are geographically clustered, not the actual UHI values.

No Variation in UHI Data Over Time: If all locations have an equal number of measurements, the density differences arise purely from how these locations are distributed in space.



#Another way for creating animations
```{r}

# Define a time range for the first week of 2014
start_time <- as.POSIXct("2014-01-01 00:00:00")
end_time <- start_time + 7 * 24 * 60 * 60  # One week later

# Subset the long_data_gam for the specified time range
time_range <- long_data_gam$DateTime >= start_time & long_data_gam$DateTime < end_time
uhi_data_week <- long_data_gam[time_range,]
# Ensure that there are points in the subset
if (nrow(uhi_data_week) > 0) {
  # Define color palette for UHI values
  UHI_colors <- viridis(length(unique(uhi_data_week$UHI)))

  # Create an animated GIF to show the evolution of UHI values over one week
  saveGIF({
    for (t in sort(unique(uhi_data_week$DateTime))) {
      subset_data <- uhi_data_week[uhi_data_week$DateTime == t,]
      UHI_values <- subset_data$UHI
      colors <- UHI_colors[as.numeric(cut(UHI_values, breaks = length(UHI_colors)))]
      
      # Plot only the spatial coordinates and UHI values
      plot(subset_data$Longitude, subset_data$Latitude, col = colors, cex = 1.5, pch = 16, 
           xlab = "Longitude", ylab = "Latitude", main = paste("Time:", t))
    }
  }, movie.name = "UHI_over_week.gif", interval = 0.5)
} else {
  cat("No points in the specified time range.\n")
}


```


#Aggregate UHI Values by Location and Visualize
```{r}

# Aggregate UHI values by averaging for each location
df_agg_location <- long_data_gam %>%
  group_by(Longitude, Latitude) %>%
  summarise(mean_UHI = mean(UHI, na.rm = TRUE))

# Plot spatial points with aggregated UHI values
ggplot(df_agg_location, aes(x = Longitude, y = Latitude, color = mean_UHI)) +
  geom_point(size = 3) +
  scale_color_viridis_c() +
  theme_minimal() +
  labs(title = "Spatial Points with Mean UHI Values", x = "Longitude", y = "Latitude", color = "Mean UHI")


#Create a Heatmap with Aggregated UHI Values
# Create a heatmap of density based on the mean UHI values
ggplot(df_agg_location, aes(x = Longitude, y = Latitude)) +
  stat_density2d(aes(fill = ..level..), geom = "polygon", color = "white") +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Density Heatmap with Mean UHI Values", x = "Longitude", y = "Latitude", fill = "Density")



```











#Spatio-temporal GAM





## Ensure Data Consistency and Order
```{r}

# Ensure Refs are included
all_refs <- unique(long_data_gam$Ref)
cat("Refs in long_data_gam:", as.character(all_refs), "\n")
long_data_gam_copy <- long_data_gam
long_data_gam <-long_data_gam %>%
  arrange(DateTime, Ref)
```



## Standardize Timezone and Format Data
```{r}

#match the same timezone as before
long_data_gam$Month <- month(long_data_gam$DateTime)
long_data_gam$DateTime <- long_data_gam$DateTime + hours(1)

# Extract and adjust time components
long_data_gam$Hour <- hour(long_data_gam$DateTime)
long_data_gam$Hour[long_data_gam$Hour == 0] <- 24

# Ensure that the necessary columns are in character or numeric format
long_data_gam$Year <- as.character(long_data_gam$Year)
long_data_gam$Month <- sprintf("%02d", as.numeric(long_data_gam$Month))
long_data_gam$Day <- sprintf("%02d", as.numeric(long_data_gam$Day))
long_data_gam$Hour <- sprintf("%02d", as.numeric(long_data_gam$Hour))

long_data_gam$DateTime <- with(long_data_gam, paste(Year, Month, Day, Hour, "00", "00", sep = "-"))
# Convert the combined string to POSIXct format
long_data_gam$DateTime <- as.POSIXct(long_data_gam$DateTime, format="%Y-%m-%d-%H-%M-%S", tz="UTC")

# Create a numeric time variable
long_data_gam$time_numeric <- as.numeric(long_data_gam$DateTime)


# Ensure that necessary variables are numeric
long_data_gam$Longitude <- as.numeric(long_data_gam$Longitude)
long_data_gam$Latitude <- as.numeric(long_data_gam$Latitude)

# Convert Month, Day, and Hour to factors
long_data_gam$Month <- as.factor(long_data_gam$Month)
long_data_gam$Day <- as.factor(long_data_gam$Day)
long_data_gam$Hour <- as.factor(long_data_gam$Hour)

```








#create the train and test dataset for the construction of stgam models
```{r}
# Split the data sequentially into training (70%) and test (30%) sets
n <- nrow(long_data_gam)
train_index_gam <- seq_len(floor(0.7 * n))
train_data_gam <- long_data_gam[train_index_gam, ]
test_data_gam <- long_data_gam[-train_index_gam, ]

# Check the dimensions of the train and test sets
dim(train_data_gam)
dim(test_data_gam)

train_data_gam <- train_data_gam %>%
  rename(latitude = Latitude)
train_data_gam <- train_data_gam %>%
  rename(longitude = Longitude)

test_data_gam <- test_data_gam %>%
  rename(latitude = Latitude)
test_data_gam <- test_data_gam %>%
  rename(longitude = Longitude)

write.csv(train_data_gam, file = "C://Users//user//Downloads//panagiota_codes//wetransfer_allsites2014and2015-1-xlsx_2024-07-28_1624//train_data_gam.csv", row.names = FALSE)
write.csv(test_data_gam, file = "C://Users//user//Downloads//panagiota_codes//wetransfer_allsites2014and2015-1-xlsx_2024-07-28_1624//test_data_gam.csv", row.names = FALSE)
```


#Model 1 STGAM development
```{r}
model1 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)
# Fit the initial GAM model on the training data
gam_model1 <- gam(model1, data = train_data_gam, method = "REML", select = TRUE)

# Summary of the model
summary(gam_model1)

# Save the model to a file
saveRDS(gam_model1, file = "gam_model1.rds")

```



#Model 1 STGAM with better k values
```{r}

model1k <- UHI ~ s(longitude, latitude, k = 40) + s(time_numeric, k = 100)
# Fit the initial GAM model on the training data
gam_model1k <- gam(model1k, data = train_data_gam, method = "REML", select = TRUE)

# Summary of the model
summary(gam_model1k)

# Save the model to a file
saveRDS(gam_model1k, file = "gam_model1k.rds")

gam.check(gam_model1k)
```






#Model 2 STGAM development
```{r}
model2 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(EF, k = 10, bs = "tp")
# Fit the initial GAM model on the training data
time_taken2 <- system.time({ gam_model2 <- gam(model2, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken2)

# Summary of the model
summary(gam_model2)

# Save the model to a file
saveRDS(gam_model2, file = "gam_model2.rds")

gam.check(gam_model2)
```



#Model 2 STGAM with better k values
```{r}
model2k <- UHI ~ s(longitude, latitude, k = 40) + s(time_numeric, k = 150)+ s(wind_speed, by = time_numeric, k = 30) + s(air_temperature, by = time_numeric, k = 100) + s(SVF, k = 40, bs = "tp") + s(EF, k = 10, bs = "tp")
# Fit the initial GAM model on the training data
time_taken2k <- system.time({ gam_model2k <- gam(model2k, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken2k)

# Summary of the model
summary(gam_model2k)

# Save the model to a file
saveRDS(gam_model2k, file = "gam_model2k.rds")

gam.check(gam_model2k)
```


#Model 2 STGAM with even better k values
```{r}
model2kk <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 150)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 100) + s(SVF, k = 25, bs = "tp") + s(EF, k = 10, bs = "tp")
# Fit the initial GAM model on the training data
time_taken2kk <- system.time({ gam_model2kk <- gam(model2kk, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken2kk)

# Summary of the model
summary(gam_model2kk)

# Save the model to a file
saveRDS(gam_model2kk, file = "gam_model2kk.rds")

gam.check(gam_model2kk)
```






#Model 3 STGAM development
```{r}
model3 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10) + s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(cld_ttl_amt_id, by = time_numeric, k = 10) + s(rltv_hum, by = time_numeric, k = 10) + s(glbl_irad_amt, by = time_numeric, k = 10)
# Fit the initial GAM model on the training data
time_taken3 <- system.time({ gam_model3 <- gam(model3, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken3)

# Summary of the model
summary(gam_model3)

# Save the model to a file
saveRDS(gam_model3, file = "gam_model3.rds")

gam.check(gam_model3)
```



#Model 3 STGAM with better k values
```{r}
model3k <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 150)+ s(wind_speed, by = time_numeric, k = 30) + s(air_temperature, by = time_numeric, k = 50) + s(SVF, k = 20, bs = "tp") + + s(cld_ttl_amt_id, by = time_numeric, k = 9) + s(rltv_hum, by = time_numeric, k = 50) + s(glbl_irad_amt, by = time_numeric, k = 50)
# Fit the initial GAM model on the training data
time_taken3k <- system.time({ gam_model3k <- gam(model3k, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken3k)

# Summary of the model
summary(gam_model3k)

# Save the model to a file
saveRDS(gam_model3k, file = "gam_model3k.rds")

gam.check(gam_model3k)
```






#Model 4 STGAM development
```{r}
model4 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10) + s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(cld_ttl_amt_id, by = time_numeric, k = 10) + s(rltv_hum, by = time_numeric, k = 10) + s(glbl_irad_amt, by = time_numeric, k = 10) + s(wind_direction, by = time_numeric, k = 10) + s(cld_base_ht, by = time_numeric, k = 10)
# Fit the initial GAM model on the training data
time_taken4 <- system.time({ gam_model4 <- gam(model4, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken4)

# Summary of the model
summary(gam_model4)

# Save the model to a file
saveRDS(gam_model4, file = "gam_model4.rds")

gam.check(gam_model4)
```



#Model 4 STGAM with better k values
```{r}
model4k <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 150)+ s(wind_speed, by = time_numeric, k = 30) + s(air_temperature, by = time_numeric, k = 60) + s(SVF, k = 10, bs = "tp") + + s(cld_ttl_amt_id, by = time_numeric, k = 9) + s(rltv_hum, by = time_numeric, k = 60) + s(glbl_irad_amt, by = time_numeric, k = 70) + s(wind_direction, by = time_numeric, k = 20) + s(cld_base_ht, by = time_numeric, k = 20)
# Fit the initial GAM model on the training data
time_taken4k <- system.time({ gam_model4k <- gam(model4k, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken4k)

# Summary of the model
summary(gam_model4k)

# Save the model to a file
saveRDS(gam_model4k, file = "gam_model4k.rds")

gam.check(gam_model4k)
```






#Model 5 STGAM development
```{r}
model5 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10) + s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(cld_ttl_amt_id, by = time_numeric, k = 10) +  s(cld_base_ht, by = time_numeric, k = 10) + s(visibility, by = time_numeric, k = 10) + s(stn_pres, by = time_numeric, k = 10) + s(rltv_hum, by = time_numeric, k = 10) + s(glbl_irad_amt, by = time_numeric, k = 10) + s(wind_direction, by = time_numeric, k = 10)
  

# Fit the initial GAM model on the training data
time_taken5 <- system.time({ gam_model5 <- gam(model5, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken5)


# Summary of the model
summary(gam_model5)

# Save the model to a file
saveRDS(gam_model5, file = "gam_model5.rds")

gam.check(gam_model5)
```






#Model 6 STGAM development
```{r}
model1_1 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + ti(longitude, latitude, time_numeric, bs = c("tp", "tp"), d = c(2, 1), k = c(10, 5))
  
# Fit the initial GAM model on the training data
time_taken1_1 <- system.time({ gam_model1_1 <- gam(model1_1, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken1_1)


# Summary of the model
summary(gam_model1_1)

# Save the model to a file
saveRDS(gam_model1_1, file = "gam_model1_1.rds")

gam.check(gam_model1_1)
```






#Model 7 STGAM development
```{r}
model1_2 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(as.numeric(Month), bs = "cc", k = 12) + s(as.numeric(Day), bs = "cc", k = 31) + s(as.numeric(Hour), bs = "cc", k = 24)
  
# Fit the initial GAM model on the training data
time_taken1_2 <- system.time({ gam_model1_2 <- gam(model1_2, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken1_2)


# Summary of the model
summary(gam_model1_2)

# Save the model to a file
saveRDS(gam_model1_2, file = "gam_model1_2.rds")

gam.check(gam_model1_2)
```






#Model 8 STGAM development
```{r}
model1_3 <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + ti(longitude, latitude, time_numeric, bs = c("tp", "tp"), d = c(2, 1), k = c(10, 5)) + s(as.numeric(Month), bs = "cc", k = 12) + s(as.numeric(Day), bs = "cc", k = 31) + s(as.numeric(Hour), bs = "cc", k = 24)
  
# Fit the initial GAM model on the training data
time_taken1_3 <- system.time({ gam_model1_3 <- gam(model1_3, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken1_3)


# Summary of the model
summary(gam_model1_3)

# Save the model to a file
saveRDS(gam_model1_3, file = "gam_model1_3.rds")

gam.check(gam_model1_3)
```







#Model 9 STGAM development
```{r}
full_model_plus <- UHI ~ s(longitude, latitude, k = 50) + s(time_numeric, k = 10)+ s(wind_speed, by = time_numeric, k = 10) + s(air_temperature, by = time_numeric, k = 10) + s(SVF, k = 10, bs = "tp") + s(cld_ttl_amt_id, by = time_numeric, k = 10) +  s(cld_base_ht, by = time_numeric, k = 10) + s(visibility, by = time_numeric, k = 10) + s(stn_pres, by = time_numeric, k = 10) + ti(longitude, latitude, time_numeric, bs = c("tp", "tp"), d = c(2, 1), k = c(10, 5)) + s(as.numeric(Month), bs = "cc", k = 12) + s(as.numeric(Hour), bs = "cc", k = 24) + s(rltv_hum, by = time_numeric, k = 10) + s(glbl_irad_amt, by = time_numeric, k = 10) + s(wind_direction, by = time_numeric, k = 10) + s(as.numeric(Day), bs = "cc", k = 31)
  


# Fit the initial GAM model on the training data
time_taken_full_model_plus <- system.time({ gam_full_model_plus <- gam(full_model_plus, data = train_data_gam, method = "REML", select = TRUE) })

print(time_taken_full_model_plus)


# Summary of the model
summary(gam_full_model_plus)

# Save the model to a file
saveRDS(gam_full_model_plus, file = "gam_full_model_plus.rds")

gam.check(gam_full_model_plus)
```




#Model 9 STGAM development with better k values formula


# Define the full model formula 
full_model <- UHI ~ s(longitude, latitude, k = 40) + s(time_numeric, k = 50) +
    s(wind_speed, by = time_numeric, k = 30) + s(air_temperature, by = time_numeric, k = 100) +
    s(SVF, k = 40, bs = "tp") + s(cld_ttl_amt_id, by = time_numeric, k = 9) +
    s(cld_base_ht, by = time_numeric, k = 100) + s(visibility, by = time_numeric, k = 80) +
    s(stn_pres, by = time_numeric, k = 200) + ti(Longitude, Latitude, time_numeric, bs = c("tp", "tp"), d = c(2, 1), k = c(40, 20)) +
    s(as.numeric(Month), bs = "cc", k = 11) + s(as.numeric(Hour), bs = "cc", k = 20) +
    s(rltv_hum, by = time_numeric, k = 300) + s(glbl_irad_amt, by = time_numeric, k = 500) +
    s(wind_direction, by = time_numeric, k = 40) + s(as.numeric(Day), bs = "cc", k = 20)




#Evaluate models and summary of the models
```{r}
#check the fit of the models
gam.check(gam_model4k)
gam.check(gam_full_model_plus)


#compare models with BIC and ANOVA
BIC(gam_model1k,gam_model2kk,gam_model3k,gam_model4k, gam_model5, gam_model1_1, gam_model1_2, gam_model1_3, gam_full_model_plus)
anova.gam(gam_model1k,gam_model2kk,gam_model3k,gam_model4k, gam_model5, gam_full_model_plus, test = "Chisq")



#summary of the final model 9
summary(gam_full_model_plus)

# To extract the estimates (coefficients)
coefficients <- summary(gam_full_model_plus)$p.coeff  # For parametric coefficients (categorical terms)
smooth_terms <- summary(gam_full_model_plus)$s.table  # For smooth terms

# Print the coefficients and smooth terms
print(coefficients)
print(smooth_terms)
```





#Proceed with gam_model4k and gam_full_model_plus

# Diagnostic Plots
```{r}
# Diagnostic Plots
par(mfrow = c(2, 2))
plot(gam_model4k, residuals = TRUE, pch = 19, cex = 0.1)
gam.check(gam_model4k)
```



# Diagnostic Plots
```{r}
# Diagnostic Plots
par(mfrow = c(2, 2))
plot(gam_full_model_plus, residuals = TRUE, pch = 19, cex = 0.1)
gam.check(gam_full_model_plus)
```

















#Predictions on the test set for model 4k and model 9
```{r}
# Assuming you already have predictions and actual values
predictions_model4k <- predict(gam_model4k, newdata = test_data_gam, allow.new.levels = TRUE)

#rename 2 variables
test_data_gam <- test_data_gam %>%
  rename(Latitude = latitude)
test_data_gam <- test_data_gam %>%
  rename(Longitude = longitude)
predictions_model_full <- predict(gam_full_model_plus, newdata = test_data_gam, allow.new.levels = TRUE)


#actual values of UHI
actuals <- test_data_gam$UHI


#construct the results dataframe for stgams
results_gam_model_full <- test_data_gam
results_gam_model_full$predicted_UHI <- predictions_model_full

```
#based on bic criterion and the performance metrics as well as the summary and gam check functions we conclude that we are going to continue with model 9 for the best STGAM.





#calculate the metrics by month for the model 9 stgam.

```{r}
results_gam_model_full$residuals <- results_gam_model_full$UHI - results_gam_model_full$predicted_UHI
# Calculate MAE and RMSE by month
monthly_performancegam <- results_gam_model_full %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2)),
              nRMSE_by_range = (RMSE/16.05437 )* 100) %>%
    ungroup()

  # Ensure Month column is in Date format
monthly_performancegam <- monthly_performancegam %>%
    mutate(Month = as.Date(Month))

print(monthly_performancegam)
```





# This block of code calculates key performance metrics for the STGAM model using the test set. 
# The metrics include Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared,and nRMSE by range which help evaluate the model's predictive accuracy.

```{r}
calculate_errors <- function(actuals, predictions) {
  # Root Mean Squared Error (RMSE)
  rmse <- sqrt(mean((predictions - actuals)^2))
  
  # Mean Absolute Error (MAE)
  mae <- mean(abs(predictions - actuals))

  # Mean Squared Error (MSE)
  mse <- mean((actuals - predictions)^2)
  
  # R-squared (RÂ²)
  r_squared <- 1 - sum((actuals - predictions)^2) / sum((actuals - mean(actuals))^2)

  # Median Absolute Error (MedAE)
  nrmse_range <- (rmse / 16.054373) * 100
   
  list(MAE = mae, MSE = mse, RMSE = rmse, nRMSE_by_range = nrmse_range)

#list with the error metrics included
  error_metrics <- list(
    RMSE = rmse,
    MAE = mae,
    MSE = mse,
    R2 = r_squared,
    nRMSE = nrmse_range
  )
  
  return(error_metrics)
}


#errors_model4k <- calculate_errors(actuals, predictions_model4k)
#print(errors_model4k)
errors_model_full <- calculate_errors(results_gam_model_full$UHI, results_gam_model_full$predicted_UHI)
print(errors_model_full)

```


Root Mean Squared Error (RMSE):
Measures the square root of the average of squared differences between predicted and actual values.
It penalizes large errors more than smaller ones.

Normalizing by Range:
Best for data with a wide range of values: If your data has a large spread (i.e., the difference between the maximum and minimum values is significant), normalizing by the range can give a more meaningful percentage that reflects the proportion of error relative to the entire spread of the data.
Useful for comparing across datasets: If you want to compare the performance of models across different datasets with varying ranges, nRMSE normalized by range is often preferred.

Mean Absolute Error (MAE):
Represents the average of the absolute differences between predicted and actual values.
It gives an idea of the magnitude of errors in the model predictions.

R-squared (RÂ²):
Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.
RÂ² ranges from 0 to 1, where 1 indicates perfect predictions.

Median Absolute Error (MedAE):
Similar to MAE, but uses the median of absolute errors.
Less sensitive to outliers compared to MAE.



## Function to Create Diagnostic Plots using the predictions
```{r}
plots <- function(actuals, predictions) {
  plot(actuals, predictions, 
      main = "Predicted vs Actuals",
      xlab = "Actual Values",
      ylab = "Predicted Values",
      pch = 19,
      col = "blue")
  abline(0, 1, col = "red")  # Adds a 45-degree line
  
  
  residuals <- predictions - actuals
  plot(predictions, residuals,
       main = "Residuals vs Fitted Values",
       xlab = "Predicted Values",
       ylab = "Residuals",
       pch = 19,
       col = "blue")
  abline(h = 0, col = "red")
  
  
  hist(residuals, 
      main = "Histogram of Residuals",
      xlab = "Residuals",
      col = "blue",
      breaks = 30)
  
  qqnorm(residuals, 
         main = "Q-Q Plot of Residuals",
         pch = 19,
         col = "blue")
  qqline(residuals, col = "red")
}


#plots(actuals, predictions_model4k)
plots(results_gam_model_full$UHI, results_gam_model_full$predicted_UHI)
```
Predicted vs. Actual Values Plot:
This scatter plot shows how well the predictions align with the actual values. Ideally, the points should lie along the 45-degree line, indicating perfect predictions.

Residuals vs. Fitted Values Plot:
This plot displays the residuals (errors) against the predicted values. It helps to detect patterns in residuals, such as non-linearity, unequal error variances (heteroscedasticity), and outliers. Ideally, the residuals should be randomly scattered around zero.

Residuals Histogram:
A histogram of residuals helps to assess whether the residuals are normally distributed. This is important because many models, including GAMs, assume that the errors are normally distributed.

Residuals Q-Q Plot:
A Q-Q (quantile-quantile) plot compares the residuals to a normal distribution. If the residuals follow a normal distribution, the points should fall approximately along the diagonal line.




# This block of code calculates performance metrics (MSE, RMSE, RÂ², MAE) for the linear regression model at each location in the test set.
# It provides a detailed breakdown of the model's predictive accuracy by location, allowing for a more granular assessment of model performance.


```{r}
# Calculate RMSE, MAE and RÂ² for each location
error_metrics_by_location <- results_gam_model_full %>%
  group_by(Ref) %>%
  summarise(
    MSE = mean((UHI - predicted_UHI)^2),
    RMSE = sqrt(MSE),
    R2 = 1 - sum((UHI - predicted_UHI)^2) / sum((UHI - mean(UHI))^2),
    MAE = mean(abs(UHI - predicted_UHI))         # Mean Absolute Error
  )

# View the error metrics per location
print(error_metrics_by_location)
```



#the randomly selected refs and fianl refs from MLR and LLM , we use the same here for consistency

```{r}
#In my case the random locations were these:
selected_refs <- c("CC8", "N3",  "NE3", "E2",  "SE2" ,"S6"  ,"SW3", "W5" , "NW5")

final_refs <- c("CC8" ,"SE1", "E5",  "SW3")

```




# This script calculates the Root Mean Squared Error (RMSE) for model predictions across different locations and visualizes these errors using a bar plot.
# It loops through each location, computes RMSE values, and creates a bar plot to compare prediction accuracy among locations.
```{r}
# Initialize an empty data frame to store RMSE values
rmse_df <- data.frame(Location = character(), RMSE = numeric(), stringsAsFactors = FALSE)

# Loop through each location in the selected_locations list
for (ref in selected_refs) {
  
  # Filter the data for the current location
  location_data <- results_gam_model_full %>% filter(Ref == ref)
  
  # Calculate RMSE for the current location
  rmse <- sqrt(mean((location_data$UHI - location_data$predicted_UHI)^2))
  
  # Store the results in the data frame
  rmse_df <- rmse_df %>% 
    add_row(Location = ref, RMSE = rmse)
}


# Create a bar plot of RMSE values for all locations
ggplot(rmse_df, aes(x = Location, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "black", color = "black", width = 0.6) +
  labs(x = "Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1),
    axis.title.x = element_text(size = 12, margin = margin(t = 10)),  # Set x-axis label size
    axis.title.y = element_text(size = 12, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  

```








# This script calculates the Root Mean Squared Error (RMSE) for model predictions grouped by letter-based categories of locations and visualizes these errors using a bar plot.
# It creates a new column to categorize locations based on their letter prefixes, calculates RMSE for each category, and plots the results.

```{r}
# Create a new column that extracts only the letters from the Ref column
# This helps to group locations based on their letter prefixes (e.g., CC, NE, etc.).
results_gam_model_full$LetterGroup <- gsub("[0-9]", "", results_gam_model_full$Ref)

# Calculate RMSE for each LetterGroup
# This summarizes RMSE values for each letter-based group to evaluate model performance across different categories.

error_by_group <- results_gam_model_full %>%
  group_by(LetterGroup) %>%
  summarise(
    RMSE = sqrt(mean((UHI - predicted_UHI)^2))  # Calculate RMSE for each group
  )

# View the error by group
# Print the calculated RMSE values for each letter-based group
print(error_by_group)



# Save the bar plot as a PNG file
png("RMSE_by_direction_stgam.png", width = 8, height = 4, units = "in", res = 300)

# Create a bar plot of RMSE values for all letter-based groups
# This plot helps in comparing prediction errors across different location categories.
ggplot(error_by_group, aes(x = LetterGroup, y = RMSE, fill = Location)) +
  geom_bar(stat = "identity", fill = "#5e6a6f", color = "#5e6a6f", width = 0.6) +
  labs(x = "Group Location",
       y = "RMSE") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size=12, angle = 0, hjust = 1),
    axis.title.x = element_text(size = 15, margin = margin(t = 10)),  # Set x-axis label size
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(size = 15, margin = margin(r = 10))   # Set y-axis label size
  ) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))
  
# Close the graphics device
dev.off()
```











# This block of code generates and saves combined plots for observed and predicted Urban Heat Island (UHI) intensity over specified sub-periods within the testing data range for a specific location.
# The function `create_combined_uhi_plot` is used to create these plots, which are then saved as image files.

```{r}

# Define the g_legend function to extract the legend from a ggplot
# This helper function extracts the legend from a ggplot object, which will be used to create a unified legend for the combined plots.

g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Define the sub-periods within the testing data range
time_ranges <- list(
  range1 = as.POSIXct(c("2015-06-15", "2015-06-21")),
  range2 = as.POSIXct(c("2015-08-15", "2015-08-21")),
  range3 = as.POSIXct(c("2015-10-15", "2015-10-21")),
  range4 = as.POSIXct(c("2015-12-15", "2015-12-21"))
)

# Function to create a combined plot for the station and time ranges
# This function generates a combined plot of observed and predicted UHI values for a specific station across the defined time ranges.

create_combined_uhi_plot <- function(ref, data, time_ranges) {
  station_data <- data %>% filter(Ref == ref)  # Filter data for the specific location
  
  plots <- lapply(time_ranges, function(time_range) {
    time_filtered_data <- station_data %>% filter(DateTime >= time_range[1] & DateTime <= time_range[2])
    ggplot(time_filtered_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size=0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size=0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
            legend.position = "bottom",
            legend.title = element_text(size = 12),  # Increase the size of the legend title
            legend.text = element_text(size = 12),
            axis.title.x = element_blank(),  # Increase x-axis label size
            axis.title.y = element_blank(),  # Increase y-axis label size
            axis.text.y = element_text(size = 12),   # Increase y-axis text size
            panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
            panel.grid.minor = element_blank())
  })
  
  # Extract legend from the first plot
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and legend into a single layout
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
  # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )
  
  return(combined_plot)
}


# Loop through each location in the selected_locations list
# The function is called in a loop to generate and save combined plots for the specified location(s).
#From the selected refs I will plot N3 because had the worst error among the other directions

for (ref in "N3") {
  
  # Generate and display the combined plot for each location
  combined_plot <- create_combined_uhi_plot(ref, results_gam_model_full, time_ranges)
  grid.newpage()
  grid.draw(combined_plot)

  ggsave("N3gam.png",plot=combined_plot,  width = 8, height = 4, units = "in", dpi = 300)
}


```




# This script generates scatter plots to visualize the relationship between observed and predicted UHI values for both training and test datasets.
# It also calculates and annotates the R-squared values on these plots to assess model performance.

# r2 scatter plot for test for the STGAM model


```{r}


lm_fit_test <- lm(predicted_UHI ~ UHI, data = results_gam_model_full)
# Extract the coefficients from the linear model
intercept_test <- coef(lm_fit_test)[1]
slope_test <- coef(lm_fit_test)[2]

# Calculate the R-squared value
R2_test <- 1 - sum((results_gam_model_full$UHI - results_gam_model_full$predicted_UHI)^2) / sum((results_gam_model_full$UHI - mean(results_gam_model_full$UHI))^2)

# Scatter plot for the test set
r_squared_plot_test <- ggplot(results_model5_full, aes(x = UHI, y = predicted_UHI)) +
  geom_point(color = "skyblue", alpha = 0.7) +  # Use skyblue color for points
  geom_smooth(method = "lm", col = "black", se = FALSE) +
  labs(x = "Observed UHI",
       y = "Predicted UHI") +
  theme_minimal()+
  ylim(c(-6, 6)) +
  xlim(c(-6, 10)) +
  annotate("text", x = max(results_model5_full$UHI) - 1, y = max(results_model5_full$predicted_UHI)+0.25, 
           label = paste("y =", round(slope_test, 3), "* x +", round(intercept_test, 3)),
           hjust = 1, vjust = 0, size = 5, color = "black") +  # Formula of best-fit line
  annotate("text", x = min(results_model5_full$UHI)+2 , y = max(results_model5_full$predicted_UHI)+0.70, 
           label = paste("RÂ² =", round(R2_test, 3)),
           hjust = 0, vjust = 1, size = 5, color = "black") +  # R-squared value in top left
  theme(
    plot.background = element_rect(fill = "white", color = "white"),
    panel.background = element_rect(fill = "white", color = "white"),  # Set all text elements to size 12
    axis.title = element_text(size = 15),  # Axis titles to size 12
    axis.text = element_text(size = 12),  # Axis tick labels to size 12
    plot.title = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", size = 0.15),  # All grid lines the same color
    panel.grid.minor = element_blank()  # Plot title to size 12
  )


# Display the R-squared plot for the train set
print(r_squared_plot_test)

# Save the plot
ggsave("r_squared_plot_gam.png", plot = r_squared_plot_test, width = 8, height = 4, units = "in", dpi = 300)
```






#The script takes the above random 4 locations and plot the 4 seasons
# This script generates and saves plots comparing observed and predicted UHI values for selected stations
# across different seasonal date ranges. It creates a combined plot for each season, showing how well the
# model's predictions match the actual UHI values.
```{r}

# Define the specific date ranges for plotting
date_ranges <- list(
  Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-09")),
  Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-09")),
  Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-09")),
  Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-06-04"))
)

# Function to create the combined plots for each date range
create_combined_plots <- function(date_range, season,test_data_with_pred,selected_stations) {
  
  # Filter data for the selected stations and date range
  selected_data <- test_data_with_pred %>%
    filter(Ref %in% selected_stations) %>%
    filter(DateTime >= date_range[1] & DateTime <= date_range[2])
  
  # Determine y-axis limits based on the range of observed and predicted values
  y_limits <- range(c(selected_data$UHI, selected_data$predicted_UHI), na.rm = TRUE)
  
  actual_vs_predicted_plot <- ggplot(selected_data, aes(x = DateTime)) +
    geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
    geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
    scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
    labs(color="Legend") +
    #scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
    #scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) +
    theme_minimal() +
    theme(axis.text.x = element_text(size=12,angle = 0, hjust = 1),
          axis.text.y = element_text(size=12),
          axis.title.x = element_text(size=15,margin = margin(t = 20)),
          axis.title.y = element_text(size=15),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          legend.background = element_rect(fill = "white", color = "white"),
          legend.position = "bottom",
          legend.title = element_text(size = 12),  # Increase the size of the legend title
          legend.text = element_text(size = 12),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank() )+
    facet_wrap(~ Ref, ncol = 2)
  
  # Save the plot
  #ggsave(filename = paste0("UHI_Predicted_vs_Actual_", season, "_2015.png"), plot = actual_vs_predicted_plot, width = 16, height = 8)
  
  print(actual_vs_predicted_plot)
}

# Loop over each season and create/save the plots
for (season in names(date_ranges)) {
  file_name <- paste0("Plot_", season, "_gam.png")
  
   # Generate the combined plot for the current season
  p<-create_combined_plots(date_ranges[[season]], season, results_gam_model_full, final_refs)
  
   # Save the plot to a file
  ggsave(file_name,plot=p,  width = 8, height = 4, units = "in", dpi = 300)

}

```











#Plot the four seasons for each ref separetely
# This script generates combined plots for observed and predicted UHI values
# across various time ranges for selected stations. It produces plots showing
# how well the model's predictions align with actual UHI values over different periods.
```{r}

# Define the sub-periods within the testing data range

time_ranges <- list(
  range1 = as.POSIXct(c("2015-05-27", "2015-06-04")),
  range2 = as.POSIXct(c("2015-08-01", "2015-08-09")),
  range3 = as.POSIXct(c("2015-10-01", "2015-10-09")),
  range4 = as.POSIXct(c("2015-12-01", "2015-12-09"))
)

# Function to extract the legend from a ggplot object
g_legend <- function(a.gplot) {
  tmp <- ggplotGrob(a.gplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Function to create a combined plot for each selected station and time range
create_combined_uhi_plot <- function(ref, data, time_ranges, model_name) {
  # Calculate the y-axis limits based on the combined range of UHI and predicted_UHI across all time ranges
  combined_data <- data %>% filter(Ref == ref & DateTime >= min(sapply(time_ranges, `[`, 1)) & DateTime <= max(sapply(time_ranges, `[`, 2)))
  y_limits <- range(c(combined_data$UHI, combined_data$predicted_UHI), na.rm = TRUE)
  print(y_limits)
  
  # Generate a plot for each time range
  plots <- lapply(seq_along(time_ranges), function(i) {
    time_range <- time_ranges[[i]]
    station_data <- data %>% filter(Ref == ref & DateTime >= time_range[1] & DateTime <= time_range[2])
    p <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Observed UHI"), size = 0.35) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 0.35) +
      scale_color_manual(values = c("Observed UHI" = "#00BFC4", "Predicted UHI" = "#F8766D")) +
      labs(color="Legend") +
      scale_x_datetime(breaks = pretty_breaks(n = 4)) +  # Set consistent x-axis grid lines
      scale_y_continuous(limits = y_limits, breaks = pretty_breaks(n = 6)) + # Set consistent y-axis limits and grid lines
      theme_minimal() +
      theme(
        axis.text.x = element_text(size=12,angle = 0, hjust = 1),
        legend.position = "bottom",
        legend.title = element_text(size = 12),  # Increase the size of the legend title
        legend.text = element_text(size = 12),
        axis.title.x = element_blank(),  # Remove x-axis label
        axis.title.y = element_blank(),
        axis.text.y = element_text(size = 12),
        panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
        panel.grid.minor = element_blank()  # Remove minor grid lines
      )
    
    return(p)
  })
  
  # Extract the legend from one of the plots
  legend <- g_legend(plots[[1]])
  
  # Remove legends from individual plots
  plots <- lapply(plots, function(p) p + theme(legend.position = "none"))
  
  # Combine the plots and add the legend at the bottom
  combined_plot <- arrangeGrob(
    do.call(arrangeGrob, c(plots, nrow = 2, ncol = 2)),
    ncol = 1
  )
  
   # Add titles and axis labels
  combined_plot <- grid.arrange(
    arrangeGrob(combined_plot,
                left = textGrob("UHI", rot = 90, gp = gpar(fontsize = 15)),
                bottom = textGrob("DateTime", gp = gpar(fontsize = 15))),
    legend,
    ncol = 1,
    heights = c(4, 0.5)
  )

  
  return(combined_plot)
}


# Generate and display combined plots
for (ref in final_refs) {
  
  
  combined_plot <- create_combined_uhi_plot(ref, results_gam_model_full, time_ranges, "Model_STGAM")
  grid.newpage()
  grid.draw(combined_plot)
  
}
```







# This function calculates and plots the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)
# on a monthly basis. It creates two plots: one for MAE and one for RMSE, visualizing how the model's
# performance varies over time. The plots are customized and saved as PNG files.
```{r}

calculate_and_plot_monthly_performance <- function(data, df_name) {
  
  # Calculate MAE and RMSE by month
  monthly_performance <- data %>%
    mutate(Month = floor_date(DateTime, "month")) %>%
    group_by(Month) %>%
    summarise(MAE = mean(abs(residuals)),
              RMSE = sqrt(mean(residuals^2))) %>%
    ungroup()

  # Ensure Month column is in Date format
  monthly_performance <- monthly_performance %>%
    mutate(Month = as.Date(Month))

  # Plot monthly MAE
  mae_plot <- ggplot(monthly_performance, aes(x = Month, y = MAE)) +
    geom_line(color = "red", size = 0.5) +
    geom_point(color = "red", size = 0.5) +
    labs(x = "Month", y = "Mean Absolute Error (MAE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())

  # Save the MAE plot
  #mae_filename <- paste0(df_name, "_Monthly_MAE.png")
  #ggsave(mae_filename, plot = mae_plot, width = 12, height = 6, units = "in", dpi = 300)

  
  
  # Plot monthly RMSE
  rmse_plot <- ggplot(monthly_performance, aes(x = Month, y = RMSE)) +
    geom_line(color = "green", size = 0.5) +
    geom_point(color = "green", size = 0.5) +
    labs(x = "Month", y = "Root Mean Squared Error (RMSE)") +
    theme_minimal() +
    scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # Reduce number of x-axis grid lines
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1),
          plot.background = element_rect(fill = "white", color = "white"),
          panel.background = element_rect(fill = "white", color = "white"),
          plot.title = element_text(size = 14),
          axis.title.x = element_text(size = 12, margin = margin(t = 10)),
          axis.title.y = element_text(size = 12, margin = margin(r = 10)),
          panel.grid.major = element_line(color = "gray", size = 0.3),  # All grid lines the same color
          panel.grid.minor = element_blank())
  
  # Save the RMSE plot
  #rmse_filename <- paste0(df_name, "_Monthly_RMSE.png")
  #ggsave(rmse_filename, plot = rmse_plot, width = 12, height = 6, units = "in", dpi = 300)


  # Print the plots
  print(mae_plot)
  print(rmse_plot)
}

calculate_and_plot_monthly_performance(results_gam_model_full, "Model STGAM")
```



# This function processes a data frame by categorizing model performance based on residuals,
# and then creates and displays combined plots for specified stations and date ranges. It includes
# plots of actual vs. predicted UHI and model performance over time. The function also prints
# information about the data processing and plot creation steps.

```{r}

# Function to process a single data frame
process_data_frame <- function(df, final_refs, df_name) {
  
  # Convert Ref to factor
  df$Ref <- as.factor(df$Ref)

  # Calculate the mean and standard deviation of the residuals
  residual_mean <- mean(df$residuals)
  residual_sd <- sd(df$residuals)

  # Categorize performance using the standard deviation-based thresholds
  df <- df %>%
    mutate(Performance = case_when(
      abs(residuals - residual_mean) > 1.2 * residual_sd ~ "Poor",
      abs(residuals - residual_mean) > residual_sd ~ "Medium",
      TRUE ~ "Good"
    ))

  # Define the specific date ranges for plotting
  date_ranges <- list(
    Summer = c(as.POSIXct("2015-08-01"), as.POSIXct("2015-08-31")),
    Spring = c(as.POSIXct("2015-05-27"), as.POSIXct("2015-05-31")),
    Winter = c(as.POSIXct("2015-12-01"), as.POSIXct("2015-12-31")),
    Fall = c(as.POSIXct("2015-10-01"), as.POSIXct("2015-10-31"))
  )

  # Function to create the combined plots for each station and date range
  create_combined_plots <- function(station_ref, season) {
    print(paste("Creating plots for Station:", station_ref, "in", season, "2015"))

    station_data <- df %>%
      filter(Ref == station_ref) %>%
      filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])

    if (nrow(station_data) == 0) {
      print(paste("No data available for Station:", station_ref, "in", season, "2015"))
      return()
    }

    actual_vs_predicted_plot_period <- ggplot(station_data, aes(x = DateTime)) +
      geom_line(aes(y = UHI, color = "Actual UHI"), size = 1) +
      geom_line(aes(y = predicted_UHI, color = "Predicted UHI"), size = 1, linetype = "dashed") +
      labs(title = paste(df_name, "- Predicted vs. Actual UHI for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Legend") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Actual UHI" = "skyblue", "Predicted UHI" = "red"))

    performance_points_plot_period <- ggplot(station_data, aes(x = DateTime, y = UHI, color = Performance)) +
      geom_point(size = 1, alpha = 0.6) +
      labs(title = paste(df_name, "- Model Performance Over Time for Station", station_ref, "(", season, "2015)"),
           x = "DateTime", y = "UHI",
           color = "Performance") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      scale_color_manual(values = c("Poor" = "red", "Medium" = "orange", "Good" = "green"))
    
    grid.arrange(actual_vs_predicted_plot_period, performance_points_plot_period, ncol = 1)
  }

  # Loop over the selected stations and seasons to create the plots
  for (station_ref in final_refs) {
    for (season in names(date_ranges)) {
      print(paste("Checking data for Station:", station_ref, "in", season, "2015"))
      filtered_data <- df %>%
        filter(Ref == station_ref) %>%
        filter(DateTime >= date_ranges[[season]][1] & DateTime <= date_ranges[[season]][2])
      
      print(paste("Number of records for Station:", station_ref, "in", season, "2015:", nrow(filtered_data)))
      
      create_combined_plots(station_ref, season)
    }
  }
}



# Apply the function to each data frame

process_data_frame(results_gam_model_full, final_refs, "Model gam")

```







# This script calculates and displays seasonal performance metrics (MAE, MSE, RMSE) for model predictions across different seasons.
```{r}
# Function to calculate additional metrics
calculate_metrics <- function(actual, predicted) {
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  nrmse_range <- (rmse / (max(actual) - min(actual))) * 100
  list(MAE = mae, MSE = mse, RMSE = rmse, nRMSE_by_range = nrmse_range)
}

# Define seasons within the testing data range
seasons <- list(
  Spring = c("2015-05-27", "2015-05-31"),
  Summer = c("2015-06-01", "2015-08-31"),
  Fall = c("2015-09-01", "2015-11-30"),
  Winter = c("2015-12-01", "2015-12-31")
)


# Function to calculate seasonal errors for a single data frame
calculate_seasonal_errors <- function(data, df_name) {
  seasonal_errors <- lapply(names(seasons), function(season) {
    season_data <- data %>%
      filter(DateTime >= as.POSIXct(seasons[[season]][1]) & DateTime <= as.POSIXct(seasons[[season]][2]))
    metrics <- calculate_metrics(season_data$UHI, season_data$predicted_UHI)
    season_errors <- data.frame(DataFrame = df_name, Season = season, MAE = metrics$MAE, MSE = metrics$MSE, RMSE = metrics$RMSE)
    return(season_errors)
  })
  
  # Combine seasonal errors into a single data frame
  seasonal_errors_df <- bind_rows(seasonal_errors)
  return(seasonal_errors_df)
}

# Apply the function to each of the three data frames
stgam_seasonal_errors <- calculate_seasonal_errors(results_gam_model_full, "Model gam")


# Print the combined seasonal errors
print(stgam_seasonal_errors)

```



#This script generates diagnostic plots to assess the residuals from a regression model fitted on the training dataset.

```{r}

# Predict on train
train_data_gam <- train_data_gam %>%
  rename(Latitude = latitude)
train_data_gam <- train_data_gam %>%
  rename(Longitude = longitude)
train_data_gam$predicted_UHI <- predict(gam_full_model_plus, newdata = train_data_gam, allow.new.levels = TRUE)

# Compute residuals
train_data_gam$residuals <- train_data_gam$UHI - train_data_gam$predicted_UHI

# Save the plots as a PNG file
png("4_plot_train_gam_full.png", width = 8, height = 4, units = "in", res = 300)

par(mfrow = c(1, 2))  # Set up a 2x2 plotting area

# a) Q-Q Plot of Residuals
qqnorm(train_data_gam$residuals, main = "(a)",  ylim = c(-10, 10), cex.axis = 1.2, cex.lab = 1.4)
qqline(train_data_gam$residuals, col = "red")

# b) Histogram of Residuals
hist(train_data_gam$residuals, main = "(b)", xlab = "Residuals", breaks = 20,cex.axis = 1.2, cex.lab = 1.4)


dev.off()
```

